{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Batwan01/2024-Challenge/blob/main/Park%20Jiwan/MLP_3_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Google Drive를 마운트합니다.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "leU9y1YkOOld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b5043a-4c1d-40c5-f692-0bb6e02f00f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "6n9GZNa4OP5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/data/samsung/train.csv\"\n",
        "\n",
        "RANDOM_SEED = 18\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 데이터 불러오기\n",
        "train_data = pd.read_csv(train_csv_path)\n",
        "X = train_data.iloc[:,1:-1].values  # 특성 데이터\n",
        "y = train_data.iloc[:,-1].values    # 타겟 데이터\n",
        "\n",
        "# 1. 특성 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# # 2. 이상치 처리 (Z-Score를 사용하여 이상치 제거)\n",
        "# z_scores = np.abs(stats.zscore(X_scaled))\n",
        "# X_clean = X_scaled[(z_scores < 3).all(axis=1)]\n",
        "# y_clean = y[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "# 3. 타겟 변수 변환 (로그 변환)\n",
        "y_log = np.log1p(y_clean)\n",
        "\n",
        "# 텐서로 변환\n",
        "X_tensor = torch.tensor(X_clean, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_log, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# 데이터셋 분할 (Train/Validation Split)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "# MLP 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=11, hidden_sizes=[64, 128, 64], output_size=1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zR4vHPIuOT6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # grid search 로 최적의 하이퍼파라미터 찾기\n",
        "# param_grid = {\n",
        "#     'hidden_sizes': [[64, 128, 64], [128, 256, 128], [32, 64, 32]],\n",
        "#     'learning_rate': [0.01, 0.001, 0.0001],\n",
        "#     'batch_size': [32, 64, 128],\n",
        "# }\n",
        "\n",
        "# def train_model(hidden_sizes, learning_rate, batch_size):\n",
        "#     model = MLP(hidden_sizes=hidden_sizes)\n",
        "#     model.cuda()\n",
        "#     criterion = nn.MSELoss().cuda()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#     for epoch in range(100):\n",
        "#         model.train()\n",
        "#         for batch_X, batch_y in train_loader:\n",
        "#             batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "#             outputs = model(batch_X)\n",
        "#             loss = criterion(outputs, batch_y)\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#     # 검증 성능 계산\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch_X, batch_y in val_loader:\n",
        "#             batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "#             outputs = model(batch_X)\n",
        "#             loss = criterion(outputs, batch_y)\n",
        "#             val_loss += loss.item()\n",
        "\n",
        "#     return val_loss / len(val_loader)\n",
        "\n",
        "# best_params = None\n",
        "# best_MSE = float('inf')\n",
        "# for params in ParameterGrid(param_grid):\n",
        "#     MSE = train_model(**params)\n",
        "#     print(f\"Testing parameters: {params} => MSE: {MSE:.4f}\")\n",
        "#     if MSE < best_MSE:\n",
        "#         best_MSE = MSE\n",
        "#         best_params = params\n",
        "\n",
        "# print(f'\\nBest parameters: {best_params} with a MSE of {best_MSE:.4f}')\n",
        "# with open('3_layer_MLP_params.json', 'w') as json_file:\n",
        "#     json.dump(best_params, json_file)"
      ],
      "metadata": {
        "id": "DNGYnpPrOYBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1815e92c-a559-4e6c-b249-f4cc6a8945ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.01} => MSE: 0.0006\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.0001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.01} => MSE: 0.0011\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.0001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.01} => MSE: 0.0006\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.001} => MSE: 0.0006\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.0001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.01} => MSE: 0.0006\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.001} => MSE: 0.0009\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.0001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.01} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.0001} => MSE: 0.0008\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.01} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.0001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.01} => MSE: 0.0010\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.001} => MSE: 0.0009\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [64, 128, 64], 'learning_rate': 0.0001} => MSE: 0.0008\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.01} => MSE: 0.0008\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.001} => MSE: 0.0008\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [128, 256, 128], 'learning_rate': 0.0001} => MSE: 0.0009\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.01} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.001} => MSE: 0.0007\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.0001} => MSE: 0.0008\n",
            "\n",
            "Best parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.001} with a MSE of 0.0006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XlwhQzDCbVyX",
        "outputId": "9d829190-472a-4f52-9ae3-6b2ed402bb70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Train Loss: 0.8291, Validation Loss: 0.0189\n",
            "Epoch 2/300, Train Loss: 0.0108, Validation Loss: 0.0058\n",
            "Epoch 3/300, Train Loss: 0.0037, Validation Loss: 0.0022\n",
            "Epoch 4/300, Train Loss: 0.0015, Validation Loss: 0.0013\n",
            "Epoch 5/300, Train Loss: 0.0010, Validation Loss: 0.0010\n",
            "Epoch 6/300, Train Loss: 0.0009, Validation Loss: 0.0009\n",
            "Epoch 7/300, Train Loss: 0.0008, Validation Loss: 0.0010\n",
            "Epoch 8/300, Train Loss: 0.0008, Validation Loss: 0.0008\n",
            "Epoch 9/300, Train Loss: 0.0007, Validation Loss: 0.0008\n",
            "Epoch 10/300, Train Loss: 0.0008, Validation Loss: 0.0008\n",
            "Epoch 11/300, Train Loss: 0.0007, Validation Loss: 0.0008\n",
            "Epoch 12/300, Train Loss: 0.0007, Validation Loss: 0.0007\n",
            "Epoch 13/300, Train Loss: 0.0006, Validation Loss: 0.0010\n",
            "Epoch 14/300, Train Loss: 0.0007, Validation Loss: 0.0010\n",
            "Epoch 15/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 16/300, Train Loss: 0.0007, Validation Loss: 0.0007\n",
            "Epoch 17/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 18/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 19/300, Train Loss: 0.0006, Validation Loss: 0.0009\n",
            "Epoch 20/300, Train Loss: 0.0006, Validation Loss: 0.0011\n",
            "Epoch 21/300, Train Loss: 0.0006, Validation Loss: 0.0014\n",
            "Epoch 22/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 23/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 24/300, Train Loss: 0.0006, Validation Loss: 0.0009\n",
            "Epoch 25/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 26/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 27/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 28/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 29/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 30/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 31/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 32/300, Train Loss: 0.0006, Validation Loss: 0.0009\n",
            "Epoch 33/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 34/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 35/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 36/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 37/300, Train Loss: 0.0006, Validation Loss: 0.0010\n",
            "Epoch 38/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 39/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 40/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 41/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 42/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 43/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 44/300, Train Loss: 0.0006, Validation Loss: 0.0006\n",
            "Epoch 45/300, Train Loss: 0.0006, Validation Loss: 0.0006\n",
            "Epoch 46/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 47/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 48/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 49/300, Train Loss: 0.0006, Validation Loss: 0.0010\n",
            "Epoch 50/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 51/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 52/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 53/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 54/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 55/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 56/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 57/300, Train Loss: 0.0005, Validation Loss: 0.0010\n",
            "Epoch 58/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 59/300, Train Loss: 0.0006, Validation Loss: 0.0006\n",
            "Epoch 60/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 61/300, Train Loss: 0.0006, Validation Loss: 0.0009\n",
            "Epoch 62/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 63/300, Train Loss: 0.0006, Validation Loss: 0.0008\n",
            "Epoch 64/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 65/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 66/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 67/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 68/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 69/300, Train Loss: 0.0005, Validation Loss: 0.0009\n",
            "Epoch 70/300, Train Loss: 0.0006, Validation Loss: 0.0007\n",
            "Epoch 71/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 72/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 73/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 74/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 75/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 76/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 77/300, Train Loss: 0.0005, Validation Loss: 0.0011\n",
            "Epoch 78/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 79/300, Train Loss: 0.0005, Validation Loss: 0.0009\n",
            "Epoch 80/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 81/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 82/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 83/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 84/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 85/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 86/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 87/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 88/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 89/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 90/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 91/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 92/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 93/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 94/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 95/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 96/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 97/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 98/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 99/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 100/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 101/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 102/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 103/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 104/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 105/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 106/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 107/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 108/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 109/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 110/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 111/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 112/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 113/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 114/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 115/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 116/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 117/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 118/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 119/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 120/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 121/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 122/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 123/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 124/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 125/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 126/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 127/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 128/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 129/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 130/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 131/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 132/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 133/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 134/300, Train Loss: 0.0005, Validation Loss: 0.0010\n",
            "Epoch 135/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 136/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 137/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 138/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 139/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 140/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 141/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 142/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 143/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 144/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 145/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 146/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 147/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 148/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 149/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 150/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 151/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 152/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 153/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 154/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 155/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 156/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 157/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 158/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 159/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 160/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 161/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 162/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 163/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 164/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 165/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 166/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 167/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 168/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 169/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 170/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 171/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 172/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 173/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 174/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 175/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 176/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 177/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 178/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 179/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 180/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 181/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 182/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 183/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 184/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 185/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 186/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 187/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 188/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 189/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 190/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 191/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 192/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 193/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 194/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 195/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 196/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 197/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 198/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 199/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 200/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 201/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 202/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 203/300, Train Loss: 0.0005, Validation Loss: 0.0009\n",
            "Epoch 204/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 205/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 206/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 207/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 208/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 209/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 210/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 211/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 212/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 213/300, Train Loss: 0.0005, Validation Loss: 0.0009\n",
            "Epoch 214/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 215/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 216/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 217/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 218/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 219/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 220/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 221/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 222/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 223/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 224/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 225/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 226/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 227/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 228/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 229/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 230/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 231/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 232/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 233/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 234/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 235/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 236/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 237/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 238/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 239/300, Train Loss: 0.0005, Validation Loss: 0.0016\n",
            "Epoch 240/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 241/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 242/300, Train Loss: 0.0005, Validation Loss: 0.0011\n",
            "Epoch 243/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 244/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 245/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 246/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 247/300, Train Loss: 0.0005, Validation Loss: 0.0009\n",
            "Epoch 248/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 249/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 250/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 251/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 252/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 253/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 254/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 255/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 256/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 257/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 258/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 259/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 260/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 261/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 262/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 263/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 264/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 265/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 266/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 267/300, Train Loss: 0.0005, Validation Loss: 0.0009\n",
            "Epoch 268/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 269/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 270/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 271/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 272/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 273/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 274/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 275/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 276/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 277/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 278/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 279/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 280/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 281/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 282/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 283/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 284/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 285/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 286/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 287/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 288/300, Train Loss: 0.0005, Validation Loss: 0.0011\n",
            "Epoch 289/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 290/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 291/300, Train Loss: 0.0005, Validation Loss: 0.0008\n",
            "Epoch 292/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 293/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 294/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 295/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 296/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 297/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 298/300, Train Loss: 0.0005, Validation Loss: 0.0006\n",
            "Epoch 299/300, Train Loss: 0.0005, Validation Loss: 0.0007\n",
            "Epoch 300/300, Train Loss: 0.0005, Validation Loss: 0.0007\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# best_hidden_sizes = best_params['hidden_sizes']\n",
        "# best_learning_rate = best_params['learning_rate']\n",
        "# best_batch_size = best_params['batch_size']\n",
        "\n",
        "best_hidden_sizes = best_params[32,64,32]\n",
        "best_learning_rate = best_params[0.001]\n",
        "best_batch_size = best_params[32]\n",
        "\n",
        "model = MLP(hidden_sizes=best_hidden_sizes).cuda()\n",
        "criterion = nn.MSELoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(300):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "        outputs = model(batch_X)\n",
        "\n",
        "        # 로그 값을 풀어준 후 MSE 계산\n",
        "        outputs_exp = torch.exp(outputs)\n",
        "        batch_y_exp = torch.exp(batch_y)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            # 로그 값을 풀어준 후 MSE 계산\n",
        "            outputs_exp = torch.exp(outputs)\n",
        "            batch_y_exp = torch.exp(batch_y)\n",
        "            loss = criterion(outputs_exp, batch_y_exp)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/300, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Validation Loss')\n",
        "plt.ylim(0, 10)  # y축 범위를 0에서 10으로 제한\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rPmSl253aWK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_csv_path = \"/content/drive/MyDrive/Colab Notebooks/data/samsung/test.csv\"\n",
        "\n",
        "X_test = pd.read_csv(test_csv_path).iloc[:,1:].values\n",
        "\n",
        "# 테스트 데이터에도 동일한 스케일링 적용\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
        "\n",
        "model_save_path = \"model_checkpoint_epoch500.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "model.eval()\n",
        "test_pred = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch_X = batch[0].cuda()\n",
        "        outputs = model(batch_X)\n",
        "        test_pred.extend(outputs.cpu().numpy())  # 예측값을 CPU로 이동하여 리스트에 추가\n",
        "\n",
        "test_pred = np.array(test_pred).flatten()\n",
        "test_pred_original_scale = np.expm1(test_pred)\n",
        "# 상위 10% 임계값 계산\n",
        "threshold = np.percentile(test_pred_original_scale, 90)\n",
        "top_10_percent_mask = test_pred_original_scale >= threshold\n",
        "from datetime import datetime\n",
        "\n",
        "# 현재 시간을 가져와서 문자열로 변환합니다.\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 파일 이름에 현재 시간을 포함시킵니다.\n",
        "submission_csv_path = f'/content/drive/MyDrive/Colab Notebooks/data/samsung/result/MLP_3_layers_{current_time}.csv'\n",
        "\n",
        "# 예측 결과를 저장하기 전에 디렉토리가 존재하는지 확인\n",
        "if not os.path.exists(os.path.dirname(submission_csv_path)):\n",
        "    os.makedirs(os.path.dirname(submission_csv_path))\n",
        "\n",
        "# 빈 데이터프레임 생성 (또는 제출 형식에 맞게 생성)\n",
        "submission_df = pd.DataFrame()\n",
        "submission_df['y'] = test_pred_original_scale  # 예측 결과를 y 컬럼에 추가\n",
        "\n",
        "# 지정된 경로에 CSV 파일로 저장\n",
        "submission_df.to_csv(submission_csv_path, index=False)"
      ],
      "metadata": {
        "id": "vsfRWDxSOccs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 로그 스케일을 원래 값으로 변환\n",
        "test_pred_original_scale_exp = np.exp(test_pred_original_scale)\n",
        "\n",
        "# 상위 10% 임계값 계산\n",
        "threshold = np.percentile(test_pred_original_scale, 90)\n",
        "top_10_percent_mask = test_pred_original_scale >= threshold\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"Top 10% threshold: {threshold:.4f}\")\n",
        "print(f\"Number of samples in top 10%: {sum(top_10_percent_mask)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3lv6-qghL5e",
        "outputId": "ed380a3a-d904-49ee-bd77-ebb4c8a2618a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10% threshold: 91.2451\n",
            "Number of samples in top 10%: 499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hO9WxZ02iLRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}