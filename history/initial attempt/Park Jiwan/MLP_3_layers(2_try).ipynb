{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "# Google Drive를 마운트합니다.\n",
    "drive.mount('/content/drive')\n",
    "train_csv_path = \"/content/drive/MyDrive/Samsung_AI_challenge/Data/train.csv\"\n",
    "\n",
    "RANDOM_SEED = 18\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 데이터 불러오기\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "X = train_data.iloc[:, 1:-1].values  # 특성 데이터\n",
    "y = train_data.iloc[:, -1].values    # 타겟 데이터\n",
    "\n",
    "# 1. 특성 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. 이상치 처리 (Z-Score를 사용하여 이상치 제거)\n",
    "z_scores = np.abs(stats.zscore(X_scaled))\n",
    "X_clean = X_scaled[(z_scores < 3).all(axis=1)]\n",
    "y_clean = y[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# 텐서로 변환\n",
    "X_tensor = torch.tensor(X_clean, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_clean, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 데이터셋 분할 (Train/Validation Split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=11, hidden_sizes=[64, 128, 64], output_size=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.fc4 = nn.Linear(hidden_sizes[2], output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# grid search 로 최적의 하이퍼파라미터 찾기\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64, 128, 64], [128, 256, 128], [32, 64, 32]],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'batch_size': [32, 64, 128],\n",
    "}\n",
    "\n",
    "def train_model(hidden_sizes, learning_rate, batch_size):\n",
    "    model = MLP(hidden_sizes=hidden_sizes)\n",
    "    model.cuda()\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 검증 성능 계산\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "best_params = None\n",
    "best_MSE = float('inf')\n",
    "for params in ParameterGrid(param_grid):\n",
    "    MSE = train_model(**params)\n",
    "    print(f\"Testing parameters: {params} => MSE: {MSE:.4f}\")\n",
    "    if MSE < best_MSE:\n",
    "        best_MSE = MSE\n",
    "        best_params = params\n",
    "\n",
    "print(f'\\nBest parameters: {best_params} with a MSE of {best_MSE:.4f}')\n",
    "with open('3_layer_MLP_params.json', 'w') as json_file:\n",
    "    json.dump(best_params, json_file)\n",
    "\n",
    "best_hidden_sizes = best_params['hidden_sizes']\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_batch_size = best_params['batch_size']\n",
    "\n",
    "model = MLP(hidden_sizes=best_hidden_sizes).cuda()\n",
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/300, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.ylim(0, 10)  # y축 범위를 0에서 10으로 제한\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 테스트 데이터 전처리 및 예측\n",
    "test_csv_path = \"/content/drive/MyDrive/Samsung_AI_challenge/Data/test.csv\"\n",
    "submission_csv_path = '/content/drive/MyDrive/Samsung_AI_challenge/Data/sample_submission.csv'\n",
    "X_test = pd.read_csv(test_csv_path).iloc[:,1:].values\n",
    "\n",
    "# 테스트 데이터에도 동일한 스케일링 적용\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "model_save_path = \"model_checkpoint_epoch500.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "model.eval()\n",
    "test_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch_X = batch[0].cuda()\n",
    "        outputs = model(batch_X)\n",
    "        test_pred.extend(outputs.cpu().numpy())  # 예측값을 CPU로 이동하여 리스트에 추가\n",
    "\n",
    "test_pred = np.array(test_pred)\n",
    "\n",
    "# 상위 10% 임계값 계산\n",
    "threshold = np.percentile(test_pred, 90)\n",
    "top_10_percent_mask = test_pred >= threshold\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission_df = pd.read_csv(submission_csv_path)\n",
    "submission_df['y'] = test_pred\n",
    "submission_df.to_csv('test_MLP.csv', index=False)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Top 10% threshold: {threshold:.4f}\")\n",
    "print(f\"Number of samples in top 10%: {sum(top_10_percent_mask)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
