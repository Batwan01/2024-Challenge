{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Batwan01/2024-Challenge/blob/main/history/24-8-31/MLP_Kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WyTS3xZLOm5",
        "outputId": "a6661783-92b2-47ec-bd11-ae33c5dbc5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 재현 가능성을 위한 시드 고정\n",
        "RANDOM_SEED = 18\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 경로 설정\n",
        "train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/contest/samsung/train.csv\"\n",
        "test_csv_path = \"/content/drive/MyDrive/Colab Notebooks/contest/samsung/test.csv\"\n",
        "submission_csv_path = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/sample_submission.csv'"
      ],
      "metadata": {
        "id": "gR5uEArVcQDT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP - 은닉층 5개\n",
        "# 현재 최고성능 모델 하이퍼파라미터(GS) : {\"batch_size\": 32, \"hidden_sizes\": [32, 64, 64, 32, 16], \"learning_rate\": 0.001}\n",
        "# Epoch : 93\n",
        "# 제출 성능 : 0.752\n",
        "class MLP5Hidden(nn.Module):\n",
        "    def __init__(self, input_size=22, hidden_sizes=[32, 64, 64, 32, 16], output_size=1):\n",
        "        super(MLP5Hidden, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.fc5 = nn.Linear(hidden_sizes[3], hidden_sizes[4])\n",
        "        self.fc6 = nn.Linear(hidden_sizes[4], output_size)\n",
        "        self.lrelu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.fc1(x))\n",
        "        x = self.lrelu(self.fc2(x))\n",
        "        x = self.lrelu(self.fc3(x))\n",
        "        x = self.lrelu(self.fc4(x))\n",
        "        x = self.lrelu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Sqc6M7YdLcAu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 잔차 연결\n",
        "class ResidualMLP(nn.Module):\n",
        "    def __init__(self, input_size=22, hidden_sizes=[32, 64, 64, 32, 16], output_size=1):\n",
        "        super(ResidualMLP, self).__init__()\n",
        "\n",
        "        # 6개의 MLP 레이어 정의\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.prelu1 = nn.PReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.prelu2 = nn.PReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.prelu3 = nn.PReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.prelu4 = nn.PReLU()\n",
        "\n",
        "        self.fc5 = nn.Linear(hidden_sizes[3], hidden_sizes[4])\n",
        "        self.prelu5 = nn.PReLU()\n",
        "\n",
        "        self.fc6 = nn.Linear(hidden_sizes[4], output_size)\n",
        "        self.prelu6 = nn.PReLU()\n",
        "\n",
        "        # 1x1 Linear 레이어로 Residual Connection의 크기 맞추기\n",
        "        self.residual1 = nn.Linear(hidden_sizes[0], hidden_sizes[2])  # fc1 -> fc3\n",
        "        self.residual2 = nn.Linear(hidden_sizes[2], hidden_sizes[4])  # fc3 -> fc5\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 첫 번째 MLP 레이어\n",
        "        out = self.prelu1(self.fc1(x))\n",
        "\n",
        "        # 첫 번째 Residual 연결 (fc1 -> fc2 -> fc3)\n",
        "        residual = self.residual1(out)  # 크기 맞추기\n",
        "        out = self.prelu2(self.fc2(out))\n",
        "        out = self.prelu3(self.fc3(out))\n",
        "        out += residual  # 첫 번째 Residual Connection\n",
        "\n",
        "        # 두 번째 Residual 연결 (fc3 -> fc4 -> fc5)\n",
        "        residual = self.residual2(out)  # 크기 맞추기\n",
        "        out = self.prelu4(self.fc4(out))\n",
        "        out = self.prelu5(self.fc5(out))\n",
        "        out += residual  # 두 번째 Residual Connection\n",
        "\n",
        "        # 최종 출력 레이어 (fc6) 및 PReLU 적용\n",
        "        out = self.prelu6(self.fc6(out))  # 최종 레이어에도 PReLU 적용\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZdHhzdrzF4Tu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터를 훈련 + 검증 데이터로 나누고 train_dataset, val_dataset 을 반환하는 함수:\n",
        "train_data = pd.read_csv(train_csv_path)\n",
        "\n",
        "X = torch.tensor(train_data.iloc[:,1:-1].values, dtype=torch.float32)\n",
        "y = torch.tensor(train_data.iloc[:,-1].values, dtype = torch.float32).view(-1,1)\n",
        "print(X.shape)\n",
        "\n",
        "# Square term 추가하기 (원본 X에 각 특징의 제곱값 추가)\n",
        "X_square = X ** 2\n",
        "X = torch.cat((X, X_square), dim=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "j4sKTEigLtD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b3b835-82ea-4b34-941a-a363b48b9c72"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40118, 11])\n",
            "torch.Size([32094, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# KFold 객체 생성\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "# K-Fold 교차 검증 수행\n",
        "fold = 1\n",
        "for train_index, val_index in kf.split(X):\n",
        "    print(f'Fold {fold}')\n",
        "\n",
        "    # 훈련/검증 데이터 분할\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, worker_init_fn=worker_init_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, worker_init_fn=worker_init_fn)\n",
        "\n",
        "    # 모델, 손실 함수, 옵티마이저 초기화\n",
        "    model = ResidualMLP(input_size=X_train.shape[1])\n",
        "    model.cuda()\n",
        "    criterion = nn.MSELoss().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # 훈련 루프\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(60):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/60, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    fold += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-yJ6eAtLxaP",
        "outputId": "3da19490-f0eb-4354-e0bb-fc548d112b98"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "Epoch 1/60, Train Loss: 225.3904, Validation Loss: 3.8880\n",
            "Epoch 2/60, Train Loss: 3.4511, Validation Loss: 3.7620\n",
            "Epoch 3/60, Train Loss: 3.4384, Validation Loss: 3.6996\n",
            "Epoch 4/60, Train Loss: 3.4008, Validation Loss: 3.8048\n",
            "Epoch 5/60, Train Loss: 3.3985, Validation Loss: 3.5892\n",
            "Epoch 6/60, Train Loss: 3.4335, Validation Loss: 3.7513\n",
            "Epoch 7/60, Train Loss: 3.4466, Validation Loss: 5.8041\n",
            "Epoch 8/60, Train Loss: 3.2484, Validation Loss: 3.4421\n",
            "Epoch 9/60, Train Loss: 3.2427, Validation Loss: 3.5823\n",
            "Epoch 10/60, Train Loss: 3.3528, Validation Loss: 3.9151\n",
            "Epoch 11/60, Train Loss: 3.1157, Validation Loss: 3.3261\n",
            "Epoch 12/60, Train Loss: 3.2093, Validation Loss: 3.3187\n",
            "Epoch 13/60, Train Loss: 3.2749, Validation Loss: 3.6540\n",
            "Epoch 14/60, Train Loss: 3.2192, Validation Loss: 3.4329\n",
            "Epoch 15/60, Train Loss: 3.1984, Validation Loss: 4.4100\n",
            "Epoch 16/60, Train Loss: 3.1124, Validation Loss: 3.4433\n",
            "Epoch 17/60, Train Loss: 3.1637, Validation Loss: 3.3485\n",
            "Epoch 18/60, Train Loss: 3.0983, Validation Loss: 3.7350\n",
            "Epoch 19/60, Train Loss: 3.2069, Validation Loss: 3.8013\n",
            "Epoch 20/60, Train Loss: 3.1047, Validation Loss: 3.4429\n",
            "Epoch 21/60, Train Loss: 3.1546, Validation Loss: 3.9341\n",
            "Epoch 22/60, Train Loss: 3.0845, Validation Loss: 3.3571\n",
            "Epoch 23/60, Train Loss: 3.0850, Validation Loss: 3.6052\n",
            "Epoch 24/60, Train Loss: 3.1059, Validation Loss: 4.0844\n",
            "Epoch 25/60, Train Loss: 3.0618, Validation Loss: 5.0156\n",
            "Epoch 26/60, Train Loss: 3.1127, Validation Loss: 3.3501\n",
            "Epoch 27/60, Train Loss: 3.0764, Validation Loss: 3.8346\n",
            "Epoch 28/60, Train Loss: 3.1934, Validation Loss: 3.2888\n",
            "Epoch 29/60, Train Loss: 3.0232, Validation Loss: 3.3902\n",
            "Epoch 30/60, Train Loss: 3.1154, Validation Loss: 3.3111\n",
            "Epoch 31/60, Train Loss: 3.0815, Validation Loss: 3.3078\n",
            "Epoch 32/60, Train Loss: 3.0775, Validation Loss: 3.8043\n",
            "Epoch 33/60, Train Loss: 3.1071, Validation Loss: 4.2150\n",
            "Epoch 34/60, Train Loss: 3.0357, Validation Loss: 3.3646\n",
            "Epoch 35/60, Train Loss: 3.0386, Validation Loss: 3.2502\n",
            "Epoch 36/60, Train Loss: 3.0870, Validation Loss: 4.0596\n",
            "Epoch 37/60, Train Loss: 3.0337, Validation Loss: 3.5360\n",
            "Epoch 38/60, Train Loss: 2.9767, Validation Loss: 3.3926\n",
            "Epoch 39/60, Train Loss: 3.0313, Validation Loss: 3.2631\n",
            "Epoch 40/60, Train Loss: 3.0467, Validation Loss: 3.3216\n",
            "Epoch 41/60, Train Loss: 3.0035, Validation Loss: 3.2236\n",
            "Epoch 42/60, Train Loss: 2.9851, Validation Loss: 3.2406\n",
            "Epoch 43/60, Train Loss: 3.0571, Validation Loss: 3.2226\n",
            "Epoch 44/60, Train Loss: 2.9764, Validation Loss: 3.3047\n",
            "Epoch 45/60, Train Loss: 3.0171, Validation Loss: 3.3742\n",
            "Epoch 46/60, Train Loss: 3.0231, Validation Loss: 3.2373\n",
            "Epoch 47/60, Train Loss: 2.9810, Validation Loss: 3.3175\n",
            "Epoch 48/60, Train Loss: 2.9782, Validation Loss: 4.7116\n",
            "Epoch 49/60, Train Loss: 2.9470, Validation Loss: 3.4322\n",
            "Epoch 50/60, Train Loss: 3.0032, Validation Loss: 3.6656\n",
            "Epoch 51/60, Train Loss: 3.0504, Validation Loss: 3.4575\n",
            "Epoch 52/60, Train Loss: 2.9338, Validation Loss: 4.4361\n",
            "Epoch 53/60, Train Loss: 2.9555, Validation Loss: 3.5589\n",
            "Epoch 54/60, Train Loss: 2.9681, Validation Loss: 3.4941\n",
            "Epoch 55/60, Train Loss: 2.9474, Validation Loss: 3.2485\n",
            "Epoch 56/60, Train Loss: 2.9704, Validation Loss: 3.2524\n",
            "Epoch 57/60, Train Loss: 2.9081, Validation Loss: 3.7213\n",
            "Epoch 58/60, Train Loss: 2.9612, Validation Loss: 3.6989\n",
            "Epoch 59/60, Train Loss: 2.9521, Validation Loss: 3.3424\n",
            "Epoch 60/60, Train Loss: 2.9116, Validation Loss: 3.2339\n",
            "Fold 2\n",
            "Epoch 1/60, Train Loss: 260.4157, Validation Loss: 6.0989\n",
            "Epoch 2/60, Train Loss: 5.0640, Validation Loss: 3.7794\n",
            "Epoch 3/60, Train Loss: 3.7198, Validation Loss: 4.1722\n",
            "Epoch 4/60, Train Loss: 3.5188, Validation Loss: 3.4228\n",
            "Epoch 5/60, Train Loss: 3.5350, Validation Loss: 3.8451\n",
            "Epoch 6/60, Train Loss: 3.5671, Validation Loss: 3.0726\n",
            "Epoch 7/60, Train Loss: 3.6098, Validation Loss: 3.1567\n",
            "Epoch 8/60, Train Loss: 3.4376, Validation Loss: 2.8851\n",
            "Epoch 9/60, Train Loss: 3.5306, Validation Loss: 2.8790\n",
            "Epoch 10/60, Train Loss: 3.5080, Validation Loss: 3.7713\n",
            "Epoch 11/60, Train Loss: 3.4872, Validation Loss: 3.9460\n",
            "Epoch 12/60, Train Loss: 3.3547, Validation Loss: 2.8158\n",
            "Epoch 13/60, Train Loss: 3.4128, Validation Loss: 2.8078\n",
            "Epoch 14/60, Train Loss: 3.3534, Validation Loss: 2.8334\n",
            "Epoch 15/60, Train Loss: 3.3428, Validation Loss: 2.7194\n",
            "Epoch 16/60, Train Loss: 3.2816, Validation Loss: 2.7147\n",
            "Epoch 17/60, Train Loss: 3.3099, Validation Loss: 3.3411\n",
            "Epoch 18/60, Train Loss: 3.2359, Validation Loss: 3.0764\n",
            "Epoch 19/60, Train Loss: 3.2464, Validation Loss: 2.7109\n",
            "Epoch 20/60, Train Loss: 3.2953, Validation Loss: 2.9707\n",
            "Epoch 21/60, Train Loss: 3.2374, Validation Loss: 3.0657\n",
            "Epoch 22/60, Train Loss: 3.2269, Validation Loss: 2.7264\n",
            "Epoch 23/60, Train Loss: 3.3064, Validation Loss: 2.7279\n",
            "Epoch 24/60, Train Loss: 3.2542, Validation Loss: 2.6633\n",
            "Epoch 25/60, Train Loss: 3.2170, Validation Loss: 4.0940\n",
            "Epoch 26/60, Train Loss: 3.2422, Validation Loss: 3.7489\n",
            "Epoch 27/60, Train Loss: 3.2006, Validation Loss: 2.6705\n",
            "Epoch 28/60, Train Loss: 3.2241, Validation Loss: 2.6772\n",
            "Epoch 29/60, Train Loss: 3.1385, Validation Loss: 2.7024\n",
            "Epoch 30/60, Train Loss: 3.2035, Validation Loss: 3.2624\n",
            "Epoch 31/60, Train Loss: 3.1821, Validation Loss: 2.6611\n",
            "Epoch 32/60, Train Loss: 3.1684, Validation Loss: 3.0928\n",
            "Epoch 33/60, Train Loss: 3.2417, Validation Loss: 2.6482\n",
            "Epoch 34/60, Train Loss: 3.1906, Validation Loss: 2.7578\n",
            "Epoch 35/60, Train Loss: 3.1972, Validation Loss: 3.4041\n",
            "Epoch 36/60, Train Loss: 3.1464, Validation Loss: 3.2210\n",
            "Epoch 37/60, Train Loss: 3.2442, Validation Loss: 3.8929\n",
            "Epoch 38/60, Train Loss: 3.1194, Validation Loss: 3.0166\n",
            "Epoch 39/60, Train Loss: 3.1096, Validation Loss: 2.6772\n",
            "Epoch 40/60, Train Loss: 3.1964, Validation Loss: 2.8604\n",
            "Epoch 41/60, Train Loss: 3.1295, Validation Loss: 2.7003\n",
            "Epoch 42/60, Train Loss: 3.1595, Validation Loss: 2.7203\n",
            "Epoch 43/60, Train Loss: 3.1382, Validation Loss: 2.7166\n",
            "Epoch 44/60, Train Loss: 3.1273, Validation Loss: 2.7539\n",
            "Epoch 45/60, Train Loss: 3.1728, Validation Loss: 2.7051\n",
            "Epoch 46/60, Train Loss: 3.1052, Validation Loss: 2.6459\n",
            "Epoch 47/60, Train Loss: 3.1568, Validation Loss: 2.7882\n",
            "Epoch 48/60, Train Loss: 3.1033, Validation Loss: 2.9260\n",
            "Epoch 49/60, Train Loss: 3.1245, Validation Loss: 3.0860\n",
            "Epoch 50/60, Train Loss: 3.1063, Validation Loss: 2.6949\n",
            "Epoch 51/60, Train Loss: 3.1614, Validation Loss: 2.6910\n",
            "Epoch 52/60, Train Loss: 3.1198, Validation Loss: 3.3193\n",
            "Epoch 53/60, Train Loss: 3.1227, Validation Loss: 2.6426\n",
            "Epoch 54/60, Train Loss: 3.1165, Validation Loss: 2.8163\n",
            "Epoch 55/60, Train Loss: 3.0750, Validation Loss: 2.6442\n",
            "Epoch 56/60, Train Loss: 3.1119, Validation Loss: 2.6589\n",
            "Epoch 57/60, Train Loss: 3.1575, Validation Loss: 2.6773\n",
            "Epoch 58/60, Train Loss: 3.0944, Validation Loss: 3.0214\n",
            "Epoch 59/60, Train Loss: 3.1403, Validation Loss: 3.1910\n",
            "Epoch 60/60, Train Loss: 3.1124, Validation Loss: 2.9099\n",
            "Fold 3\n",
            "Epoch 1/60, Train Loss: 228.8777, Validation Loss: 4.2074\n",
            "Epoch 2/60, Train Loss: 4.0387, Validation Loss: 2.9406\n",
            "Epoch 3/60, Train Loss: 3.5378, Validation Loss: 2.7145\n",
            "Epoch 4/60, Train Loss: 3.5100, Validation Loss: 2.7428\n",
            "Epoch 5/60, Train Loss: 3.5593, Validation Loss: 3.4828\n",
            "Epoch 6/60, Train Loss: 3.4915, Validation Loss: 6.1053\n",
            "Epoch 7/60, Train Loss: 3.6232, Validation Loss: 2.9407\n",
            "Epoch 8/60, Train Loss: 3.4566, Validation Loss: 2.6046\n",
            "Epoch 9/60, Train Loss: 3.4299, Validation Loss: 2.6976\n",
            "Epoch 10/60, Train Loss: 3.4714, Validation Loss: 2.6972\n",
            "Epoch 11/60, Train Loss: 3.3778, Validation Loss: 2.7693\n",
            "Epoch 12/60, Train Loss: 3.4221, Validation Loss: 2.9169\n",
            "Epoch 13/60, Train Loss: 3.3990, Validation Loss: 2.5488\n",
            "Epoch 14/60, Train Loss: 3.3717, Validation Loss: 4.6891\n",
            "Epoch 15/60, Train Loss: 3.4333, Validation Loss: 3.1806\n",
            "Epoch 16/60, Train Loss: 3.3633, Validation Loss: 2.6496\n",
            "Epoch 17/60, Train Loss: 3.3531, Validation Loss: 2.9029\n",
            "Epoch 18/60, Train Loss: 3.3085, Validation Loss: 3.8362\n",
            "Epoch 19/60, Train Loss: 3.3874, Validation Loss: 4.0574\n",
            "Epoch 20/60, Train Loss: 3.3650, Validation Loss: 2.5054\n",
            "Epoch 21/60, Train Loss: 3.3590, Validation Loss: 2.5169\n",
            "Epoch 22/60, Train Loss: 3.3756, Validation Loss: 2.9899\n",
            "Epoch 23/60, Train Loss: 3.2826, Validation Loss: 3.3184\n",
            "Epoch 24/60, Train Loss: 3.3356, Validation Loss: 3.1207\n",
            "Epoch 25/60, Train Loss: 3.2137, Validation Loss: 2.6030\n",
            "Epoch 26/60, Train Loss: 3.2834, Validation Loss: 3.2574\n",
            "Epoch 27/60, Train Loss: 3.1925, Validation Loss: 3.3094\n",
            "Epoch 28/60, Train Loss: 3.2996, Validation Loss: 2.6365\n",
            "Epoch 29/60, Train Loss: 3.2010, Validation Loss: 2.5004\n",
            "Epoch 30/60, Train Loss: 3.3337, Validation Loss: 2.4901\n",
            "Epoch 31/60, Train Loss: 3.2266, Validation Loss: 2.5835\n",
            "Epoch 32/60, Train Loss: 3.2871, Validation Loss: 2.6718\n",
            "Epoch 33/60, Train Loss: 3.1883, Validation Loss: 2.5929\n",
            "Epoch 34/60, Train Loss: 3.1809, Validation Loss: 2.4643\n",
            "Epoch 35/60, Train Loss: 3.1903, Validation Loss: 2.4779\n",
            "Epoch 36/60, Train Loss: 3.1672, Validation Loss: 2.7764\n",
            "Epoch 37/60, Train Loss: 3.1946, Validation Loss: 3.6168\n",
            "Epoch 38/60, Train Loss: 3.2121, Validation Loss: 2.9270\n",
            "Epoch 39/60, Train Loss: 3.1678, Validation Loss: 7.4643\n",
            "Epoch 40/60, Train Loss: 3.2465, Validation Loss: 2.4641\n",
            "Epoch 41/60, Train Loss: 3.1997, Validation Loss: 2.5518\n",
            "Epoch 42/60, Train Loss: 3.1874, Validation Loss: 2.5521\n",
            "Epoch 43/60, Train Loss: 3.2106, Validation Loss: 2.4873\n",
            "Epoch 44/60, Train Loss: 3.1735, Validation Loss: 2.8660\n",
            "Epoch 45/60, Train Loss: 3.1735, Validation Loss: 2.5558\n",
            "Epoch 46/60, Train Loss: 3.1727, Validation Loss: 2.4885\n",
            "Epoch 47/60, Train Loss: 3.1493, Validation Loss: 2.8740\n",
            "Epoch 48/60, Train Loss: 3.1774, Validation Loss: 2.5526\n",
            "Epoch 49/60, Train Loss: 3.1998, Validation Loss: 2.4504\n",
            "Epoch 50/60, Train Loss: 3.1678, Validation Loss: 2.4557\n",
            "Epoch 51/60, Train Loss: 3.1782, Validation Loss: 2.6978\n",
            "Epoch 52/60, Train Loss: 3.1588, Validation Loss: 2.5976\n",
            "Epoch 53/60, Train Loss: 3.1528, Validation Loss: 2.5243\n",
            "Epoch 54/60, Train Loss: 3.1732, Validation Loss: 2.7234\n",
            "Epoch 55/60, Train Loss: 3.1949, Validation Loss: 2.6613\n",
            "Epoch 56/60, Train Loss: 3.1569, Validation Loss: 2.8304\n",
            "Epoch 57/60, Train Loss: 3.1671, Validation Loss: 2.7008\n",
            "Epoch 58/60, Train Loss: 3.1074, Validation Loss: 2.6021\n",
            "Epoch 59/60, Train Loss: 3.1170, Validation Loss: 2.4690\n",
            "Epoch 60/60, Train Loss: 3.0924, Validation Loss: 2.4717\n",
            "Fold 4\n",
            "Epoch 1/60, Train Loss: 227.4923, Validation Loss: 5.6661\n",
            "Epoch 2/60, Train Loss: 4.0158, Validation Loss: 3.7642\n",
            "Epoch 3/60, Train Loss: 3.3243, Validation Loss: 3.4155\n",
            "Epoch 4/60, Train Loss: 3.3300, Validation Loss: 3.2416\n",
            "Epoch 5/60, Train Loss: 3.3200, Validation Loss: 3.3160\n",
            "Epoch 6/60, Train Loss: 3.3229, Validation Loss: 3.6600\n",
            "Epoch 7/60, Train Loss: 3.3135, Validation Loss: 4.8987\n",
            "Epoch 8/60, Train Loss: 3.3377, Validation Loss: 3.5507\n",
            "Epoch 9/60, Train Loss: 3.2186, Validation Loss: 4.7996\n",
            "Epoch 10/60, Train Loss: 3.2605, Validation Loss: 3.2736\n",
            "Epoch 11/60, Train Loss: 3.2303, Validation Loss: 3.4840\n",
            "Epoch 12/60, Train Loss: 3.2293, Validation Loss: 3.5019\n",
            "Epoch 13/60, Train Loss: 3.1448, Validation Loss: 3.3439\n",
            "Epoch 14/60, Train Loss: 3.1996, Validation Loss: 3.4241\n",
            "Epoch 15/60, Train Loss: 3.2997, Validation Loss: 3.5116\n",
            "Epoch 16/60, Train Loss: 3.0859, Validation Loss: 3.7940\n",
            "Epoch 17/60, Train Loss: 3.2066, Validation Loss: 3.1312\n",
            "Epoch 18/60, Train Loss: 3.1745, Validation Loss: 3.3675\n",
            "Epoch 19/60, Train Loss: 3.2812, Validation Loss: 3.1276\n",
            "Epoch 20/60, Train Loss: 3.0931, Validation Loss: 4.2548\n",
            "Epoch 21/60, Train Loss: 3.1608, Validation Loss: 3.2381\n",
            "Epoch 22/60, Train Loss: 3.1340, Validation Loss: 3.8181\n",
            "Epoch 23/60, Train Loss: 3.1367, Validation Loss: 3.2847\n",
            "Epoch 24/60, Train Loss: 3.1106, Validation Loss: 3.9836\n",
            "Epoch 25/60, Train Loss: 3.1498, Validation Loss: 3.1196\n",
            "Epoch 26/60, Train Loss: 3.0941, Validation Loss: 5.6925\n",
            "Epoch 27/60, Train Loss: 3.1151, Validation Loss: 3.1152\n",
            "Epoch 28/60, Train Loss: 3.1121, Validation Loss: 3.2567\n",
            "Epoch 29/60, Train Loss: 3.1544, Validation Loss: 3.1322\n",
            "Epoch 30/60, Train Loss: 3.0687, Validation Loss: 3.7694\n",
            "Epoch 31/60, Train Loss: 3.1165, Validation Loss: 3.1024\n",
            "Epoch 32/60, Train Loss: 3.0268, Validation Loss: 3.5085\n",
            "Epoch 33/60, Train Loss: 3.0441, Validation Loss: 3.3487\n",
            "Epoch 34/60, Train Loss: 3.0803, Validation Loss: 3.2069\n",
            "Epoch 35/60, Train Loss: 3.0761, Validation Loss: 4.0001\n",
            "Epoch 36/60, Train Loss: 3.0732, Validation Loss: 3.4211\n",
            "Epoch 37/60, Train Loss: 3.0737, Validation Loss: 3.2258\n",
            "Epoch 38/60, Train Loss: 3.0327, Validation Loss: 3.4715\n",
            "Epoch 39/60, Train Loss: 3.0837, Validation Loss: 3.7342\n",
            "Epoch 40/60, Train Loss: 3.0441, Validation Loss: 3.8093\n",
            "Epoch 41/60, Train Loss: 3.0809, Validation Loss: 3.7445\n",
            "Epoch 42/60, Train Loss: 3.0756, Validation Loss: 3.5791\n",
            "Epoch 43/60, Train Loss: 3.0423, Validation Loss: 4.2501\n",
            "Epoch 44/60, Train Loss: 3.0274, Validation Loss: 3.1290\n",
            "Epoch 45/60, Train Loss: 3.0114, Validation Loss: 3.4220\n",
            "Epoch 46/60, Train Loss: 3.0192, Validation Loss: 3.1244\n",
            "Epoch 47/60, Train Loss: 3.0349, Validation Loss: 4.2057\n",
            "Epoch 48/60, Train Loss: 3.1157, Validation Loss: 3.8518\n",
            "Epoch 49/60, Train Loss: 3.0371, Validation Loss: 3.1538\n",
            "Epoch 50/60, Train Loss: 3.0242, Validation Loss: 3.6841\n",
            "Epoch 51/60, Train Loss: 2.9754, Validation Loss: 3.1517\n",
            "Epoch 52/60, Train Loss: 3.0161, Validation Loss: 3.1725\n",
            "Epoch 53/60, Train Loss: 3.0091, Validation Loss: 3.1953\n",
            "Epoch 54/60, Train Loss: 2.9739, Validation Loss: 3.2091\n",
            "Epoch 55/60, Train Loss: 3.0199, Validation Loss: 3.2047\n",
            "Epoch 56/60, Train Loss: 2.9775, Validation Loss: 4.1143\n",
            "Epoch 57/60, Train Loss: 2.9781, Validation Loss: 3.6549\n",
            "Epoch 58/60, Train Loss: 3.0314, Validation Loss: 3.7598\n",
            "Epoch 59/60, Train Loss: 2.9314, Validation Loss: 3.1182\n",
            "Epoch 60/60, Train Loss: 3.0839, Validation Loss: 3.2647\n",
            "Fold 5\n",
            "Epoch 1/60, Train Loss: 244.3528, Validation Loss: 3.8991\n",
            "Epoch 2/60, Train Loss: 3.8576, Validation Loss: 3.3660\n",
            "Epoch 3/60, Train Loss: 3.5453, Validation Loss: 2.9811\n",
            "Epoch 4/60, Train Loss: 3.5915, Validation Loss: 2.9657\n",
            "Epoch 5/60, Train Loss: 3.6114, Validation Loss: 3.2960\n",
            "Epoch 6/60, Train Loss: 3.5106, Validation Loss: 3.7079\n",
            "Epoch 7/60, Train Loss: 3.4670, Validation Loss: 3.1796\n",
            "Epoch 8/60, Train Loss: 3.3924, Validation Loss: 3.5711\n",
            "Epoch 9/60, Train Loss: 3.4527, Validation Loss: 4.2939\n",
            "Epoch 10/60, Train Loss: 3.4549, Validation Loss: 3.9024\n",
            "Epoch 11/60, Train Loss: 3.5414, Validation Loss: 2.7499\n",
            "Epoch 12/60, Train Loss: 3.3741, Validation Loss: 2.7529\n",
            "Epoch 13/60, Train Loss: 3.3444, Validation Loss: 4.2775\n",
            "Epoch 14/60, Train Loss: 3.4024, Validation Loss: 3.0785\n",
            "Epoch 15/60, Train Loss: 3.3744, Validation Loss: 3.4559\n",
            "Epoch 16/60, Train Loss: 3.2819, Validation Loss: 3.1908\n",
            "Epoch 17/60, Train Loss: 3.3059, Validation Loss: 2.7616\n",
            "Epoch 18/60, Train Loss: 3.4334, Validation Loss: 2.7459\n",
            "Epoch 19/60, Train Loss: 3.2747, Validation Loss: 3.5934\n",
            "Epoch 20/60, Train Loss: 3.2908, Validation Loss: 3.2760\n",
            "Epoch 21/60, Train Loss: 3.2517, Validation Loss: 2.9353\n",
            "Epoch 22/60, Train Loss: 3.4072, Validation Loss: 3.1355\n",
            "Epoch 23/60, Train Loss: 3.2531, Validation Loss: 2.8205\n",
            "Epoch 24/60, Train Loss: 3.3420, Validation Loss: 2.9205\n",
            "Epoch 25/60, Train Loss: 3.2600, Validation Loss: 2.7456\n",
            "Epoch 26/60, Train Loss: 3.2367, Validation Loss: 2.7716\n",
            "Epoch 27/60, Train Loss: 3.2051, Validation Loss: 3.2969\n",
            "Epoch 28/60, Train Loss: 3.2562, Validation Loss: 3.1116\n",
            "Epoch 29/60, Train Loss: 3.2304, Validation Loss: 3.4861\n",
            "Epoch 30/60, Train Loss: 3.3010, Validation Loss: 3.6677\n",
            "Epoch 31/60, Train Loss: 3.2417, Validation Loss: 3.1507\n",
            "Epoch 32/60, Train Loss: 3.1775, Validation Loss: 3.1009\n",
            "Epoch 33/60, Train Loss: 3.2000, Validation Loss: 2.6310\n",
            "Epoch 34/60, Train Loss: 3.1387, Validation Loss: 2.7584\n",
            "Epoch 35/60, Train Loss: 3.1837, Validation Loss: 2.8133\n",
            "Epoch 36/60, Train Loss: 3.2473, Validation Loss: 2.6713\n",
            "Epoch 37/60, Train Loss: 3.1813, Validation Loss: 4.3703\n",
            "Epoch 38/60, Train Loss: 3.1495, Validation Loss: 2.7066\n",
            "Epoch 39/60, Train Loss: 3.1633, Validation Loss: 3.1874\n",
            "Epoch 40/60, Train Loss: 3.1412, Validation Loss: 3.5096\n",
            "Epoch 41/60, Train Loss: 3.1739, Validation Loss: 3.1302\n",
            "Epoch 42/60, Train Loss: 3.1252, Validation Loss: 2.7365\n",
            "Epoch 43/60, Train Loss: 3.1379, Validation Loss: 2.6961\n",
            "Epoch 44/60, Train Loss: 3.1265, Validation Loss: 5.3798\n",
            "Epoch 45/60, Train Loss: 3.1734, Validation Loss: 2.6603\n",
            "Epoch 46/60, Train Loss: 3.1492, Validation Loss: 2.7919\n",
            "Epoch 47/60, Train Loss: 3.1267, Validation Loss: 2.6544\n",
            "Epoch 48/60, Train Loss: 3.1872, Validation Loss: 2.6392\n",
            "Epoch 49/60, Train Loss: 3.1756, Validation Loss: 5.1961\n",
            "Epoch 50/60, Train Loss: 3.1612, Validation Loss: 3.2458\n",
            "Epoch 51/60, Train Loss: 3.0960, Validation Loss: 3.2098\n",
            "Epoch 52/60, Train Loss: 3.1135, Validation Loss: 2.8286\n",
            "Epoch 53/60, Train Loss: 3.1385, Validation Loss: 2.7161\n",
            "Epoch 54/60, Train Loss: 3.1464, Validation Loss: 3.5924\n",
            "Epoch 55/60, Train Loss: 3.0876, Validation Loss: 2.9234\n",
            "Epoch 56/60, Train Loss: 3.1463, Validation Loss: 3.0960\n",
            "Epoch 57/60, Train Loss: 3.0917, Validation Loss: 2.8366\n",
            "Epoch 58/60, Train Loss: 3.0469, Validation Loss: 2.6363\n",
            "Epoch 59/60, Train Loss: 3.1388, Validation Loss: 2.6724\n",
            "Epoch 60/60, Train Loss: 3.1100, Validation Loss: 3.8589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(test_csv_path).iloc[:,1:]\n",
        "\n",
        "# Square term 추가하기\n",
        "X_test_square = X_test ** 2\n",
        "X_test = pd.concat([X_test, X_test_square], axis=1)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_pred = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch_X = batch[0].cuda()\n",
        "        outputs = model(batch_X)\n",
        "        test_pred.extend(outputs.cpu().numpy())  # 예측값을 CPU로 이동하여 리스트에 추가\n",
        "\n",
        "test_pred = np.array(test_pred).flatten()\n",
        "\n",
        "submission_df = pd.read_csv(submission_csv_path)\n",
        "submission_df['y'] = test_pred  # 예측 결과를 y 컬럼에 추가\n",
        "\n",
        "# 지정된 경로에 CSV 파일로 저장\n",
        "submission_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/contest/samsung/result/MLP_square_term_Kfold.csv\", index=False)"
      ],
      "metadata": {
        "id": "c79qIcAnMFhu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "FOvlWwAbRiAt",
        "outputId": "c35247c4-d96a-46c0-bb24-38f55f2fb202"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 y\n",
              "count  4986.000000\n",
              "mean     85.929970\n",
              "std       3.494783\n",
              "min      82.263710\n",
              "25%      83.408710\n",
              "50%      84.286884\n",
              "75%      87.501263\n",
              "max      95.067207"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7521d2eb-735d-4a47-8bbb-bc87db91199e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4986.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>85.929970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.494783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>82.263710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>83.408710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84.286884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>87.501263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>95.067207</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7521d2eb-735d-4a47-8bbb-bc87db91199e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7521d2eb-735d-4a47-8bbb-bc87db91199e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7521d2eb-735d-4a47-8bbb-bc87db91199e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8f20ebfe-ecdd-4d26-962d-233331b5d8d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f20ebfe-ecdd-4d26-962d-233331b5d8d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8f20ebfe-ecdd-4d26-962d-233331b5d8d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"submission_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1736.7014052341817,\n        \"min\": 3.494783401489258,\n        \"max\": 4986.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          85.92996978759766,\n          84.28688430786133,\n          4986.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def find_non_matching_ids(file1, file2):\n",
        "    # 두 개의 CSV 파일을 읽어옴\n",
        "    df1 = pd.read_csv(file1)\n",
        "    df2 = pd.read_csv(file2)\n",
        "\n",
        "    # y 값 기준으로 내림차순 정렬\n",
        "    df1_sorted = df1.sort_values(by='y', ascending=False)\n",
        "    df2_sorted = df2.sort_values(by='y', ascending=False)\n",
        "\n",
        "    # file1의 상위 10% 항목 계산\n",
        "    top_10_percent_count = int(len(df1_sorted) * 0.1)\n",
        "    top_10_percent_ids_df1 = set(df1_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file2의 상위 10% ID 추출\n",
        "    top_10_percent_ids_df2 = set(df2_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file1의 상위 10% 중 file2의 상위 10%에 없는 ID 계산\n",
        "    non_matching_ids = top_10_percent_ids_df1 - top_10_percent_ids_df2\n",
        "    num_non_matching = len(non_matching_ids)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"file1의 상위 10% 항목 개수: {top_10_percent_count}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 개수: {num_non_matching}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 ID: {non_matching_ids}\")\n",
        "\n",
        "    return top_10_percent_count, num_non_matching, list(non_matching_ids)\n",
        "\n",
        "# 사용 예시\n",
        "file1 ='/content/drive/MyDrive/Colab Notebooks/contest/samsung/MLP_Residual_Connection_18.csv' # best 성능 파일\n",
        "file2 = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/result/MLP_square_term_Kfold.csv' # 측정하고자 하는 파일\n",
        "top_10_percent_count, num_non_matching, non_matching_ids = find_non_matching_ids(file1, file2)\n"
      ],
      "metadata": {
        "id": "LLAeVE6OIk1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3190544a-ff09-4c3e-abef-45a9b55d0347"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file1의 상위 10% 항목 개수: 498\n",
            "file1의 상위 10% 중 file2에 없는 항목 개수: 11\n",
            "file1의 상위 10% 중 file2에 없는 항목 ID: {'TEST_3811', 'TEST_1321', 'TEST_1178', 'TEST_3172', 'TEST_0784', 'TEST_0037', 'TEST_1309', 'TEST_1478', 'TEST_1502', 'TEST_2275', 'TEST_4318'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARZY2gWZkBg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}