{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0bXIZvuAr+deZy45lxeMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Batwan01/2024-Challenge/blob/main/history/24-9-11/claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU4c-Donz5F5",
        "outputId": "47412529-efe5-4a72-aeac-ed5b00e19757"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLPLXPGdzuqY",
        "outputId": "d7ad26d8-2688-4e82-f8bb-d53de424edd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Average Loss: 0.0012\n",
            "Epoch [2/100], Average Loss: 0.0004\n",
            "Epoch [3/100], Average Loss: 0.0004\n",
            "Epoch [4/100], Average Loss: 0.0003\n",
            "Epoch [5/100], Average Loss: 0.0003\n",
            "Epoch [6/100], Average Loss: 0.0003\n",
            "Epoch [7/100], Average Loss: 0.0003\n",
            "Epoch [8/100], Average Loss: 0.0003\n",
            "Epoch [9/100], Average Loss: 0.0003\n",
            "Epoch [10/100], Average Loss: 0.0003\n",
            "Epoch [11/100], Average Loss: 0.0003\n",
            "Epoch [12/100], Average Loss: 0.0003\n",
            "Epoch [13/100], Average Loss: 0.0003\n",
            "Epoch [14/100], Average Loss: 0.0003\n",
            "Epoch [15/100], Average Loss: 0.0003\n",
            "Epoch [16/100], Average Loss: 0.0003\n",
            "Epoch [17/100], Average Loss: 0.0003\n",
            "Epoch [18/100], Average Loss: 0.0003\n",
            "Epoch [19/100], Average Loss: 0.0003\n",
            "Epoch [20/100], Average Loss: 0.0003\n",
            "Epoch [21/100], Average Loss: 0.0003\n",
            "Epoch [22/100], Average Loss: 0.0003\n",
            "Epoch [23/100], Average Loss: 0.0003\n",
            "Epoch [24/100], Average Loss: 0.0003\n",
            "Epoch [25/100], Average Loss: 0.0003\n",
            "Epoch [26/100], Average Loss: 0.0003\n",
            "Epoch [27/100], Average Loss: 0.0003\n",
            "Epoch [28/100], Average Loss: 0.0003\n",
            "Epoch [29/100], Average Loss: 0.0003\n",
            "Epoch [30/100], Average Loss: 0.0003\n",
            "Epoch [31/100], Average Loss: 0.0003\n",
            "Epoch [32/100], Average Loss: 0.0003\n",
            "Epoch [33/100], Average Loss: 0.0003\n",
            "Epoch [34/100], Average Loss: 0.0003\n",
            "Epoch [35/100], Average Loss: 0.0003\n",
            "Epoch [36/100], Average Loss: 0.0003\n",
            "Epoch [37/100], Average Loss: 0.0003\n",
            "Epoch [38/100], Average Loss: 0.0003\n",
            "Epoch [39/100], Average Loss: 0.0003\n",
            "Epoch [40/100], Average Loss: 0.0003\n",
            "Epoch [41/100], Average Loss: 0.0003\n",
            "Epoch [42/100], Average Loss: 0.0003\n",
            "Epoch [43/100], Average Loss: 0.0003\n",
            "Epoch [44/100], Average Loss: 0.0003\n",
            "Epoch [45/100], Average Loss: 0.0003\n",
            "Epoch [46/100], Average Loss: 0.0003\n",
            "Epoch [47/100], Average Loss: 0.0003\n",
            "Epoch [48/100], Average Loss: 0.0003\n",
            "Epoch [49/100], Average Loss: 0.0003\n",
            "Epoch [50/100], Average Loss: 0.0003\n",
            "Epoch [51/100], Average Loss: 0.0003\n",
            "Epoch [52/100], Average Loss: 0.0003\n",
            "Epoch [53/100], Average Loss: 0.0003\n",
            "Epoch [54/100], Average Loss: 0.0003\n",
            "Epoch [55/100], Average Loss: 0.0003\n",
            "Epoch [56/100], Average Loss: 0.0003\n",
            "Epoch [57/100], Average Loss: 0.0003\n",
            "Epoch [58/100], Average Loss: 0.0003\n",
            "Epoch [59/100], Average Loss: 0.0003\n",
            "Epoch [60/100], Average Loss: 0.0003\n",
            "Epoch [61/100], Average Loss: 0.0003\n",
            "Epoch [62/100], Average Loss: 0.0003\n",
            "Epoch [63/100], Average Loss: 0.0003\n",
            "Epoch [64/100], Average Loss: 0.0003\n",
            "Epoch [65/100], Average Loss: 0.0003\n",
            "Epoch [66/100], Average Loss: 0.0003\n",
            "Epoch [67/100], Average Loss: 0.0003\n",
            "Epoch [68/100], Average Loss: 0.0003\n",
            "Epoch [69/100], Average Loss: 0.0003\n",
            "Epoch [70/100], Average Loss: 0.0003\n",
            "Epoch [71/100], Average Loss: 0.0003\n",
            "Epoch [72/100], Average Loss: 0.0003\n",
            "Epoch [73/100], Average Loss: 0.0003\n",
            "Epoch [74/100], Average Loss: 0.0003\n",
            "Epoch [75/100], Average Loss: 0.0003\n",
            "Epoch [76/100], Average Loss: 0.0003\n",
            "Epoch [77/100], Average Loss: 0.0003\n",
            "Epoch [78/100], Average Loss: 0.0003\n",
            "Epoch [79/100], Average Loss: 0.0003\n",
            "Epoch [80/100], Average Loss: 0.0003\n",
            "Epoch [81/100], Average Loss: 0.0003\n",
            "Epoch [82/100], Average Loss: 0.0003\n",
            "Epoch [83/100], Average Loss: 0.0003\n",
            "Epoch [84/100], Average Loss: 0.0003\n",
            "Epoch [85/100], Average Loss: 0.0003\n",
            "Epoch [86/100], Average Loss: 0.0003\n",
            "Epoch [87/100], Average Loss: 0.0003\n",
            "Epoch [88/100], Average Loss: 0.0003\n",
            "Epoch [89/100], Average Loss: 0.0003\n",
            "Epoch [90/100], Average Loss: 0.0003\n",
            "Epoch [91/100], Average Loss: 0.0003\n",
            "Epoch [92/100], Average Loss: 0.0003\n",
            "Epoch [93/100], Average Loss: 0.0003\n",
            "Epoch [94/100], Average Loss: 0.0003\n",
            "Epoch [95/100], Average Loss: 0.0003\n",
            "Epoch [96/100], Average Loss: 0.0003\n",
            "Epoch [97/100], Average Loss: 0.0003\n",
            "Epoch [98/100], Average Loss: 0.0003\n",
            "Epoch [99/100], Average Loss: 0.0003\n",
            "Epoch [100/100], Average Loss: 0.0003\n",
            "Training completed!\n",
            "Model saved!\n",
            "0       83.397293\n",
            "1       82.072411\n",
            "2       90.044044\n",
            "3       90.798225\n",
            "4       82.193550\n",
            "          ...    \n",
            "4981    83.234154\n",
            "4982    90.747437\n",
            "4983    82.792519\n",
            "4984    82.837906\n",
            "4985    82.318443\n",
            "Name: y, Length: 4986, dtype: float32\n",
            "Predictions saved to 'MLP_Residual_normalize.csv'!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# MLP5 + Residual 모델 정의\n",
        "class MLP5Residual(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP5Residual, self).__init__()\n",
        "        self.fc_in = nn.Linear(input_size, hidden_size)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc_in(x))\n",
        "        residual = x\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x) + residual)  # Residual connection\n",
        "        residual = x\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.relu(self.fc5(x) + residual)  # Residual connection\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "def load_and_preprocess_data(train_path, test_path):\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    X_train = train_df.iloc[:, 1:-1].values  # x_0 to x_10\n",
        "    y_train = train_df.iloc[:, -1].values  # y\n",
        "    X_test = test_df.iloc[:, 1:].values  # x_0 to x_10\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, y_train, X_test_scaled\n",
        "\n",
        "# y값 스케일링 함수 (0 ~ 100 → -0.5 ~ 0.5)\n",
        "def scale_y(y, min_val=0, max_val=100, new_min=-0.5, new_max=0.5):\n",
        "    return ((y - min_val) / (max_val - min_val)) * (new_max - new_min) + new_min\n",
        "\n",
        "# y값 역변환 함수 (-0.5 ~ 0.5 → 0 ~ 100)\n",
        "def inverse_scale_y(y_scaled, min_val=0, max_val=100, new_min=-0.5, new_max=0.5):\n",
        "    return ((y_scaled - new_min) / (new_max - new_min)) * (max_val - min_val) + min_val\n",
        "\n",
        "# 모델 훈련 함수\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "# 메인 실행 부분\n",
        "if __name__ == \"__main__\":\n",
        "    # 파일 경로 설정\n",
        "    train_path = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/train.csv'\n",
        "    test_path = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/test.csv'\n",
        "    submission_csv_path = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/sample_submission.csv'\n",
        "\n",
        "    # 하이퍼파라미터 설정\n",
        "    input_size = 11  # x_0 to x_10\n",
        "    hidden_size = 64\n",
        "    output_size = 1\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 100\n",
        "\n",
        "    # 디바이스 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 데이터 로드 및 전처리\n",
        "    X_train, y_train, X_test = load_and_preprocess_data(train_path, test_path)\n",
        "\n",
        "    # y값 스케일링\n",
        "    y_train_scaled = scale_y(y_train)\n",
        "\n",
        "    # 데이터셋 및 데이터로더 생성\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                                  torch.tensor(y_train_scaled, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 모델, 손실 함수, 옵티마이저 초기화\n",
        "    model = MLP5Residual(input_size, hidden_size, output_size)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 모델 훈련\n",
        "    train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/contest/samsung/results/mlp5_residual_model.pth')\n",
        "    print(\"Model saved!\")\n",
        "\n",
        "    # 테스트 데이터에 대한 예측\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)  # X_test는 이미 NumPy 배열입니다.\n",
        "    test_dataset = TensorDataset(X_test_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    test_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch_X = batch[0].to(device)\n",
        "            outputs = model(batch_X)\n",
        "            test_pred.extend(outputs.cpu().numpy())\n",
        "\n",
        "    test_pred = np.array(test_pred).flatten()\n",
        "\n",
        "    # 예측값을 원래 범위로 변환\n",
        "    test_pred_original = inverse_scale_y(test_pred)\n",
        "\n",
        "    # 제출 파일 생성\n",
        "    submission_df = pd.read_csv(submission_csv_path)\n",
        "    submission_df['y'] = test_pred_original\n",
        "    print(submission_df['y'])\n",
        "\n",
        "    # 지정된 경로에 CSV 파일로 저장\n",
        "    submission_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/contest/samsung/results/MLP_Residual_normalize.csv\", index=False)\n",
        "    print(\"Predictions saved to 'MLP_Residual_normalize.csv'!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_non_matching_ids(file1, file2):\n",
        "    # 두 개의 CSV 파일을 읽어옴\n",
        "    df1 = pd.read_csv(file1)\n",
        "    df2 = pd.read_csv(file2)\n",
        "\n",
        "    # y 값 기준으로 내림차순 정렬\n",
        "    df1_sorted = df1.sort_values(by='y', ascending=False)\n",
        "    df2_sorted = df2.sort_values(by='y', ascending=False)\n",
        "\n",
        "    # file1의 상위 10% 항목 계산\n",
        "    top_10_percent_count = int(len(df1_sorted) * 0.1)\n",
        "    top_10_percent_ids_df1 = set(df1_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file2의 상위 10% ID 추출\n",
        "    top_10_percent_ids_df2 = set(df2_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file1의 상위 10% 중 file2의 상위 10%에 없는 ID 계산\n",
        "    non_matching_ids = top_10_percent_ids_df1 - top_10_percent_ids_df2\n",
        "    num_non_matching = len(non_matching_ids)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"file1의 상위 10% 항목 개수: {top_10_percent_count}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 개수: {num_non_matching}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 ID: {non_matching_ids}\")\n",
        "\n",
        "    return top_10_percent_count, num_non_matching, list(non_matching_ids)\n",
        "\n",
        "# 사용 예시\n",
        "file1 ='/content/drive/MyDrive/Colab Notebooks/contest/samsung/MLP_Residual_Connection_drop_x2_x6(0.752).csv' # best 성능 파일\n",
        "file2 = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/MLP_Residual_Connection_Mul_0.752.csv'\n",
        "file3 = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/results/MLP_Residual_normalize.csv' # 측정하고자 하는 파일\n",
        "top_10_percent_count, num_non_matching, non_matching_ids = find_non_matching_ids(file1, file3)\n",
        "top_10_percent_count, num_non_matching, non_matching_ids = find_non_matching_ids(file2, file3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjMeBGHP2c0Z",
        "outputId": "c9526ce3-c208-4f96-dcec-a40eaa0cd0d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file1의 상위 10% 항목 개수: 498\n",
            "file1의 상위 10% 중 file2에 없는 항목 개수: 36\n",
            "file1의 상위 10% 중 file2에 없는 항목 ID: {'TEST_0037', 'TEST_3536', 'TEST_4551', 'TEST_0108', 'TEST_0103', 'TEST_1309', 'TEST_4179', 'TEST_2220', 'TEST_4033', 'TEST_2577', 'TEST_4707', 'TEST_2613', 'TEST_0367', 'TEST_4070', 'TEST_3512', 'TEST_1909', 'TEST_2275', 'TEST_1178', 'TEST_4876', 'TEST_2538', 'TEST_1574', 'TEST_1502', 'TEST_0493', 'TEST_0935', 'TEST_4949', 'TEST_0898', 'TEST_2845', 'TEST_0635', 'TEST_4011', 'TEST_2609', 'TEST_4578', 'TEST_1362', 'TEST_0107', 'TEST_4221', 'TEST_3672', 'TEST_3042'}\n",
            "file1의 상위 10% 항목 개수: 498\n",
            "file1의 상위 10% 중 file2에 없는 항목 개수: 23\n",
            "file1의 상위 10% 중 file2에 없는 항목 ID: {'TEST_4146', 'TEST_0108', 'TEST_0103', 'TEST_2220', 'TEST_4511', 'TEST_4033', 'TEST_2577', 'TEST_4707', 'TEST_2613', 'TEST_4070', 'TEST_4982', 'TEST_1909', 'TEST_4876', 'TEST_2538', 'TEST_2632', 'TEST_0493', 'TEST_4949', 'TEST_0635', 'TEST_4578', 'TEST_1362', 'TEST_0107', 'TEST_4221', 'TEST_3042'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "164-mlpV2mTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}