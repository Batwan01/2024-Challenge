{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Batwan01/2024-Challenge/blob/main/history/24-9-9/MLP5%2BPReLU_base_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "GBmt3kLNeFAS",
        "outputId": "4aad32c3-6818-43e0-f568-a9b9938d50cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, ElasticNet, Lars, LassoLars, OrthogonalMatchingPursuit, BayesianRidge, ARDRegression, PassiveAggressiveRegressor, RANSACRegressor, HuberRegressor)\n",
        "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "\n",
        "# 재현 가능성을 위한 시드 고정\n",
        "RANDOM_SEED = 18\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n"
      ],
      "metadata": {
        "id": "8N5kdRfjFDiC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dY1X_zZmsbqK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h66mrXCydXAy",
        "outputId": "a6fcbad7-c0e1-4eb3-c527-75b4fb692067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드 및 전처리\n",
        "train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/contest/samsung/train.csv\"\n",
        "test_csv_path = \"/content/drive/MyDrive/Colab Notebooks/contest/samsung/test.csv\"\n",
        "train = pd.read_csv(train_csv_path)\n",
        "test = pd.read_csv(test_csv_path)\n",
        "com = pd.concat([train, test])\n",
        "com = com.drop(['x_2', 'x_6'], axis=1)\n",
        "\n",
        "train_data = com[:40118]\n",
        "X_test = com[40118:].drop('y', axis=1).iloc[:, 1:]\n",
        "\n",
        "# 입력 데이터와 라벨 분리\n",
        "X_train_df = train_data.drop(['ID', 'y'], axis=1)\n",
        "y_train_df = train_data['y']\n",
        "\n",
        "# 70 미만 값 제거\n",
        "mask = y_train_df >= 70\n",
        "X_train = X_train_df[mask]\n",
        "y_train = y_train_df[mask]\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "9KIgBfH9dT7t",
        "outputId": "24e5febc-c7f9-4dd1-8fdd-254cd6dfd554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4986, 9)\n",
            "(40110, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "# NRMSE 계산 함수\n",
        "def lg_nrmse(gt, preds):\n",
        "    all_nrmse = []\n",
        "    for idx in range(gt.shape[1]):\n",
        "        rmse = mean_squared_error(gt[:, idx], preds[:, idx], squared=False)\n",
        "        nrmse = rmse / np.mean(np.abs(gt[:, idx]))\n",
        "        all_nrmse.append(nrmse)\n",
        "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
        "    return score\n",
        "\n",
        "# 기본 모델들 정의\n",
        "base_models = [\n",
        "    ('HistGradientBoosting', HistGradientBoostingRegressor(random_state=RANDOM_SEED)),\n",
        "    ('ExtraTrees', ExtraTreesRegressor(n_jobs=-1, random_state=RANDOM_SEED)),\n",
        "    ('RandomForest', RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED)),\n",
        "    ('LightGBM', LGBMRegressor(n_jobs=-1, random_state=RANDOM_SEED, force_col_wise=True)),\n",
        "    ('XGBoost', XGBRegressor(tree_method='hist', device='cuda', random_state=RANDOM_SEED, n_jobs=-1))  # XGBoost 추가 및 GPU 사용\n",
        "]"
      ],
      "metadata": {
        "id": "7jAvL_AFNvtG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 모델 정의 (meta-model)\n",
        "final_model = LinearRegression()\n",
        "\n",
        "# Stacking Regressor 정의\n",
        "stacking_regressor = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=final_model\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "\n",
        "# KFold 교차 검증 설정\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Mean Squared Error를 평가 지표로 사용\n",
        "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "\n",
        "# 교차 검증 수행\n",
        "cv_scores = cross_val_score(stacking_regressor, X_train, y_train, cv=kfold, scoring=scorer)\n",
        "\n",
        "# 평균 MSE 출력\n",
        "print(f\"Mean Squared Error (negative) for each fold: {cv_scores}\")\n",
        "print(f\"Mean Squared Error (negative) across folds: {-cv_scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "jU8kdBtoRAZe",
        "outputId": "b45538ac-e3bd-4d12-de69-7b9e40c88861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.650282\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.650161\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.651586\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.643993\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.664031\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.641638\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.646558\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.655487\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.644303\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.646046\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.650180\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.636775\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642574\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642431\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.641513\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642631\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.655095\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.631200\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.646182\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.644697\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.644469\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.645243\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.659594\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.636905\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.637305\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.641499\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.633424\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.636546\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.648492\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.626562\n",
            "Mean Squared Error (negative) for each fold: [-2.52550226 -2.50766999 -2.56141951 -2.44896082 -2.62725415]\n",
            "Mean Squared Error (negative) across folds: 2.5342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_regressor1 = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=final_model\n",
        ")\n",
        "stacking_regressor2 = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=final_model\n",
        ")\n",
        "stacking_regressor3 = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=final_model\n",
        ")\n",
        "stacking_regressor4 = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=final_model\n",
        ")\n",
        "stacking_regressor5 = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=final_model\n",
        ")\n",
        "stack_list = [stacking_regressor1, stacking_regressor2, stacking_regressor3, stacking_regressor4, stacking_regressor5]\n",
        "x_train_fold = []\n",
        "y_train_fold = []\n",
        "\n",
        "for train_index, test_index in kfold.split(X_train, y_train):\n",
        "    X = X_train.iloc[train_index] # Use .iloc to select rows by position.\n",
        "    y = y_train.iloc[train_index] # Use .iloc to select rows by position.\n",
        "    x_train_fold.append(X) # Use append instead of appned.\n",
        "    y_train_fold.append(y)\n"
      ],
      "metadata": {
        "id": "jfi-xrFOiXyS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    stack_list[i].fit(x_train_fold[i], y_train_fold[i])"
      ],
      "metadata": {
        "id": "xIL3LHwXlM22",
        "outputId": "5a0a66e9-a9c5-4ffc-8a5b-d0e5ffb3f9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.646558\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.655487\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.644303\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.646046\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.650180\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.636775\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642574\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642431\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.641513\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642631\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.655095\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.631200\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.646182\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.644697\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.644469\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.645243\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.659594\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.636905\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.637305\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.641499\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.633424\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25670, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.636546\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.648492\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 25671, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.626562\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-a3843a00234e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstack_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 (fit) 수행\n",
        "stacking_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Fu7biNEQdTBi",
        "outputId": "c9fd74ca-acb2-43f5-d8ab-b4e2ae1b7b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 40110, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.644580\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.646746\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642818\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.643259\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.655282\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.634795\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingRegressor(estimators=[('HistGradientBoosting',\n",
              "                               HistGradientBoostingRegressor(random_state=18)),\n",
              "                              ('ExtraTrees',\n",
              "                               ExtraTreesRegressor(n_jobs=-1, random_state=18)),\n",
              "                              ('RandomForest',\n",
              "                               RandomForestRegressor(n_jobs=-1,\n",
              "                                                     random_state=18)),\n",
              "                              ('LightGBM',\n",
              "                               LGBMRegressor(force_col_wise=True, n_jobs=-1,\n",
              "                                             random_state=18)),\n",
              "                              ('XGBoost',\n",
              "                               XGBRegressor(base_score=None, booste...\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_rate=None, max_bin=None,\n",
              "                                            max_cat_threshold=None,\n",
              "                                            max_cat_to_onehot=None,\n",
              "                                            max_delta_step=None, max_depth=None,\n",
              "                                            max_leaves=None,\n",
              "                                            min_child_weight=None, missing=nan,\n",
              "                                            monotone_constraints=None,\n",
              "                                            multi_strategy=None,\n",
              "                                            n_estimators=None, n_jobs=-1,\n",
              "                                            num_parallel_tree=None,\n",
              "                                            random_state=18, ...))],\n",
              "                  final_estimator=LinearRegression())"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;HistGradientBoosting&#x27;,\n",
              "                               HistGradientBoostingRegressor(random_state=18)),\n",
              "                              (&#x27;ExtraTrees&#x27;,\n",
              "                               ExtraTreesRegressor(n_jobs=-1, random_state=18)),\n",
              "                              (&#x27;RandomForest&#x27;,\n",
              "                               RandomForestRegressor(n_jobs=-1,\n",
              "                                                     random_state=18)),\n",
              "                              (&#x27;LightGBM&#x27;,\n",
              "                               LGBMRegressor(force_col_wise=True, n_jobs=-1,\n",
              "                                             random_state=18)),\n",
              "                              (&#x27;XGBoost&#x27;,\n",
              "                               XGBRegressor(base_score=None, booste...\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_rate=None, max_bin=None,\n",
              "                                            max_cat_threshold=None,\n",
              "                                            max_cat_to_onehot=None,\n",
              "                                            max_delta_step=None, max_depth=None,\n",
              "                                            max_leaves=None,\n",
              "                                            min_child_weight=None, missing=nan,\n",
              "                                            monotone_constraints=None,\n",
              "                                            multi_strategy=None,\n",
              "                                            n_estimators=None, n_jobs=-1,\n",
              "                                            num_parallel_tree=None,\n",
              "                                            random_state=18, ...))],\n",
              "                  final_estimator=LinearRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;HistGradientBoosting&#x27;,\n",
              "                               HistGradientBoostingRegressor(random_state=18)),\n",
              "                              (&#x27;ExtraTrees&#x27;,\n",
              "                               ExtraTreesRegressor(n_jobs=-1, random_state=18)),\n",
              "                              (&#x27;RandomForest&#x27;,\n",
              "                               RandomForestRegressor(n_jobs=-1,\n",
              "                                                     random_state=18)),\n",
              "                              (&#x27;LightGBM&#x27;,\n",
              "                               LGBMRegressor(force_col_wise=True, n_jobs=-1,\n",
              "                                             random_state=18)),\n",
              "                              (&#x27;XGBoost&#x27;,\n",
              "                               XGBRegressor(base_score=None, booste...\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_rate=None, max_bin=None,\n",
              "                                            max_cat_threshold=None,\n",
              "                                            max_cat_to_onehot=None,\n",
              "                                            max_delta_step=None, max_depth=None,\n",
              "                                            max_leaves=None,\n",
              "                                            min_child_weight=None, missing=nan,\n",
              "                                            monotone_constraints=None,\n",
              "                                            multi_strategy=None,\n",
              "                                            n_estimators=None, n_jobs=-1,\n",
              "                                            num_parallel_tree=None,\n",
              "                                            random_state=18, ...))],\n",
              "                  final_estimator=LinearRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>HistGradientBoosting</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingRegressor(random_state=18)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>ExtraTrees</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor(n_jobs=-1, random_state=18)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>RandomForest</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_jobs=-1, random_state=18)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>LightGBM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(force_col_wise=True, n_jobs=-1, random_state=18)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>XGBoost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=-1,\n",
              "             num_parallel_tree=None, random_state=18, ...)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터로 성능 평가\n",
        "y_train_pred = stacking_regressor.predict(X_train)  # 훈련 데이터에 대한 예측\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)  # 훈련 데이터에서 MSE 계산\n",
        "print(f\"Mean Squared Error on training data: {mse_train:.4f}\")\n",
        "\n",
        "# 테스트 데이터에 대한 예측 (성능 평가 X)\n",
        "y_pred = stacking_regressor.predict(X_test)  # 테스트 데이터에 대한 예측 수행\n",
        "\n",
        "# 테스트 데이터의 예측 결과 출력 또는 저장\n",
        "print(f\"Predictions for test data: {y_pred[:5]}\")  # 테스트 예측값의 일부를 출력\n"
      ],
      "metadata": {
        "id": "2Y_4dAW7rgOE",
        "outputId": "9c251c18-c40a-4880-ba43-fd7efaaab12f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on training data: 2.1603\n",
            "Predictions for test data: [83.69120843 82.23591382 90.8824177  91.66460792 82.03445883]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 저장 (1차원 배열 처리)\n",
        "submission = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/contest/samsung/sample_submission.csv\")\n",
        "\n",
        "# 'y' 열에 예측 결과 할당 (결과가 1차원 배열일 때)\n",
        "submission['y'] = y_pred  # 'y' 컬럼에 1차원 예측 결과를 넣음\n",
        "\n",
        "# CSV 파일로 저장\n",
        "submission.to_csv(\"/content/drive/MyDrive/Colab Notebooks/contest/samsung/results/Stacking_Predictions.csv\", index=False)\n",
        "\n",
        "print(\"예측 결과가 CSV 파일로 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "UJ1JslclWhrx",
        "outputId": "17e1d5fa-9154-410c-b9c2-ab49818ab224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 결과가 CSV 파일로 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_non_matching_ids(file1, file2):\n",
        "    # 두 개의 CSV 파일을 읽어옴\n",
        "    df1 = pd.read_csv(file1)\n",
        "    df2 = pd.read_csv(file2)\n",
        "\n",
        "    # y 값 기준으로 내림차순 정렬\n",
        "    df1_sorted = df1.sort_values(by='y', ascending=False)\n",
        "    df2_sorted = df2.sort_values(by='y', ascending=False)\n",
        "\n",
        "    # file1의 상위 10% 항목 계산\n",
        "    top_10_percent_count = int(len(df1_sorted) * 0.1)\n",
        "    top_10_percent_ids_df1 = set(df1_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file2의 상위 10% ID 추출\n",
        "    top_10_percent_ids_df2 = set(df2_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file1의 상위 10% 중 file2의 상위 10%에 없는 ID 계산\n",
        "    non_matching_ids = top_10_percent_ids_df1 - top_10_percent_ids_df2\n",
        "    num_non_matching = len(non_matching_ids)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"file1의 상위 10% 항목 개수: {top_10_percent_count}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 개수: {num_non_matching}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 ID: {non_matching_ids}\")\n",
        "\n",
        "    return top_10_percent_count, num_non_matching, list(non_matching_ids)\n",
        "\n",
        "# 사용 예시\n",
        "file1 ='/content/drive/MyDrive/Colab Notebooks/contest/samsung/MLP_Residual_Connection_drop_x2_x6(0.752).csv' # best 성능 파일\n",
        "#file2 = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/MLP_Residual_Connection_18.csv'\n",
        "file2 = '/content/Stacking_Predictions.csv'\n",
        "file3 = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/results/Stacking_Predictions.csv' # 측정하고자 하는 파일\n",
        "#top_10_percent_count, num_non_matching, non_matching_ids = find_non_matching_ids(file1, file3)\n",
        "top_10_percent_count, num_non_matching, non_matching_ids = find_non_matching_ids(file2, file3)"
      ],
      "metadata": {
        "id": "F5K64efPqdRe",
        "outputId": "38f97968-7b9b-4922-ff7b-c5031a8c7480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file1의 상위 10% 항목 개수: 498\n",
            "file1의 상위 10% 중 file2에 없는 항목 개수: 41\n",
            "file1의 상위 10% 중 file2에 없는 항목 ID: {'TEST_0917', 'TEST_3403', 'TEST_3889', 'TEST_4221', 'TEST_1592', 'TEST_1015', 'TEST_1690', 'TEST_2733', 'TEST_3838', 'TEST_1162', 'TEST_1803', 'TEST_2937', 'TEST_4069', 'TEST_2064', 'TEST_4738', 'TEST_1377', 'TEST_3734', 'TEST_2765', 'TEST_1978', 'TEST_1839', 'TEST_0823', 'TEST_4543', 'TEST_3718', 'TEST_0713', 'TEST_2418', 'TEST_1904', 'TEST_3209', 'TEST_2865', 'TEST_4682', 'TEST_3625', 'TEST_1327', 'TEST_4472', 'TEST_1335', 'TEST_4828', 'TEST_2351', 'TEST_0864', 'TEST_3329', 'TEST_2125', 'TEST_0548', 'TEST_0197', 'TEST_3065'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = list(non_matching_ids)\n",
        "# 확인하고자 하는 ID 리스트\n",
        "ids_to_check = ['TEST_3536', 'TEST_1574', 'TEST_3731', 'TEST_1780', 'TEST_1746', 'TEST_3287', 'TEST_1429', 'TEST_1731']\n",
        "ids_to_check_false = ['TEST_1377', 'TEST_2035', 'TEST_3042', 'TEST_4949', 'TEST_4578', 'TEST_4707', 'TEST_2538', 'TEST_1909']\n",
        "# non_matching_ids에 있는지 확인\n",
        "matching_ids_T = [id_ for id_ in ids_to_check if id_ in arr]\n",
        "matching_ids_F = [id_ for id_ in ids_to_check_false if id_ in arr]\n",
        "# 결과 출력\n",
        "print(\"정답 예측\")\n",
        "print(f\"다음 ID들이 non_matching_ids에 있습니다: {matching_ids_T}\")\n",
        "print(f\"non_matching_ids에서 확인된 ID 개수: {len(matching_ids_T)}\")\n",
        "\n",
        "# 결과 출력\n",
        "print(\"오답 예측\")\n",
        "print(f\"다음 ID들이 non_matching_ids에 있습니다: {matching_ids_F}\")\n",
        "print(f\"non_matching_ids에서 확인된 ID 개수: {len(matching_ids_F)}\")"
      ],
      "metadata": {
        "id": "aORDCm7Mrunc",
        "outputId": "84973be4-f0f0-450e-c570-d92df660eaac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정답 예측\n",
            "다음 ID들이 non_matching_ids에 있습니다: ['TEST_1574']\n",
            "non_matching_ids에서 확인된 ID 개수: 1\n",
            "오답 예측\n",
            "다음 ID들이 non_matching_ids에 있습니다: ['TEST_4949', 'TEST_4578', 'TEST_2538', 'TEST_1909']\n",
            "non_matching_ids에서 확인된 ID 개수: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSPB9Iuvr4V3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}