{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Batwan01/2024-Challenge/blob/main/history/24-9-9/MLP5%2BPReLU_base_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "GBmt3kLNeFAS",
        "outputId": "4aad32c3-6818-43e0-f568-a9b9938d50cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, ElasticNet, Lars, LassoLars, OrthogonalMatchingPursuit, BayesianRidge, ARDRegression, PassiveAggressiveRegressor, RANSACRegressor, HuberRegressor)\n",
        "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "# 재현 가능성을 위한 시드 고정\n",
        "RANDOM_SEED = 18\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n"
      ],
      "metadata": {
        "id": "8N5kdRfjFDiC",
        "outputId": "4aed9a77-0577-476d-a394-b3e49151ec86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h66mrXCydXAy",
        "outputId": "2204bb12-4a53-489f-b851-727a38b14dbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드 및 전처리\n",
        "train_csv_path = \"/content/drive/MyDrive/Colab Notebooks/contest/samsung/train.csv\"\n",
        "test_csv_path = \"/content/drive/MyDrive/Colab Notebooks/contest/samsung/test.csv\"\n",
        "train = pd.read_csv(train_csv_path)\n",
        "test = pd.read_csv(test_csv_path)\n",
        "com = pd.concat([train, test])\n",
        "com = com.drop(['x_2', 'x_6'], axis=1)\n",
        "\n",
        "train_data = com[:40118]\n",
        "X_test_df = com[40118:].drop('y', axis=1).iloc[:, 1:]\n",
        "\n",
        "# 입력 데이터와 라벨 분리\n",
        "X_train_df = train_data.drop(['ID', 'y'], axis=1)\n",
        "y_train_df = train_data['y']\n",
        "\n",
        "# 70 미만 값 제거\n",
        "mask = y_train_df >= 70\n",
        "X_train_df = X_train_df[mask]\n",
        "y_train_df = y_train_df[mask]\n",
        "\n",
        "# 스무딩 처리 (여기서는 가정된 변수명을 그대로 사용함)\n",
        "X_train = pd.get_dummies(X_train_df, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test_df, drop_first=True)\n",
        "\n",
        "# X_train과 X_test의 열 차원 맞추기\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "9KIgBfH9dT7t",
        "outputId": "96deedb8-70cc-4d5a-f2bf-dfdecfb221e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4986, 9)\n",
            "(40110, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stacking_ml_datasets(model, X_train_n, y_train_n, X_test_n, n_folds=5, fitting=True):\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "    # 넘파이 배열로 변환\n",
        "    if isinstance(X_train_n, pd.DataFrame):\n",
        "        X_train_n = X_train_n.values\n",
        "    if isinstance(X_test_n, pd.DataFrame):\n",
        "        X_test_n = X_test_n.values\n",
        "\n",
        "    if y_train_n.ndim == 1:\n",
        "        train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
        "        test_pred = np.zeros((X_test_n.shape[0], 1, n_folds))\n",
        "    else:\n",
        "        train_fold_pred = np.zeros((X_train_n.shape[0], y_train_n.shape[1]))\n",
        "        test_pred = np.zeros((X_test_n.shape[0], y_train_n.shape[1], n_folds))\n",
        "\n",
        "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n, y_train_n)):\n",
        "        X_tr = X_train_n[train_index]\n",
        "        y_tr = y_train_n[train_index]\n",
        "\n",
        "        # y_tr이 1차원이면 2차원으로 변환\n",
        "        if y_tr.ndim == 1:\n",
        "            y_tr = y_tr.reshape(-1, 1)\n",
        "\n",
        "        X_te = X_train_n[valid_index]\n",
        "\n",
        "        if fitting:\n",
        "            model.fit(X_tr, y_tr)\n",
        "\n",
        "        train_pred = model.predict(X_te)\n",
        "        if train_pred.ndim == 1:\n",
        "            train_pred = train_pred.reshape(-1, 1)\n",
        "        train_fold_pred[valid_index] = train_pred\n",
        "\n",
        "        test_fold_pred = model.predict(X_test_n)\n",
        "        if test_fold_pred.ndim == 1:\n",
        "            test_fold_pred = test_fold_pred.reshape(-1, 1)\n",
        "        test_pred[:, :, folder_counter] = test_fold_pred\n",
        "\n",
        "    test_pred_mean = np.mean(test_pred, axis=2)\n",
        "\n",
        "    return train_fold_pred, test_pred_mean\n"
      ],
      "metadata": {
        "id": "gOW82JxMduWU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NRMSE 계산 함수\n",
        "def lg_nrmse(gt, preds):\n",
        "    all_nrmse = []\n",
        "    for idx in range(gt.shape[1]):\n",
        "        rmse = mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
        "        nrmse = rmse / np.mean(np.abs(gt[:,idx]))\n",
        "        all_nrmse.append(nrmse)\n",
        "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
        "    return score\n",
        "\n",
        "# 여러 모델 정의\n",
        "base_ml = {\n",
        "    'Linear Regression': LinearRegression(n_jobs=-1),\n",
        "    'Ridge': Ridge(random_state=RANDOM_SEED),\n",
        "    'Lasso': Lasso(random_state=RANDOM_SEED),\n",
        "    'ElasticNet': ElasticNet(random_state=RANDOM_SEED),\n",
        "    #'Lars': Lars(random_state=RANDOM_SEED),\n",
        "    #'LassoLars': LassoLars(random_state=RANDOM_SEED),\n",
        "    #'OMP': OrthogonalMatchingPursuit(),\n",
        "    'BayesianRidge': MultiOutputRegressor(BayesianRidge()),\n",
        "    #'ARDRegression': MultiOutputRegressor(ARDRegression()),\n",
        "    #'PAR': MultiOutputRegressor(PassiveAggressiveRegressor(random_state=RANDOM_SEED)),\n",
        "    'RANSAC': RANSACRegressor(random_state=RANDOM_SEED),\n",
        "    'Huber': MultiOutputRegressor(HuberRegressor()),\n",
        "    #'KNN': KNeighborsRegressor(n_jobs=-1),\n",
        "    'DecisionTree': DecisionTreeRegressor(random_state=RANDOM_SEED),\n",
        "    #'ExtraTree': ExtraTreeRegressor(random_state=RANDOM_SEED),\n",
        "    'Bagging': BaggingRegressor(n_jobs=-1, random_state=RANDOM_SEED),\n",
        "    'ExtraTrees': ExtraTreesRegressor(n_jobs=-1, random_state=RANDOM_SEED),\n",
        "    'RandomForest': RandomForestRegressor(n_jobs=-1, random_state=RANDOM_SEED),\n",
        "    'HistGradientBoosting': MultiOutputRegressor(HistGradientBoostingRegressor(random_state=RANDOM_SEED)),\n",
        "    'XGBoost': XGBRegressor(tree_method='gpu_hist', gpu_id=0, n_jobs=-1, random_state=RANDOM_SEED),\n",
        "    'LightGBM': MultiOutputRegressor(LGBMRegressor(n_jobs=-1, random_state=RANDOM_SEED)),\n",
        "    'CatBoost': MultiOutputRegressor(CatBoostRegressor(task_type=\"GPU\", devices='0', verbose=False, random_state=RANDOM_SEED))\n",
        "}"
      ],
      "metadata": {
        "id": "3R0A6slmd06P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스태킹 데이터셋 생성\n",
        "meta_ml_X_train = []\n",
        "meta_ml_X_test = []\n",
        "n_folds = 5\n",
        "for name, model in base_ml.items():\n",
        "    print(f'Running {name}...')\n",
        "\n",
        "    # 인덱스 리셋 (기존 인덱스 문제를 방지하기 위해)\n",
        "    X_train_reset = X_train.reset_index(drop=True)\n",
        "    y_train_reset = y_train_df.reset_index(drop=True)\n",
        "\n",
        "    # MultiOutputRegressor일 경우 y_train을 2차원으로 변환\n",
        "    if isinstance(model, MultiOutputRegressor):\n",
        "        # y_train_df의 모양이 1차원일 경우 2차원으로 변환\n",
        "        if y_train_reset.ndim == 1:\n",
        "            y_train_reshaped = y_train_reset.values.reshape(-1, 1)\n",
        "        else:\n",
        "            y_train_reshaped = y_train_reset.values\n",
        "\n",
        "        temp_X_train, temp_X_test = get_stacking_ml_datasets(model, X_train_reset, y_train_reshaped, X_test, n_folds)\n",
        "    else:\n",
        "        # MultiOutputRegressor가 아닌 경우 1차원 y_train 사용\n",
        "        temp_X_train, temp_X_test = get_stacking_ml_datasets(model, X_train_reset, y_train_reset.values.ravel(), X_test, n_folds)\n",
        "\n",
        "    meta_ml_X_train.append(temp_X_train)\n",
        "    meta_ml_X_test.append(temp_X_test)\n",
        "\n",
        "# 스태킹 데이터 결합\n",
        "meta_ml_X_train = np.hstack(meta_ml_X_train)\n",
        "meta_ml_X_test = np.hstack(meta_ml_X_test)\n"
      ],
      "metadata": {
        "id": "V7T9j-izdw3C",
        "outputId": "c188460d-a3b8-45c9-f5d7-f677a26be06f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Linear Regression...\n",
            "Running Ridge...\n",
            "Running Lasso...\n",
            "Running ElasticNet...\n",
            "Running BayesianRidge...\n",
            "Running RANSAC...\n",
            "Running Huber...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running DecisionTree...\n",
            "Running Bagging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_bagging.py:509: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_bagging.py:509: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_bagging.py:509: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_bagging.py:509: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_bagging.py:509: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running ExtraTrees...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running RandomForest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running HistGradientBoosting...\n",
            "Running XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:58:17] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:58:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:58:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:58:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002617 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.642449\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003069 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.652839\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002478 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.633981\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002518 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.641608\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002786 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 32088, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 83.652024\n",
            "Running CatBoost...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 메타 모델 학습 및 예측\n",
        "meta_clf = LinearRegression()\n",
        "meta_clf.fit(meta_ml_X_train, y_train_df)\n",
        "prediction = meta_clf.predict(meta_ml_X_test)\n",
        "\n",
        "# 결과값을 반올림하여 처리\n",
        "result = prediction.round(3)"
      ],
      "metadata": {
        "id": "_wyKl6xud5vs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 저장 (1차원 배열 처리)\n",
        "submission = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/contest/samsung/sample_submission.csv\")\n",
        "\n",
        "# 'y' 열에 예측 결과 할당 (결과가 1차원 배열일 때)\n",
        "submission['y'] = result  # 'y' 컬럼에 1차원 예측 결과를 넣음\n",
        "\n",
        "# CSV 파일로 저장\n",
        "submission.to_csv(\"/content/drive/MyDrive/Colab Notebooks/contest/samsung/results/Stacking_Predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "_DCJpiyBd8AI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_non_matching_ids(file1, file2):\n",
        "    # 두 개의 CSV 파일을 읽어옴\n",
        "    df1 = pd.read_csv(file1)\n",
        "    df2 = pd.read_csv(file2)\n",
        "\n",
        "    # y 값 기준으로 내림차순 정렬\n",
        "    df1_sorted = df1.sort_values(by='y', ascending=False)\n",
        "    df2_sorted = df2.sort_values(by='y', ascending=False)\n",
        "\n",
        "    # file1의 상위 10% 항목 계산\n",
        "    top_10_percent_count = int(len(df1_sorted) * 0.1)\n",
        "    top_10_percent_ids_df1 = set(df1_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file2의 상위 10% ID 추출\n",
        "    top_10_percent_ids_df2 = set(df2_sorted.head(top_10_percent_count)['ID'])\n",
        "\n",
        "    # file1의 상위 10% 중 file2의 상위 10%에 없는 ID 계산\n",
        "    non_matching_ids = top_10_percent_ids_df1 - top_10_percent_ids_df2\n",
        "    num_non_matching = len(non_matching_ids)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"file1의 상위 10% 항목 개수: {top_10_percent_count}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 개수: {num_non_matching}\")\n",
        "    print(f\"file1의 상위 10% 중 file2에 없는 항목 ID: {non_matching_ids}\")\n",
        "\n",
        "    return top_10_percent_count, num_non_matching, list(non_matching_ids)\n",
        "\n",
        "# 사용 예시\n",
        "file1 ='/content/drive/MyDrive/Colab Notebooks/contest/samsung/MLP_Residual_Connection_drop_x2_x6(0.752).csv' # best 성능 파일\n",
        "file2 = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/MLP_Residual_Connection_18.csv'\n",
        "file3 = '/content/drive/MyDrive/Colab Notebooks/contest/samsung/result/Stacking_Predictions.csv' # 측정하고자 하는 파일\n",
        "top_10_percent_count, num_non_matching, non_matching_ids = find_non_matching_ids(file1, file3)\n",
        "top_10_percent_count, num_non_matching, non_matching_ids = find_non_matching_ids(file2, file3)"
      ],
      "metadata": {
        "id": "F5K64efPqdRe",
        "outputId": "69226128-8fb6-4d27-f1b0-c6c51c83f659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file1의 상위 10% 항목 개수: 498\n",
            "file1의 상위 10% 중 file2에 없는 항목 개수: 32\n",
            "file1의 상위 10% 중 file2에 없는 항목 ID: {'TEST_1978', 'TEST_3265', 'TEST_4828', 'TEST_1510', 'TEST_1178', 'TEST_0425', 'TEST_1309', 'TEST_1853', 'TEST_4876', 'TEST_3536', 'TEST_4070', 'TEST_0037', 'TEST_4154', 'TEST_0103', 'TEST_2220', 'TEST_2538', 'TEST_1574', 'TEST_1803', 'TEST_4033', 'TEST_4043', 'TEST_0635', 'TEST_4069', 'TEST_0898', 'TEST_1909', 'TEST_4179', 'TEST_0107', 'TEST_4738', 'TEST_0935', 'TEST_3718', 'TEST_2772', 'TEST_3512', 'TEST_4551'}\n",
            "file1의 상위 10% 항목 개수: 498\n",
            "file1의 상위 10% 중 file2에 없는 항목 개수: 35\n",
            "file1의 상위 10% 중 file2에 없는 항목 ID: {'TEST_1321', 'TEST_1978', 'TEST_3265', 'TEST_4828', 'TEST_1510', 'TEST_1178', 'TEST_0425', 'TEST_1309', 'TEST_1853', 'TEST_4876', 'TEST_3536', 'TEST_4070', 'TEST_0037', 'TEST_4154', 'TEST_0103', 'TEST_2220', 'TEST_2538', 'TEST_1574', 'TEST_4511', 'TEST_4043', 'TEST_4033', 'TEST_0635', 'TEST_0784', 'TEST_4069', 'TEST_2632', 'TEST_0898', 'TEST_1909', 'TEST_0107', 'TEST_4738', 'TEST_0935', 'TEST_4982', 'TEST_3718', 'TEST_2772', 'TEST_1792', 'TEST_2981'}\n"
          ]
        }
      ]
    }
  ]
}