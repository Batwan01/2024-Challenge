{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chan-note/2024-Challenge/blob/main/MLP_TEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2KZC-cmKREl"
      },
      "source": [
        "# Samsung AI Challenge : Black-box Optimization\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. 초기 세팅 (필수 실행)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CTNFa1kUiS8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TSYWao-mdfy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43697308-5c2b-4f7b-e726-a975ba9e77c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 재현 가능성을 위한 시드 고정\n",
        "RANDOM_SEED = 18\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Dataloader 시드 고정 (아직 사용 x)\n",
        "'''\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(RANDOM_SEED)\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# DataLoader(worker_init_fn=worker_init_fn)\n",
        "'''\n",
        "\n",
        "# 데이터 경로 설정\n",
        "train_csv_path = \"/content/drive/MyDrive/Samsung_AI_challenge/Data/train.csv\"\n",
        "test_csv_path = \"/content/drive/MyDrive/Samsung_AI_challenge/Data/test.csv\"\n",
        "submission_csv_path = '/content/drive/MyDrive/Samsung_AI_challenge/Data/sample_submission.csv'\n",
        "\n",
        "# 기타 경로 설정 : 저장되는 데이터 이름 등 (model, method 에 맞게 설정)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mode 설정 : 어떤 model, 최적화 기법을 사용할 지 정하는 부분 (현재는 GS = Grid Search 만 사용)\n",
        "mode = 'MLP5HiddenBnDrop' # 사용할 모델 class 이름 : [모델 정의 참조]\n",
        "method = \"GS\" # 사용할 모델 하이퍼파라미터 최적화 알고리즘 이름"
      ],
      "metadata": {
        "id": "4ucCS4iWJwY1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 모델 정의 (필수 실행)"
      ],
      "metadata": {
        "id": "gEB4bnwZj60f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1 순수 MLP Linear + ReLU 모델"
      ],
      "metadata": {
        "id": "2ECHOl45t9eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP - 은닉층 3개"
      ],
      "metadata": {
        "id": "FEEbgqaDvPDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP - 은닉층 3개\n",
        "'''\n",
        "Grid Search 하이퍼파라미터 범위\n",
        "\n",
        "[Epoch 100 기준]\n",
        "param_grid = {\n",
        "    'hidden_sizes': [[64, 128, 64], [128, 256, 128], [32, 64, 32]],\n",
        "    'learning_rate': [0.01, 0.001, 0.0001],\n",
        "    'batch_size': [32, 64, 128],\n",
        "}\n",
        "'''\n",
        "# 현재 최고성능 모델 하이퍼파라미터(GS) : {\"batch_size\": 64, \"hidden_sizes\": [64, 128, 64], \"learning_rate\": 0.01}\n",
        "# Epoch : 500\n",
        "# Top 10% threshold : 92.0392\n",
        "# 제출 성능 : 0.728\n",
        "class MLP3Hidden(nn.Module):\n",
        "    def __init__(self, input_size=11, hidden_sizes=[64, 128, 64], output_size=1):\n",
        "        super(MLP3Hidden, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7cNslOmIk5_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP - 은닉층 4개"
      ],
      "metadata": {
        "id": "tDKQE-3GvWkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP - 은닉층 4개\n",
        "'''\n",
        "Grid Search 하이퍼파라미터 범위\n",
        "\n",
        "'''\n",
        "# 현재 최고성능 모델 하이퍼파라미터(GS) :\n",
        "# Epoch :\n",
        "# Top 10% threshold :\n",
        "# 제출 성능 :\n",
        "class MLP4Hidden(nn.Module):\n",
        "    def __init__(self, input_size=11, hidden_sizes=[64, 128, 64, 32], output_size=1):\n",
        "        super(MLP4Hidden, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.fc5 = nn.Linear(hidden_sizes[3], output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ThyWaZtJvL_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP - 은닉층 5개 (best)(0.736)"
      ],
      "metadata": {
        "id": "74A7YgW-vbdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP - 은닉층 5개\n",
        "'''\n",
        "Grid Search 하이퍼파라미터 범위\n",
        "\n",
        "[Epoch 100 기준]\n",
        "param_grid = {\n",
        "    'hidden_sizes': [[64, 128, 128, 64, 32], [128, 256, 256, 128, 64], [32, 64, 64, 32, 16]],\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'batch_size': [32, 64, 128],\n",
        "}\n",
        "'''\n",
        "# 현재 최고성능 모델 하이퍼파라미터(GS) : {\"batch_size\": 32, \"hidden_sizes\": [32, 64, 64, 32, 16], \"learning_rate\": 0.001}\n",
        "# Epoch : 93\n",
        "# Top 10% threshold: 92.9191\n",
        "# 제출 성능 : 0.736\n",
        "class MLP5Hidden(nn.Module):\n",
        "    def __init__(self, input_size=11, hidden_sizes=[32, 64, 64, 32, 16], output_size=1):\n",
        "        super(MLP5Hidden, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.fc5 = nn.Linear(hidden_sizes[3], hidden_sizes[4])\n",
        "        self.fc6 = nn.Linear(hidden_sizes[4], output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "C26L73cGvY8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1 MLP Linear + BatchNorm + Dropout + ReLU 모델"
      ],
      "metadata": {
        "id": "GGMNcDKWvulo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP - 은닉층 3개 + BN + Drop"
      ],
      "metadata": {
        "id": "XwbyedQJw9TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP - 은닉층 3개 + BatchNorm + Dropout\n",
        "'''\n",
        "Grid Search 하이퍼파라미터 범위\n",
        "\n",
        "'''\n",
        "# 현재 최고성능 모델 하이퍼파라미터(GS) :\n",
        "# Epoch :\n",
        "# Top 10% threshold :\n",
        "# 제출 성능 :\n",
        "class MLP3HiddenBnDrop(nn.Module):\n",
        "    def __init__(self, input_size=11, hidden_sizes=[64, 128, 64], output_size=1):\n",
        "        super(MLP3HiddenBnDrop, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_sizes[0])\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_sizes[1])\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_sizes[2])\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(self.fc1(x))\n",
        "        x = self.relu(self.dropout1(x))\n",
        "\n",
        "        x = self.bn2(self.fc2(x))\n",
        "        x = self.relu(self.dropout2(x))\n",
        "\n",
        "        x = self.bn3(self.fc3(x))\n",
        "        x = self.relu(self.dropout3(x))\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "N_ZuWQ5Av7xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP - 은닉층 5개 + BN + Drop"
      ],
      "metadata": {
        "id": "OZqr3RLVv1eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP - 은닉층 5개 + BatchNorm + Dropout\n",
        "'''\n",
        "Grid Search 하이퍼파라미터 범위\n",
        "\n",
        "'''\n",
        "# 현재 최고성능 모델 하이퍼파라미터(GS) :\n",
        "# Epoch :\n",
        "# Top 10% threshold :\n",
        "# 제출 성능 :\n",
        "class MLP5HiddenBnDrop(nn.Module):\n",
        "    def __init__(self, input_size=11, hidden_sizes=[64, 128, 64, 32, 16], dropout_rate = 0.5, output_size=1):\n",
        "        super(MLP5HiddenBnDrop, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_sizes[0])\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_sizes[1])\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_sizes[2])\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_sizes[3])\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc5 = nn.Linear(hidden_sizes[3], hidden_sizes[4])\n",
        "        self.bn5 = nn.BatchNorm1d(hidden_sizes[4])\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc6 = nn.Linear(hidden_sizes[4], output_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(self.fc1(x))\n",
        "        x = self.relu(self.dropout1(x))\n",
        "\n",
        "        x = self.bn2(self.fc2(x))\n",
        "        x = self.relu(self.dropout2(x))\n",
        "\n",
        "        x = self.bn3(self.fc3(x))\n",
        "        x = self.relu(self.dropout3(x))\n",
        "\n",
        "        x = self.bn4(self.fc4(x))\n",
        "        x = self.relu(self.dropout4(x))\n",
        "\n",
        "        x = self.bn5(self.fc5(x))\n",
        "        x = self.relu(self.dropout5(x))\n",
        "\n",
        "        x = self.fc6(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "urMK3xj4vp4C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 모델 하이퍼파라미터 최적화 (선택)"
      ],
      "metadata": {
        "id": "2Dxtn9rQxRIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. 훈련데이터 -> 훈련+검증 데이터"
      ],
      "metadata": {
        "id": "bItSrRndxX6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터를 훈련 + 검증 데이터로 나누고 train_dataset, val_dataset 을 반환하는 함수\n",
        "def split_dataloader(train_csv_path, val_ratio=0.2, random_state=RANDOM_SEED):\n",
        "    train_data = pd.read_csv(train_csv_path)\n",
        "\n",
        "    X = torch.tensor(train_data.iloc[:,1:-1].values, dtype=torch.float32)\n",
        "    y = torch.tensor(train_data.iloc[:,-1].values, dtype = torch.float32).view(-1,1)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_ratio, random_state=random_state)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "train_dataset, val_dataset = split_dataloader(train_csv_path)"
      ],
      "metadata": {
        "id": "S7oEjp5eyAHu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. Grid Search 최적화 알고리즘"
      ],
      "metadata": {
        "id": "DKoiQTtUzc5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mode 별 하이퍼파라미터 범위 설정"
      ],
      "metadata": {
        "id": "gpHg2FiU0qS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if mode == 'MLP3Hidden':\n",
        "    param_grid = {\n",
        "    'hidden_sizes': [[64, 128, 64], [128, 256, 128], [32, 64, 32]],\n",
        "    'learning_rate': [0.01, 0.001, 0.0001],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    }\n",
        "\n",
        "elif mode == 'MLP4Hidden':\n",
        "    param_grid = {\n",
        "    'hidden_sizes': [[64, 128, 128, 64], [128, 256, 128, 64], [32, 64, 64, 32]],\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    }\n",
        "\n",
        "elif mode == 'MLP5Hidden':\n",
        "    param_grid = {\n",
        "    'hidden_sizes': [[64, 128, 128, 64, 32], [128, 256, 256, 128, 64], [32, 64, 64, 32, 16]],\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    }\n",
        "\n",
        "elif mode == 'MLP5HiddenBnDrop':\n",
        "    param_grid = {\n",
        "    'hidden_sizes': [[16, 32, 64, 32, 16], [16, 32, 16, 8, 4], [32, 64, 128, 64, 32],\n",
        "     [64, 32, 16, 8, 4], [32, 32, 32, 32, 32], [32, 64, 64, 32, 16], [64, 128, 256, 128, 64], [128, 256, 256, 128, 64]],\n",
        "    'learning_rate': [0.01, 0.001],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    }\n",
        "\n",
        "with open(f'{mode}_param_grid_range.json', 'w') as json_file:\n",
        "    json.dump(param_grid, json_file)"
      ],
      "metadata": {
        "id": "K2q-zfqb000Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mode 별 탐색을 위한 함수 정의"
      ],
      "metadata": {
        "id": "KXbECUbP1nWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mode_model(mode, hidden_sizes):\n",
        "    if mode == 'MLP3Hidden':\n",
        "        return MLP3Hidden(hidden_sizes=hidden_sizes)\n",
        "    elif mode == 'MLP4Hidden':\n",
        "        return MLP4Hidden(hidden_sizes=hidden_sizes)\n",
        "    elif mode == 'MLP5Hidden':\n",
        "        return MLP5Hidden(hidden_sizes=hidden_sizes)\n",
        "    elif mode == 'MLP5HiddenBnDrop':\n",
        "        return MLP5HiddenBnDrop(hidden_sizes=hidden_sizes)\n",
        "\n",
        "def train_validation_model(mode, hidden_sizes, learning_rate, batch_size):\n",
        "\n",
        "    model = mode_model(mode, hidden_sizes).cuda()\n",
        "    criterion = nn.MSELoss().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    MSE = val_loss / len(val_loader)\n",
        "    return MSE\n"
      ],
      "metadata": {
        "id": "IR5yMQVc474A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search 수행"
      ],
      "metadata": {
        "id": "T6fFa4Tf5Gnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(parma_grid):\n",
        "    best_params = None\n",
        "    best_MSE = float('inf')\n",
        "\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        MSE = train_validation_model(mode, **params)\n",
        "        print(f\"Testing parameters: {params} => MSE: {MSE:.4f}\")\n",
        "        if MSE < best_MSE:\n",
        "            best_MSE = MSE\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_MSE\n",
        "\n",
        "\n",
        "best_params, best_MSE = grid_search(param_grid)\n",
        "print(f'\\nBest parameters: {best_params} with a MSE of {best_MSE:.4f}')\n",
        "\n",
        "with open(f'{mode}_bestparams.json', 'w') as json_file:\n",
        "    json.dump(best_params, json_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLXaTm4X5FLd",
        "outputId": "ddb0788a-cc6b-4d1c-e09e-087d05964942"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [16, 32, 64, 32, 16], 'learning_rate': 0.01} => MSE: 3.8308\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [16, 32, 64, 32, 16], 'learning_rate': 0.001} => MSE: 4.0820\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [16, 32, 16, 8, 4], 'learning_rate': 0.01} => MSE: 5.6711\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [16, 32, 16, 8, 4], 'learning_rate': 0.001} => MSE: 7.5905\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 128, 64, 32], 'learning_rate': 0.01} => MSE: 3.4248\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 128, 64, 32], 'learning_rate': 0.001} => MSE: 3.6486\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [64, 32, 16, 8, 4], 'learning_rate': 0.01} => MSE: 4.3958\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [64, 32, 16, 8, 4], 'learning_rate': 0.001} => MSE: 7.5908\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 32, 32, 32, 32], 'learning_rate': 0.01} => MSE: 3.6609\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 32, 32, 32, 32], 'learning_rate': 0.001} => MSE: 3.7803\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 64, 32, 16], 'learning_rate': 0.01} => MSE: 3.6492\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 64, 32, 16], 'learning_rate': 0.001} => MSE: 3.8556\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [64, 128, 256, 128, 64], 'learning_rate': 0.01} => MSE: 3.5729\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [64, 128, 256, 128, 64], 'learning_rate': 0.001} => MSE: 3.7873\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [128, 256, 256, 128, 64], 'learning_rate': 0.01} => MSE: 3.3863\n",
            "Testing parameters: {'batch_size': 32, 'hidden_sizes': [128, 256, 256, 128, 64], 'learning_rate': 0.001} => MSE: 3.7432\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [16, 32, 64, 32, 16], 'learning_rate': 0.01} => MSE: 3.8740\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [16, 32, 64, 32, 16], 'learning_rate': 0.001} => MSE: 11.0932\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [16, 32, 16, 8, 4], 'learning_rate': 0.01} => MSE: 5.0616\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [16, 32, 16, 8, 4], 'learning_rate': 0.001} => MSE: 360.9888\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 64, 128, 64, 32], 'learning_rate': 0.01} => MSE: 3.5581\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 64, 128, 64, 32], 'learning_rate': 0.001} => MSE: 5.2414\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [64, 32, 16, 8, 4], 'learning_rate': 0.01} => MSE: 7.5978\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [64, 32, 16, 8, 4], 'learning_rate': 0.001} => MSE: 87.4039\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 32, 32, 32, 32], 'learning_rate': 0.01} => MSE: 3.3778\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 32, 32, 32, 32], 'learning_rate': 0.001} => MSE: 4.8214\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 64, 64, 32, 16], 'learning_rate': 0.01} => MSE: 3.7292\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [32, 64, 64, 32, 16], 'learning_rate': 0.001} => MSE: 9.0203\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [64, 128, 256, 128, 64], 'learning_rate': 0.01} => MSE: 3.4012\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [64, 128, 256, 128, 64], 'learning_rate': 0.001} => MSE: 3.9273\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [128, 256, 256, 128, 64], 'learning_rate': 0.01} => MSE: 3.2983\n",
            "Testing parameters: {'batch_size': 64, 'hidden_sizes': [128, 256, 256, 128, 64], 'learning_rate': 0.001} => MSE: 3.5671\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [16, 32, 64, 32, 16], 'learning_rate': 0.01} => MSE: 3.8567\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [16, 32, 64, 32, 16], 'learning_rate': 0.001} => MSE: 19.0716\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [16, 32, 16, 8, 4], 'learning_rate': 0.01} => MSE: 7.5888\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [16, 32, 16, 8, 4], 'learning_rate': 0.001} => MSE: 153.5339\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 64, 128, 64, 32], 'learning_rate': 0.01} => MSE: 3.3351\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 64, 128, 64, 32], 'learning_rate': 0.001} => MSE: 6.8666\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [64, 32, 16, 8, 4], 'learning_rate': 0.01} => MSE: 4.7008\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [64, 32, 16, 8, 4], 'learning_rate': 0.001} => MSE: 159.5288\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 32, 32, 32, 32], 'learning_rate': 0.01} => MSE: 3.4409\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 32, 32, 32, 32], 'learning_rate': 0.001} => MSE: 8.0887\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 64, 64, 32, 16], 'learning_rate': 0.01} => MSE: 3.6752\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [32, 64, 64, 32, 16], 'learning_rate': 0.001} => MSE: 17.4716\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [64, 128, 256, 128, 64], 'learning_rate': 0.01} => MSE: 3.2810\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [64, 128, 256, 128, 64], 'learning_rate': 0.001} => MSE: 4.3686\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [128, 256, 256, 128, 64], 'learning_rate': 0.01} => MSE: 3.3449\n",
            "Testing parameters: {'batch_size': 128, 'hidden_sizes': [128, 256, 256, 128, 64], 'learning_rate': 0.001} => MSE: 4.5901\n",
            "\n",
            "Best parameters: {'batch_size': 128, 'hidden_sizes': [64, 128, 256, 128, 64], 'learning_rate': 0.01} with a MSE of 3.2810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-3 Epoch 최적화(Loss 시각화)"
      ],
      "metadata": {
        "id": "tuDvEydJ69dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련 함수 정의(loss 리스트 반환)"
      ],
      "metadata": {
        "id": "Ah6TMcv672zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_visualize(mode, hidden_sizes, learning_rate, batch_size):\n",
        "\n",
        "    model = mode_model(mode, hidden_sizes).cuda()\n",
        "    criterion = nn.MSELoss().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(500):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/500, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "ggxtAs_H73kK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = retrain_visualize(mode, best_params['hidden_sizes'], best_params['learning_rate'], best_params['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNXkcNfr9M99",
        "outputId": "125ec8fa-9a22-40d6-fc50-f851a40c2204"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 1362.1020, Validation Loss: 11.3178\n",
            "Epoch 2/100, Train Loss: 162.8709, Validation Loss: 25.9197\n",
            "Epoch 3/100, Train Loss: 152.9725, Validation Loss: 14.3421\n",
            "Epoch 4/100, Train Loss: 142.7468, Validation Loss: 14.8515\n",
            "Epoch 5/100, Train Loss: 134.8041, Validation Loss: 8.0030\n",
            "Epoch 6/100, Train Loss: 127.7205, Validation Loss: 10.2070\n",
            "Epoch 7/100, Train Loss: 125.1028, Validation Loss: 12.1648\n",
            "Epoch 8/100, Train Loss: 118.2551, Validation Loss: 5.9717\n",
            "Epoch 9/100, Train Loss: 113.7681, Validation Loss: 6.1274\n",
            "Epoch 10/100, Train Loss: 111.9445, Validation Loss: 5.4404\n",
            "Epoch 11/100, Train Loss: 110.4407, Validation Loss: 6.2491\n",
            "Epoch 12/100, Train Loss: 105.0876, Validation Loss: 10.0406\n",
            "Epoch 13/100, Train Loss: 103.8301, Validation Loss: 4.2311\n",
            "Epoch 14/100, Train Loss: 100.6404, Validation Loss: 6.6670\n",
            "Epoch 15/100, Train Loss: 98.3155, Validation Loss: 8.7716\n",
            "Epoch 16/100, Train Loss: 95.0510, Validation Loss: 7.5357\n",
            "Epoch 17/100, Train Loss: 90.8443, Validation Loss: 3.9365\n",
            "Epoch 18/100, Train Loss: 87.3312, Validation Loss: 3.9060\n",
            "Epoch 19/100, Train Loss: 85.2946, Validation Loss: 5.8800\n",
            "Epoch 20/100, Train Loss: 82.3183, Validation Loss: 6.2318\n",
            "Epoch 21/100, Train Loss: 77.6136, Validation Loss: 3.8715\n",
            "Epoch 22/100, Train Loss: 75.1418, Validation Loss: 3.8042\n",
            "Epoch 23/100, Train Loss: 73.2070, Validation Loss: 4.4478\n",
            "Epoch 24/100, Train Loss: 69.9742, Validation Loss: 5.5382\n",
            "Epoch 25/100, Train Loss: 67.6764, Validation Loss: 5.3622\n",
            "Epoch 26/100, Train Loss: 65.3803, Validation Loss: 4.2256\n",
            "Epoch 27/100, Train Loss: 60.7877, Validation Loss: 3.5612\n",
            "Epoch 28/100, Train Loss: 58.3952, Validation Loss: 5.0488\n",
            "Epoch 29/100, Train Loss: 56.0033, Validation Loss: 3.7401\n",
            "Epoch 30/100, Train Loss: 52.8487, Validation Loss: 3.8229\n",
            "Epoch 31/100, Train Loss: 50.5865, Validation Loss: 4.6260\n",
            "Epoch 32/100, Train Loss: 47.2983, Validation Loss: 3.4119\n",
            "Epoch 33/100, Train Loss: 44.9599, Validation Loss: 3.9060\n",
            "Epoch 34/100, Train Loss: 42.6055, Validation Loss: 5.0566\n",
            "Epoch 35/100, Train Loss: 40.3197, Validation Loss: 4.0707\n",
            "Epoch 36/100, Train Loss: 37.7815, Validation Loss: 4.5975\n",
            "Epoch 37/100, Train Loss: 35.7682, Validation Loss: 3.6476\n",
            "Epoch 38/100, Train Loss: 33.0109, Validation Loss: 3.3562\n",
            "Epoch 39/100, Train Loss: 31.6293, Validation Loss: 3.3231\n",
            "Epoch 40/100, Train Loss: 29.2428, Validation Loss: 3.9222\n",
            "Epoch 41/100, Train Loss: 27.0789, Validation Loss: 3.8919\n",
            "Epoch 42/100, Train Loss: 25.5550, Validation Loss: 3.2457\n",
            "Epoch 43/100, Train Loss: 23.8767, Validation Loss: 5.1678\n",
            "Epoch 44/100, Train Loss: 21.9838, Validation Loss: 4.0254\n",
            "Epoch 45/100, Train Loss: 20.2130, Validation Loss: 3.4210\n",
            "Epoch 46/100, Train Loss: 18.7434, Validation Loss: 3.7913\n",
            "Epoch 47/100, Train Loss: 17.5314, Validation Loss: 3.5406\n",
            "Epoch 48/100, Train Loss: 16.2593, Validation Loss: 4.3899\n",
            "Epoch 49/100, Train Loss: 14.8345, Validation Loss: 3.5188\n",
            "Epoch 50/100, Train Loss: 13.8721, Validation Loss: 4.1948\n",
            "Epoch 51/100, Train Loss: 12.8015, Validation Loss: 3.5673\n",
            "Epoch 52/100, Train Loss: 11.9131, Validation Loss: 3.3417\n",
            "Epoch 53/100, Train Loss: 10.8537, Validation Loss: 3.9162\n",
            "Epoch 54/100, Train Loss: 9.9690, Validation Loss: 3.6146\n",
            "Epoch 55/100, Train Loss: 9.0071, Validation Loss: 3.4802\n",
            "Epoch 56/100, Train Loss: 8.4865, Validation Loss: 3.4234\n",
            "Epoch 57/100, Train Loss: 7.7754, Validation Loss: 3.5910\n",
            "Epoch 58/100, Train Loss: 7.1765, Validation Loss: 3.2820\n",
            "Epoch 59/100, Train Loss: 6.7466, Validation Loss: 3.4033\n",
            "Epoch 60/100, Train Loss: 6.2620, Validation Loss: 3.5549\n",
            "Epoch 61/100, Train Loss: 5.8049, Validation Loss: 3.2616\n",
            "Epoch 62/100, Train Loss: 5.4102, Validation Loss: 3.3009\n",
            "Epoch 63/100, Train Loss: 5.0443, Validation Loss: 3.4421\n",
            "Epoch 64/100, Train Loss: 4.7702, Validation Loss: 3.4649\n",
            "Epoch 65/100, Train Loss: 4.4394, Validation Loss: 3.2681\n",
            "Epoch 66/100, Train Loss: 4.2315, Validation Loss: 3.3241\n",
            "Epoch 67/100, Train Loss: 4.0594, Validation Loss: 3.2540\n",
            "Epoch 68/100, Train Loss: 3.8969, Validation Loss: 3.4621\n",
            "Epoch 69/100, Train Loss: 3.7207, Validation Loss: 3.2650\n",
            "Epoch 70/100, Train Loss: 3.6616, Validation Loss: 3.2886\n",
            "Epoch 71/100, Train Loss: 3.5458, Validation Loss: 3.2950\n",
            "Epoch 72/100, Train Loss: 3.4463, Validation Loss: 3.3745\n",
            "Epoch 73/100, Train Loss: 3.4255, Validation Loss: 3.3649\n",
            "Epoch 74/100, Train Loss: 3.2892, Validation Loss: 3.2734\n",
            "Epoch 75/100, Train Loss: 3.2965, Validation Loss: 3.2353\n",
            "Epoch 76/100, Train Loss: 3.2398, Validation Loss: 3.2366\n",
            "Epoch 77/100, Train Loss: 3.1754, Validation Loss: 3.2703\n",
            "Epoch 78/100, Train Loss: 3.1499, Validation Loss: 3.4237\n",
            "Epoch 79/100, Train Loss: 3.1208, Validation Loss: 3.2455\n",
            "Epoch 80/100, Train Loss: 3.1007, Validation Loss: 3.2784\n",
            "Epoch 81/100, Train Loss: 3.1395, Validation Loss: 3.4972\n",
            "Epoch 82/100, Train Loss: 3.1207, Validation Loss: 3.3359\n",
            "Epoch 83/100, Train Loss: 3.1030, Validation Loss: 3.2501\n",
            "Epoch 84/100, Train Loss: 3.0929, Validation Loss: 3.3623\n",
            "Epoch 85/100, Train Loss: 3.1268, Validation Loss: 3.3195\n",
            "Epoch 86/100, Train Loss: 3.0970, Validation Loss: 3.3960\n",
            "Epoch 87/100, Train Loss: 3.1453, Validation Loss: 3.2726\n",
            "Epoch 88/100, Train Loss: 3.1025, Validation Loss: 3.3049\n",
            "Epoch 89/100, Train Loss: 3.0740, Validation Loss: 3.3332\n",
            "Epoch 90/100, Train Loss: 3.0596, Validation Loss: 3.2185\n",
            "Epoch 91/100, Train Loss: 3.1086, Validation Loss: 3.2211\n",
            "Epoch 92/100, Train Loss: 3.0854, Validation Loss: 3.3452\n",
            "Epoch 93/100, Train Loss: 3.0520, Validation Loss: 3.2725\n",
            "Epoch 94/100, Train Loss: 3.0928, Validation Loss: 3.2672\n",
            "Epoch 95/100, Train Loss: 3.1148, Validation Loss: 3.2500\n",
            "Epoch 96/100, Train Loss: 3.0253, Validation Loss: 3.2697\n",
            "Epoch 97/100, Train Loss: 3.0988, Validation Loss: 3.3207\n",
            "Epoch 98/100, Train Loss: 3.0665, Validation Loss: 3.2882\n",
            "Epoch 99/100, Train Loss: 3.0506, Validation Loss: 3.2657\n",
            "Epoch 100/100, Train Loss: 3.0827, Validation Loss: 3.2153\n",
            "Epoch 101/100, Train Loss: 3.1000, Validation Loss: 3.3331\n",
            "Epoch 102/100, Train Loss: 3.0569, Validation Loss: 3.2200\n",
            "Epoch 103/100, Train Loss: 3.0704, Validation Loss: 3.3154\n",
            "Epoch 104/100, Train Loss: 3.0600, Validation Loss: 3.2923\n",
            "Epoch 105/100, Train Loss: 3.0274, Validation Loss: 3.3210\n",
            "Epoch 106/100, Train Loss: 3.0953, Validation Loss: 3.2391\n",
            "Epoch 107/100, Train Loss: 3.0643, Validation Loss: 3.2551\n",
            "Epoch 108/100, Train Loss: 3.0669, Validation Loss: 3.2923\n",
            "Epoch 109/100, Train Loss: 3.0727, Validation Loss: 3.3115\n",
            "Epoch 110/100, Train Loss: 3.0432, Validation Loss: 3.3266\n",
            "Epoch 111/100, Train Loss: 3.0363, Validation Loss: 3.2497\n",
            "Epoch 112/100, Train Loss: 3.0807, Validation Loss: 3.2675\n",
            "Epoch 113/100, Train Loss: 3.0709, Validation Loss: 3.3244\n",
            "Epoch 114/100, Train Loss: 3.0800, Validation Loss: 3.2549\n",
            "Epoch 115/100, Train Loss: 3.0309, Validation Loss: 3.3070\n",
            "Epoch 116/100, Train Loss: 3.0241, Validation Loss: 3.2677\n",
            "Epoch 117/100, Train Loss: 3.0174, Validation Loss: 3.2191\n",
            "Epoch 118/100, Train Loss: 2.9908, Validation Loss: 3.2810\n",
            "Epoch 119/100, Train Loss: 3.0256, Validation Loss: 3.3432\n",
            "Epoch 120/100, Train Loss: 3.0474, Validation Loss: 3.3293\n",
            "Epoch 121/100, Train Loss: 3.0448, Validation Loss: 3.2932\n",
            "Epoch 122/100, Train Loss: 3.0552, Validation Loss: 3.2610\n",
            "Epoch 123/100, Train Loss: 3.0340, Validation Loss: 3.4259\n",
            "Epoch 124/100, Train Loss: 2.9908, Validation Loss: 3.2991\n",
            "Epoch 125/100, Train Loss: 3.0511, Validation Loss: 3.2441\n",
            "Epoch 126/100, Train Loss: 3.0349, Validation Loss: 3.2343\n",
            "Epoch 127/100, Train Loss: 3.0221, Validation Loss: 3.2911\n",
            "Epoch 128/100, Train Loss: 3.0529, Validation Loss: 3.2458\n",
            "Epoch 129/100, Train Loss: 3.0458, Validation Loss: 3.3149\n",
            "Epoch 130/100, Train Loss: 3.0433, Validation Loss: 3.3011\n",
            "Epoch 131/100, Train Loss: 3.0081, Validation Loss: 3.4153\n",
            "Epoch 132/100, Train Loss: 3.0016, Validation Loss: 3.2924\n",
            "Epoch 133/100, Train Loss: 3.0248, Validation Loss: 3.2418\n",
            "Epoch 134/100, Train Loss: 3.0333, Validation Loss: 3.2318\n",
            "Epoch 135/100, Train Loss: 2.9978, Validation Loss: 3.2526\n",
            "Epoch 136/100, Train Loss: 3.0099, Validation Loss: 3.2546\n",
            "Epoch 137/100, Train Loss: 3.0245, Validation Loss: 3.3176\n",
            "Epoch 138/100, Train Loss: 2.9836, Validation Loss: 3.2553\n",
            "Epoch 139/100, Train Loss: 3.0281, Validation Loss: 3.2396\n",
            "Epoch 140/100, Train Loss: 3.0368, Validation Loss: 3.2442\n",
            "Epoch 141/100, Train Loss: 2.9756, Validation Loss: 3.2382\n",
            "Epoch 142/100, Train Loss: 3.0426, Validation Loss: 3.3082\n",
            "Epoch 143/100, Train Loss: 2.9996, Validation Loss: 3.4416\n",
            "Epoch 144/100, Train Loss: 3.0317, Validation Loss: 3.3443\n",
            "Epoch 145/100, Train Loss: 3.0386, Validation Loss: 3.2249\n",
            "Epoch 146/100, Train Loss: 3.0462, Validation Loss: 3.2673\n",
            "Epoch 147/100, Train Loss: 3.0203, Validation Loss: 3.3926\n",
            "Epoch 148/100, Train Loss: 3.0057, Validation Loss: 3.2763\n",
            "Epoch 149/100, Train Loss: 3.0331, Validation Loss: 3.4779\n",
            "Epoch 150/100, Train Loss: 3.0318, Validation Loss: 3.2958\n",
            "Epoch 151/100, Train Loss: 3.0286, Validation Loss: 3.3021\n",
            "Epoch 152/100, Train Loss: 2.9997, Validation Loss: 3.4100\n",
            "Epoch 153/100, Train Loss: 3.0358, Validation Loss: 3.3330\n",
            "Epoch 154/100, Train Loss: 2.9949, Validation Loss: 3.3366\n",
            "Epoch 155/100, Train Loss: 3.0031, Validation Loss: 3.2568\n",
            "Epoch 156/100, Train Loss: 3.0033, Validation Loss: 3.2771\n",
            "Epoch 157/100, Train Loss: 3.0288, Validation Loss: 3.4268\n",
            "Epoch 158/100, Train Loss: 3.0117, Validation Loss: 3.2298\n",
            "Epoch 159/100, Train Loss: 3.0246, Validation Loss: 3.3322\n",
            "Epoch 160/100, Train Loss: 2.9984, Validation Loss: 3.2494\n",
            "Epoch 161/100, Train Loss: 3.0206, Validation Loss: 3.3216\n",
            "Epoch 162/100, Train Loss: 3.0264, Validation Loss: 3.3348\n",
            "Epoch 163/100, Train Loss: 3.0224, Validation Loss: 3.2852\n",
            "Epoch 164/100, Train Loss: 2.9885, Validation Loss: 3.2579\n",
            "Epoch 165/100, Train Loss: 3.0062, Validation Loss: 3.2600\n",
            "Epoch 166/100, Train Loss: 3.0053, Validation Loss: 3.2340\n",
            "Epoch 167/100, Train Loss: 3.0221, Validation Loss: 3.3362\n",
            "Epoch 168/100, Train Loss: 3.0488, Validation Loss: 3.2459\n",
            "Epoch 169/100, Train Loss: 3.0207, Validation Loss: 3.3207\n",
            "Epoch 170/100, Train Loss: 3.0405, Validation Loss: 3.4777\n",
            "Epoch 171/100, Train Loss: 2.9996, Validation Loss: 3.2359\n",
            "Epoch 172/100, Train Loss: 2.9922, Validation Loss: 3.2337\n",
            "Epoch 173/100, Train Loss: 2.9933, Validation Loss: 3.3436\n",
            "Epoch 174/100, Train Loss: 3.0170, Validation Loss: 3.2965\n",
            "Epoch 175/100, Train Loss: 3.0103, Validation Loss: 3.3541\n",
            "Epoch 176/100, Train Loss: 2.9942, Validation Loss: 3.2513\n",
            "Epoch 177/100, Train Loss: 2.9916, Validation Loss: 3.2282\n",
            "Epoch 178/100, Train Loss: 2.9951, Validation Loss: 3.2753\n",
            "Epoch 179/100, Train Loss: 3.0089, Validation Loss: 3.3166\n",
            "Epoch 180/100, Train Loss: 2.9891, Validation Loss: 3.2510\n",
            "Epoch 181/100, Train Loss: 2.9816, Validation Loss: 3.2305\n",
            "Epoch 182/100, Train Loss: 3.0052, Validation Loss: 3.5624\n",
            "Epoch 183/100, Train Loss: 3.0372, Validation Loss: 3.2462\n",
            "Epoch 184/100, Train Loss: 2.9774, Validation Loss: 3.2852\n",
            "Epoch 185/100, Train Loss: 2.9846, Validation Loss: 3.2300\n",
            "Epoch 186/100, Train Loss: 3.0437, Validation Loss: 3.3978\n",
            "Epoch 187/100, Train Loss: 2.9994, Validation Loss: 3.3185\n",
            "Epoch 188/100, Train Loss: 3.0067, Validation Loss: 3.2620\n",
            "Epoch 189/100, Train Loss: 2.9839, Validation Loss: 3.3287\n",
            "Epoch 190/100, Train Loss: 2.9744, Validation Loss: 3.2827\n",
            "Epoch 191/100, Train Loss: 2.9812, Validation Loss: 3.4582\n",
            "Epoch 192/100, Train Loss: 2.9827, Validation Loss: 3.2433\n",
            "Epoch 193/100, Train Loss: 2.9945, Validation Loss: 3.2372\n",
            "Epoch 194/100, Train Loss: 2.9767, Validation Loss: 3.4391\n",
            "Epoch 195/100, Train Loss: 2.9787, Validation Loss: 3.2719\n",
            "Epoch 196/100, Train Loss: 2.9763, Validation Loss: 3.2506\n",
            "Epoch 197/100, Train Loss: 2.9818, Validation Loss: 3.3119\n",
            "Epoch 198/100, Train Loss: 2.9730, Validation Loss: 3.2773\n",
            "Epoch 199/100, Train Loss: 2.9866, Validation Loss: 3.3583\n",
            "Epoch 200/100, Train Loss: 2.9942, Validation Loss: 3.3655\n",
            "Epoch 201/100, Train Loss: 3.0087, Validation Loss: 3.2445\n",
            "Epoch 202/100, Train Loss: 2.9845, Validation Loss: 3.3438\n",
            "Epoch 203/100, Train Loss: 2.9966, Validation Loss: 3.5760\n",
            "Epoch 204/100, Train Loss: 2.9895, Validation Loss: 3.2830\n",
            "Epoch 205/100, Train Loss: 2.9764, Validation Loss: 3.3088\n",
            "Epoch 206/100, Train Loss: 2.9825, Validation Loss: 3.5154\n",
            "Epoch 207/100, Train Loss: 2.9891, Validation Loss: 3.2371\n",
            "Epoch 208/100, Train Loss: 2.9943, Validation Loss: 3.2294\n",
            "Epoch 209/100, Train Loss: 3.0086, Validation Loss: 3.2908\n",
            "Epoch 210/100, Train Loss: 3.0098, Validation Loss: 3.3210\n",
            "Epoch 211/100, Train Loss: 2.9726, Validation Loss: 3.3413\n",
            "Epoch 212/100, Train Loss: 2.9865, Validation Loss: 3.3606\n",
            "Epoch 213/100, Train Loss: 3.0171, Validation Loss: 3.2970\n",
            "Epoch 214/100, Train Loss: 2.9992, Validation Loss: 3.2379\n",
            "Epoch 215/100, Train Loss: 3.0111, Validation Loss: 3.3187\n",
            "Epoch 216/100, Train Loss: 2.9958, Validation Loss: 3.2272\n",
            "Epoch 217/100, Train Loss: 3.0069, Validation Loss: 3.3677\n",
            "Epoch 218/100, Train Loss: 2.9678, Validation Loss: 3.4465\n",
            "Epoch 219/100, Train Loss: 3.0003, Validation Loss: 3.3022\n",
            "Epoch 220/100, Train Loss: 2.9785, Validation Loss: 3.2986\n",
            "Epoch 221/100, Train Loss: 3.0246, Validation Loss: 3.4065\n",
            "Epoch 222/100, Train Loss: 2.9953, Validation Loss: 3.2428\n",
            "Epoch 223/100, Train Loss: 2.9701, Validation Loss: 3.3581\n",
            "Epoch 224/100, Train Loss: 3.0184, Validation Loss: 3.2576\n",
            "Epoch 225/100, Train Loss: 3.0123, Validation Loss: 3.2396\n",
            "Epoch 226/100, Train Loss: 3.0084, Validation Loss: 3.2508\n",
            "Epoch 227/100, Train Loss: 3.0103, Validation Loss: 3.2308\n",
            "Epoch 228/100, Train Loss: 2.9796, Validation Loss: 3.2828\n",
            "Epoch 229/100, Train Loss: 2.9898, Validation Loss: 3.2275\n",
            "Epoch 230/100, Train Loss: 2.9878, Validation Loss: 3.2552\n",
            "Epoch 231/100, Train Loss: 2.9886, Validation Loss: 3.2592\n",
            "Epoch 232/100, Train Loss: 2.9939, Validation Loss: 3.4463\n",
            "Epoch 233/100, Train Loss: 2.9959, Validation Loss: 3.5101\n",
            "Epoch 234/100, Train Loss: 2.9596, Validation Loss: 3.3155\n",
            "Epoch 235/100, Train Loss: 2.9883, Validation Loss: 3.2838\n",
            "Epoch 236/100, Train Loss: 3.0004, Validation Loss: 3.2402\n",
            "Epoch 237/100, Train Loss: 2.9884, Validation Loss: 3.4368\n",
            "Epoch 238/100, Train Loss: 3.0009, Validation Loss: 3.2877\n",
            "Epoch 239/100, Train Loss: 2.9739, Validation Loss: 3.3237\n",
            "Epoch 240/100, Train Loss: 2.9753, Validation Loss: 3.5072\n",
            "Epoch 241/100, Train Loss: 2.9974, Validation Loss: 3.2410\n",
            "Epoch 242/100, Train Loss: 2.9570, Validation Loss: 3.2678\n",
            "Epoch 243/100, Train Loss: 2.9977, Validation Loss: 3.2572\n",
            "Epoch 244/100, Train Loss: 2.9689, Validation Loss: 3.4679\n",
            "Epoch 245/100, Train Loss: 2.9697, Validation Loss: 3.2844\n",
            "Epoch 246/100, Train Loss: 2.9944, Validation Loss: 3.3327\n",
            "Epoch 247/100, Train Loss: 3.0006, Validation Loss: 3.2326\n",
            "Epoch 248/100, Train Loss: 2.9903, Validation Loss: 3.2720\n",
            "Epoch 249/100, Train Loss: 2.9995, Validation Loss: 3.2900\n",
            "Epoch 250/100, Train Loss: 3.0106, Validation Loss: 3.2409\n",
            "Epoch 251/100, Train Loss: 3.0139, Validation Loss: 3.3220\n",
            "Epoch 252/100, Train Loss: 2.9841, Validation Loss: 3.2968\n",
            "Epoch 253/100, Train Loss: 2.9706, Validation Loss: 3.2614\n",
            "Epoch 254/100, Train Loss: 2.9587, Validation Loss: 3.3302\n",
            "Epoch 255/100, Train Loss: 2.9773, Validation Loss: 3.2556\n",
            "Epoch 256/100, Train Loss: 2.9704, Validation Loss: 3.2303\n",
            "Epoch 257/100, Train Loss: 2.9797, Validation Loss: 3.2360\n",
            "Epoch 258/100, Train Loss: 2.9533, Validation Loss: 3.2790\n",
            "Epoch 259/100, Train Loss: 2.9937, Validation Loss: 3.3288\n",
            "Epoch 260/100, Train Loss: 2.9895, Validation Loss: 3.2672\n",
            "Epoch 261/100, Train Loss: 2.9819, Validation Loss: 3.2436\n",
            "Epoch 262/100, Train Loss: 2.9962, Validation Loss: 3.3972\n",
            "Epoch 263/100, Train Loss: 2.9707, Validation Loss: 3.2673\n",
            "Epoch 264/100, Train Loss: 2.9991, Validation Loss: 3.2866\n",
            "Epoch 265/100, Train Loss: 2.9935, Validation Loss: 3.3017\n",
            "Epoch 266/100, Train Loss: 2.9826, Validation Loss: 3.2507\n",
            "Epoch 267/100, Train Loss: 2.9720, Validation Loss: 3.2532\n",
            "Epoch 268/100, Train Loss: 2.9915, Validation Loss: 3.2342\n",
            "Epoch 269/100, Train Loss: 2.9710, Validation Loss: 3.2846\n",
            "Epoch 270/100, Train Loss: 3.0061, Validation Loss: 3.2596\n",
            "Epoch 271/100, Train Loss: 2.9908, Validation Loss: 3.2746\n",
            "Epoch 272/100, Train Loss: 2.9875, Validation Loss: 3.2770\n",
            "Epoch 273/100, Train Loss: 2.9646, Validation Loss: 3.3907\n",
            "Epoch 274/100, Train Loss: 2.9516, Validation Loss: 3.3544\n",
            "Epoch 275/100, Train Loss: 2.9835, Validation Loss: 3.2622\n",
            "Epoch 276/100, Train Loss: 2.9941, Validation Loss: 3.3961\n",
            "Epoch 277/100, Train Loss: 2.9849, Validation Loss: 3.2722\n",
            "Epoch 278/100, Train Loss: 2.9701, Validation Loss: 3.2328\n",
            "Epoch 279/100, Train Loss: 2.9519, Validation Loss: 3.3483\n",
            "Epoch 280/100, Train Loss: 3.0030, Validation Loss: 3.2662\n",
            "Epoch 281/100, Train Loss: 2.9746, Validation Loss: 3.3475\n",
            "Epoch 282/100, Train Loss: 2.9946, Validation Loss: 3.2808\n",
            "Epoch 283/100, Train Loss: 2.9861, Validation Loss: 3.5224\n",
            "Epoch 284/100, Train Loss: 2.9749, Validation Loss: 3.2978\n",
            "Epoch 285/100, Train Loss: 2.9854, Validation Loss: 3.4310\n",
            "Epoch 286/100, Train Loss: 2.9598, Validation Loss: 3.2998\n",
            "Epoch 287/100, Train Loss: 2.9867, Validation Loss: 3.2832\n",
            "Epoch 288/100, Train Loss: 2.9794, Validation Loss: 3.2314\n",
            "Epoch 289/100, Train Loss: 2.9803, Validation Loss: 3.3375\n",
            "Epoch 290/100, Train Loss: 2.9965, Validation Loss: 3.2712\n",
            "Epoch 291/100, Train Loss: 2.9668, Validation Loss: 3.3799\n",
            "Epoch 292/100, Train Loss: 2.9634, Validation Loss: 3.3669\n",
            "Epoch 293/100, Train Loss: 2.9937, Validation Loss: 3.4044\n",
            "Epoch 294/100, Train Loss: 2.9919, Validation Loss: 3.2935\n",
            "Epoch 295/100, Train Loss: 2.9462, Validation Loss: 3.2408\n",
            "Epoch 296/100, Train Loss: 2.9937, Validation Loss: 3.3421\n",
            "Epoch 297/100, Train Loss: 2.9661, Validation Loss: 3.2780\n",
            "Epoch 298/100, Train Loss: 3.0027, Validation Loss: 3.2900\n",
            "Epoch 299/100, Train Loss: 2.9641, Validation Loss: 3.2725\n",
            "Epoch 300/100, Train Loss: 2.9691, Validation Loss: 3.2799\n",
            "Epoch 301/100, Train Loss: 2.9808, Validation Loss: 3.2316\n",
            "Epoch 302/100, Train Loss: 3.0086, Validation Loss: 3.2970\n",
            "Epoch 303/100, Train Loss: 2.9759, Validation Loss: 3.2338\n",
            "Epoch 304/100, Train Loss: 2.9686, Validation Loss: 3.2522\n",
            "Epoch 305/100, Train Loss: 2.9777, Validation Loss: 3.3782\n",
            "Epoch 306/100, Train Loss: 2.9809, Validation Loss: 3.2486\n",
            "Epoch 307/100, Train Loss: 2.9852, Validation Loss: 3.3287\n",
            "Epoch 308/100, Train Loss: 2.9819, Validation Loss: 3.2836\n",
            "Epoch 309/100, Train Loss: 2.9730, Validation Loss: 3.2855\n",
            "Epoch 310/100, Train Loss: 3.0042, Validation Loss: 3.2344\n",
            "Epoch 311/100, Train Loss: 2.9719, Validation Loss: 3.3493\n",
            "Epoch 312/100, Train Loss: 2.9581, Validation Loss: 3.3300\n",
            "Epoch 313/100, Train Loss: 2.9912, Validation Loss: 3.2476\n",
            "Epoch 314/100, Train Loss: 2.9581, Validation Loss: 3.2904\n",
            "Epoch 315/100, Train Loss: 2.9744, Validation Loss: 3.2414\n",
            "Epoch 316/100, Train Loss: 2.9683, Validation Loss: 3.3006\n",
            "Epoch 317/100, Train Loss: 2.9962, Validation Loss: 3.2941\n",
            "Epoch 318/100, Train Loss: 2.9994, Validation Loss: 3.2978\n",
            "Epoch 319/100, Train Loss: 2.9636, Validation Loss: 3.3286\n",
            "Epoch 320/100, Train Loss: 2.9743, Validation Loss: 3.3064\n",
            "Epoch 321/100, Train Loss: 2.9791, Validation Loss: 3.3306\n",
            "Epoch 322/100, Train Loss: 2.9685, Validation Loss: 3.2576\n",
            "Epoch 323/100, Train Loss: 2.9919, Validation Loss: 3.3527\n",
            "Epoch 324/100, Train Loss: 2.9787, Validation Loss: 3.4242\n",
            "Epoch 325/100, Train Loss: 2.9828, Validation Loss: 3.3508\n",
            "Epoch 326/100, Train Loss: 2.9926, Validation Loss: 3.3385\n",
            "Epoch 327/100, Train Loss: 2.9553, Validation Loss: 3.2510\n",
            "Epoch 328/100, Train Loss: 2.9535, Validation Loss: 3.2670\n",
            "Epoch 329/100, Train Loss: 2.9669, Validation Loss: 3.2857\n",
            "Epoch 330/100, Train Loss: 3.0044, Validation Loss: 3.2615\n",
            "Epoch 331/100, Train Loss: 2.9717, Validation Loss: 3.2653\n",
            "Epoch 332/100, Train Loss: 2.9823, Validation Loss: 3.3143\n",
            "Epoch 333/100, Train Loss: 2.9521, Validation Loss: 3.3684\n",
            "Epoch 334/100, Train Loss: 2.9871, Validation Loss: 3.2696\n",
            "Epoch 335/100, Train Loss: 2.9643, Validation Loss: 3.3391\n",
            "Epoch 336/100, Train Loss: 2.9677, Validation Loss: 3.6262\n",
            "Epoch 337/100, Train Loss: 2.9858, Validation Loss: 3.2721\n",
            "Epoch 338/100, Train Loss: 2.9730, Validation Loss: 3.3598\n",
            "Epoch 339/100, Train Loss: 2.9807, Validation Loss: 3.2506\n",
            "Epoch 340/100, Train Loss: 2.9904, Validation Loss: 3.2880\n",
            "Epoch 341/100, Train Loss: 2.9487, Validation Loss: 3.2234\n",
            "Epoch 342/100, Train Loss: 2.9637, Validation Loss: 3.3466\n",
            "Epoch 343/100, Train Loss: 2.9376, Validation Loss: 3.2302\n",
            "Epoch 344/100, Train Loss: 2.9863, Validation Loss: 3.2575\n",
            "Epoch 345/100, Train Loss: 2.9802, Validation Loss: 3.2384\n",
            "Epoch 346/100, Train Loss: 2.9572, Validation Loss: 3.2332\n",
            "Epoch 347/100, Train Loss: 2.9786, Validation Loss: 3.2907\n",
            "Epoch 348/100, Train Loss: 2.9725, Validation Loss: 3.2535\n",
            "Epoch 349/100, Train Loss: 2.9876, Validation Loss: 3.3121\n",
            "Epoch 350/100, Train Loss: 3.0063, Validation Loss: 3.3382\n",
            "Epoch 351/100, Train Loss: 2.9802, Validation Loss: 3.3058\n",
            "Epoch 352/100, Train Loss: 2.9763, Validation Loss: 3.4768\n",
            "Epoch 353/100, Train Loss: 2.9765, Validation Loss: 3.4440\n",
            "Epoch 354/100, Train Loss: 3.0112, Validation Loss: 3.2372\n",
            "Epoch 355/100, Train Loss: 2.9643, Validation Loss: 3.2579\n",
            "Epoch 356/100, Train Loss: 2.9686, Validation Loss: 3.2877\n",
            "Epoch 357/100, Train Loss: 2.9726, Validation Loss: 3.2673\n",
            "Epoch 358/100, Train Loss: 2.9732, Validation Loss: 3.3086\n",
            "Epoch 359/100, Train Loss: 2.9705, Validation Loss: 3.2505\n",
            "Epoch 360/100, Train Loss: 2.9482, Validation Loss: 3.4945\n",
            "Epoch 361/100, Train Loss: 2.9695, Validation Loss: 3.2675\n",
            "Epoch 362/100, Train Loss: 2.9644, Validation Loss: 3.4380\n",
            "Epoch 363/100, Train Loss: 2.9582, Validation Loss: 3.3700\n",
            "Epoch 364/100, Train Loss: 2.9561, Validation Loss: 3.2669\n",
            "Epoch 365/100, Train Loss: 2.9673, Validation Loss: 3.3191\n",
            "Epoch 366/100, Train Loss: 2.9681, Validation Loss: 3.3135\n",
            "Epoch 367/100, Train Loss: 2.9635, Validation Loss: 3.2969\n",
            "Epoch 368/100, Train Loss: 2.9586, Validation Loss: 3.3177\n",
            "Epoch 369/100, Train Loss: 2.9661, Validation Loss: 3.2471\n",
            "Epoch 370/100, Train Loss: 2.9896, Validation Loss: 3.2580\n",
            "Epoch 371/100, Train Loss: 2.9967, Validation Loss: 3.2632\n",
            "Epoch 372/100, Train Loss: 2.9743, Validation Loss: 3.4802\n",
            "Epoch 373/100, Train Loss: 2.9858, Validation Loss: 3.2913\n",
            "Epoch 374/100, Train Loss: 2.9701, Validation Loss: 3.3010\n",
            "Epoch 375/100, Train Loss: 3.0123, Validation Loss: 3.3089\n",
            "Epoch 376/100, Train Loss: 2.9457, Validation Loss: 3.2949\n",
            "Epoch 377/100, Train Loss: 2.9791, Validation Loss: 3.4071\n",
            "Epoch 378/100, Train Loss: 2.9829, Validation Loss: 3.3116\n",
            "Epoch 379/100, Train Loss: 2.9789, Validation Loss: 3.2634\n",
            "Epoch 380/100, Train Loss: 2.9655, Validation Loss: 3.3195\n",
            "Epoch 381/100, Train Loss: 2.9521, Validation Loss: 3.3088\n",
            "Epoch 382/100, Train Loss: 3.0036, Validation Loss: 3.2657\n",
            "Epoch 383/100, Train Loss: 2.9558, Validation Loss: 3.2391\n",
            "Epoch 384/100, Train Loss: 2.9756, Validation Loss: 3.3029\n",
            "Epoch 385/100, Train Loss: 2.9356, Validation Loss: 3.2810\n",
            "Epoch 386/100, Train Loss: 2.9665, Validation Loss: 3.3063\n",
            "Epoch 387/100, Train Loss: 2.9760, Validation Loss: 3.2349\n",
            "Epoch 388/100, Train Loss: 2.9781, Validation Loss: 3.2630\n",
            "Epoch 389/100, Train Loss: 2.9720, Validation Loss: 3.4382\n",
            "Epoch 390/100, Train Loss: 2.9714, Validation Loss: 3.6860\n",
            "Epoch 391/100, Train Loss: 2.9618, Validation Loss: 3.3165\n",
            "Epoch 392/100, Train Loss: 2.9441, Validation Loss: 3.2796\n",
            "Epoch 393/100, Train Loss: 2.9500, Validation Loss: 3.2545\n",
            "Epoch 394/100, Train Loss: 2.9694, Validation Loss: 3.3410\n",
            "Epoch 395/100, Train Loss: 2.9746, Validation Loss: 3.2369\n",
            "Epoch 396/100, Train Loss: 2.9641, Validation Loss: 3.3188\n",
            "Epoch 397/100, Train Loss: 2.9390, Validation Loss: 3.3738\n",
            "Epoch 398/100, Train Loss: 2.9479, Validation Loss: 3.2958\n",
            "Epoch 399/100, Train Loss: 2.9846, Validation Loss: 3.4016\n",
            "Epoch 400/100, Train Loss: 2.9540, Validation Loss: 3.3156\n",
            "Epoch 401/100, Train Loss: 2.9798, Validation Loss: 3.3291\n",
            "Epoch 402/100, Train Loss: 2.9612, Validation Loss: 3.2733\n",
            "Epoch 403/100, Train Loss: 2.9768, Validation Loss: 3.3781\n",
            "Epoch 404/100, Train Loss: 2.9707, Validation Loss: 3.3339\n",
            "Epoch 405/100, Train Loss: 2.9608, Validation Loss: 3.2242\n",
            "Epoch 406/100, Train Loss: 2.9918, Validation Loss: 3.3918\n",
            "Epoch 407/100, Train Loss: 2.9836, Validation Loss: 3.2468\n",
            "Epoch 408/100, Train Loss: 2.9590, Validation Loss: 3.2312\n",
            "Epoch 409/100, Train Loss: 2.9992, Validation Loss: 3.2583\n",
            "Epoch 410/100, Train Loss: 2.9640, Validation Loss: 3.2726\n",
            "Epoch 411/100, Train Loss: 2.9918, Validation Loss: 3.2644\n",
            "Epoch 412/100, Train Loss: 2.9767, Validation Loss: 3.3697\n",
            "Epoch 413/100, Train Loss: 2.9856, Validation Loss: 3.3410\n",
            "Epoch 414/100, Train Loss: 2.9595, Validation Loss: 3.3142\n",
            "Epoch 415/100, Train Loss: 2.9840, Validation Loss: 3.2691\n",
            "Epoch 416/100, Train Loss: 2.9461, Validation Loss: 3.3868\n",
            "Epoch 417/100, Train Loss: 2.9595, Validation Loss: 3.2227\n",
            "Epoch 418/100, Train Loss: 2.9565, Validation Loss: 3.2889\n",
            "Epoch 419/100, Train Loss: 2.9521, Validation Loss: 3.3825\n",
            "Epoch 420/100, Train Loss: 2.9832, Validation Loss: 3.2771\n",
            "Epoch 421/100, Train Loss: 2.9652, Validation Loss: 3.2237\n",
            "Epoch 422/100, Train Loss: 2.9604, Validation Loss: 3.2453\n",
            "Epoch 423/100, Train Loss: 2.9536, Validation Loss: 3.2040\n",
            "Epoch 424/100, Train Loss: 2.9911, Validation Loss: 3.2401\n",
            "Epoch 425/100, Train Loss: 2.9571, Validation Loss: 3.3537\n",
            "Epoch 426/100, Train Loss: 2.9599, Validation Loss: 3.2982\n",
            "Epoch 427/100, Train Loss: 2.9565, Validation Loss: 3.2964\n",
            "Epoch 428/100, Train Loss: 2.9911, Validation Loss: 3.2902\n",
            "Epoch 429/100, Train Loss: 2.9740, Validation Loss: 3.3072\n",
            "Epoch 430/100, Train Loss: 2.9552, Validation Loss: 3.2467\n",
            "Epoch 431/100, Train Loss: 2.9811, Validation Loss: 3.2231\n",
            "Epoch 432/100, Train Loss: 2.9659, Validation Loss: 3.4495\n",
            "Epoch 433/100, Train Loss: 2.9646, Validation Loss: 3.3521\n",
            "Epoch 434/100, Train Loss: 2.9691, Validation Loss: 3.3695\n",
            "Epoch 435/100, Train Loss: 2.9700, Validation Loss: 3.3349\n",
            "Epoch 436/100, Train Loss: 2.9739, Validation Loss: 3.2426\n",
            "Epoch 437/100, Train Loss: 2.9851, Validation Loss: 3.2607\n",
            "Epoch 438/100, Train Loss: 2.9798, Validation Loss: 3.2801\n",
            "Epoch 439/100, Train Loss: 2.9751, Validation Loss: 3.3683\n",
            "Epoch 440/100, Train Loss: 2.9678, Validation Loss: 3.2435\n",
            "Epoch 441/100, Train Loss: 2.9474, Validation Loss: 3.3196\n",
            "Epoch 442/100, Train Loss: 2.9922, Validation Loss: 3.2701\n",
            "Epoch 443/100, Train Loss: 2.9687, Validation Loss: 3.3524\n",
            "Epoch 444/100, Train Loss: 2.9762, Validation Loss: 3.2466\n",
            "Epoch 445/100, Train Loss: 2.9825, Validation Loss: 3.2468\n",
            "Epoch 446/100, Train Loss: 2.9872, Validation Loss: 3.2416\n",
            "Epoch 447/100, Train Loss: 2.9701, Validation Loss: 3.3398\n",
            "Epoch 448/100, Train Loss: 2.9584, Validation Loss: 3.3258\n",
            "Epoch 449/100, Train Loss: 2.9786, Validation Loss: 3.3230\n",
            "Epoch 450/100, Train Loss: 2.9531, Validation Loss: 3.2213\n",
            "Epoch 451/100, Train Loss: 2.9720, Validation Loss: 3.2383\n",
            "Epoch 452/100, Train Loss: 2.9489, Validation Loss: 3.2659\n",
            "Epoch 453/100, Train Loss: 2.9602, Validation Loss: 3.2258\n",
            "Epoch 454/100, Train Loss: 2.9744, Validation Loss: 3.3681\n",
            "Epoch 455/100, Train Loss: 2.9663, Validation Loss: 3.2374\n",
            "Epoch 456/100, Train Loss: 2.9582, Validation Loss: 3.2768\n",
            "Epoch 457/100, Train Loss: 2.9521, Validation Loss: 3.2732\n",
            "Epoch 458/100, Train Loss: 2.9642, Validation Loss: 3.3716\n",
            "Epoch 459/100, Train Loss: 2.9795, Validation Loss: 3.2886\n",
            "Epoch 460/100, Train Loss: 2.9539, Validation Loss: 3.2637\n",
            "Epoch 461/100, Train Loss: 2.9869, Validation Loss: 3.2246\n",
            "Epoch 462/100, Train Loss: 2.9386, Validation Loss: 3.3210\n",
            "Epoch 463/100, Train Loss: 2.9689, Validation Loss: 3.2965\n",
            "Epoch 464/100, Train Loss: 2.9455, Validation Loss: 3.3097\n",
            "Epoch 465/100, Train Loss: 2.9824, Validation Loss: 3.2209\n",
            "Epoch 466/100, Train Loss: 2.9845, Validation Loss: 3.2462\n",
            "Epoch 467/100, Train Loss: 2.9746, Validation Loss: 3.3172\n",
            "Epoch 468/100, Train Loss: 2.9993, Validation Loss: 3.4238\n",
            "Epoch 469/100, Train Loss: 2.9418, Validation Loss: 3.3314\n",
            "Epoch 470/100, Train Loss: 2.9501, Validation Loss: 3.3914\n",
            "Epoch 471/100, Train Loss: 2.9646, Validation Loss: 3.3249\n",
            "Epoch 472/100, Train Loss: 2.9920, Validation Loss: 3.3533\n",
            "Epoch 473/100, Train Loss: 2.9550, Validation Loss: 3.2963\n",
            "Epoch 474/100, Train Loss: 2.9441, Validation Loss: 3.2432\n",
            "Epoch 475/100, Train Loss: 2.9957, Validation Loss: 3.3344\n",
            "Epoch 476/100, Train Loss: 2.9572, Validation Loss: 3.2997\n",
            "Epoch 477/100, Train Loss: 2.9506, Validation Loss: 3.2460\n",
            "Epoch 478/100, Train Loss: 2.9885, Validation Loss: 3.3521\n",
            "Epoch 479/100, Train Loss: 2.9585, Validation Loss: 3.2543\n",
            "Epoch 480/100, Train Loss: 2.9498, Validation Loss: 3.2514\n",
            "Epoch 481/100, Train Loss: 2.9487, Validation Loss: 3.4041\n",
            "Epoch 482/100, Train Loss: 2.9746, Validation Loss: 3.3289\n",
            "Epoch 483/100, Train Loss: 2.9739, Validation Loss: 3.3512\n",
            "Epoch 484/100, Train Loss: 2.9704, Validation Loss: 3.2396\n",
            "Epoch 485/100, Train Loss: 2.9568, Validation Loss: 3.2579\n",
            "Epoch 486/100, Train Loss: 2.9834, Validation Loss: 3.5423\n",
            "Epoch 487/100, Train Loss: 2.9692, Validation Loss: 3.2444\n",
            "Epoch 488/100, Train Loss: 2.9717, Validation Loss: 3.2778\n",
            "Epoch 489/100, Train Loss: 2.9446, Validation Loss: 3.2389\n",
            "Epoch 490/100, Train Loss: 2.9886, Validation Loss: 3.4108\n",
            "Epoch 491/100, Train Loss: 2.9765, Validation Loss: 3.2507\n",
            "Epoch 492/100, Train Loss: 2.9923, Validation Loss: 3.3853\n",
            "Epoch 493/100, Train Loss: 2.9448, Validation Loss: 3.2290\n",
            "Epoch 494/100, Train Loss: 2.9969, Validation Loss: 3.4163\n",
            "Epoch 495/100, Train Loss: 2.9597, Validation Loss: 3.2357\n",
            "Epoch 496/100, Train Loss: 2.9542, Validation Loss: 3.3884\n",
            "Epoch 497/100, Train Loss: 2.9835, Validation Loss: 3.2358\n",
            "Epoch 498/100, Train Loss: 2.9466, Validation Loss: 3.2284\n",
            "Epoch 499/100, Train Loss: 2.9517, Validation Loss: 3.2618\n",
            "Epoch 500/100, Train Loss: 2.9441, Validation Loss: 3.2569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train / validation loss 시각화"
      ],
      "metadata": {
        "id": "pda8AaAo9XYn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "4eMul0RHdfy-",
        "outputId": "d44fa297-e5b5-4f24-b369-8035ef9bf9db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Validation Loss')\n",
        "plt.ylim(0, 8)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(f'{mode}_loss.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 모델 훈련 및 테스트"
      ],
      "metadata": {
        "id": "O-TNs_lB99EV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-1 best param 불러오기 (파일)  "
      ],
      "metadata": {
        "id": "uRdPTK-H-S5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### best parameter 를 파일로 불러오고 싶거나 모델 하이퍼파라미터 최적화 안했을 경우만 실행"
      ],
      "metadata": {
        "id": "D373Xod--nhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = \"\"\n",
        "best_params = json.load(open(params_path, 'r'))"
      ],
      "metadata": {
        "id": "s6I6MI7v-58n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-2 DataLoader"
      ],
      "metadata": {
        "id": "My0dfZ6t_NNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dataloader(train_csv_path):\n",
        "    train_data = pd.read_csv(train_csv_path)\n",
        "\n",
        "    X_train = train_data.iloc[:,1:-1].values\n",
        "    y_train = train_data.iloc[:,-1].values\n",
        "\n",
        "    X_train = torch.tensor(X_train, dtype = torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype = torch.float32).view(-1,1)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "\n",
        "    return train_dataset\n",
        "\n",
        "def test_dataloader(test_csv_path):\n",
        "    test_data = pd.read_csv(test_csv_path)\n",
        "\n",
        "    X_test = pd.read_csv(test_csv_path).iloc[:,1:].values\n",
        "    X_test = torch.tensor(X_test, dtype = torch.float32)\n",
        "\n",
        "    test_dataset = TensorDataset(X_test)\n",
        "\n",
        "    return test_dataset\n",
        "\n",
        "train_dataset = train_dataloader(train_csv_path)\n",
        "test_dataset = test_dataloader(test_csv_path)"
      ],
      "metadata": {
        "id": "AMapvgu8_bFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-4 Train"
      ],
      "metadata": {
        "id": "9gCIar4JAECo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test(mode, num_epochs, hidden_sizes, learning_rate, batch_size):\n",
        "\n",
        "    model = mode_model(mode, hidden_sizes).cuda()\n",
        "    criterion = nn.MSELoss().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        '''\n",
        "        나중에 중간 중간 parameter 저장 예정\n",
        "        '''\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}')\n",
        "\n",
        "    model_save_path = f\"{mode}_epoch{num_epochs}.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    model.eval()\n",
        "    test_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch_X = batch[0].cuda()\n",
        "            outputs = model(batch_X)\n",
        "            test_pred.extend(outputs.cpu().numpy())\n",
        "\n",
        "    test_pred = np.array(test_pred)\n",
        "    return test_pred\n",
        "\n",
        "num_epochs = 100\n",
        "test_pred = train_test(mode, num_epochs, best_params['hidden_sizes'], best_params['learning_rate'], best_params['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "8ar9j0zBAPcf",
        "outputId": "eb9999e8-e8b2-45ea-ddde-4058b1a3358f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 367.7600\n",
            "Epoch 2/100, Train Loss: 4.0501\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a43ceaefd97a>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_sizes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-a43ceaefd97a>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(mode, num_epochs, hidden_sizes, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/decorators.py\u001b[0m in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minnermost_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDisableContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDisableContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         if (\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrace_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m             and (\n\u001b[1;32m    412\u001b[0m                 \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_call_impl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_wrapped_call_impl\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m   3376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inlined_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inlined_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskipped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36mcheck_verbose\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m     \u001b[0;31m# Consulte the central trace rules defined in torch._dynamo.trace_rules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m     rule = torch._dynamo.trace_rules.lookup_inner(\n\u001b[0m\u001b[1;32m   3362\u001b[0m         \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inlined_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36mlookup_inner\u001b[0;34m(obj, name, filename, is_direct_call)\u001b[0m\n\u001b[1;32m   3438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3440\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_aten_op_or_tensor_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTorchInGraphFunctionVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3442\u001b[0m         \u001b[0mrule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_torch_obj_rule_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36mis_aten_op_or_tensor_method\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2840\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_aten_op_or_tensor_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2841\u001b[0;31m     return obj in get_tensor_method() or isinstance(\n\u001b[0m\u001b[1;32m   2842\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpOverloadPacket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpOverload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-4 Test"
      ],
      "metadata": {
        "id": "wSBZi6AvBXQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83-9AKe1rvk9",
        "outputId": "d9138ad0-5bfa-470c-c53d-7bc4d85ba748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10% threshold: 93.3584\n",
            "Number of samples in top 10%: [499]\n"
          ]
        }
      ],
      "source": [
        "# 상위 10% 임계값 계산\n",
        "threshold = np.percentile(test_pred, 90)\n",
        "top_10_percent_mask = test_pred >= threshold\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission_df = pd.read_csv(submission_csv_path)\n",
        "submission_df['y'] = test_pred\n",
        "submission_df.to_csv(f'{mode}_{method}_epoch{num_epochs}.csv', index=False)\n",
        "\n",
        "# 결과 저장\n",
        "log_file_path = 'TOP10_threshold.txt'\n",
        "with open(log_file_path, 'w') as log_file:\n",
        "    log_file.write(f\"Top 10% threshold: {threshold:.4f}\\n\")\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"Top 10% threshold: {threshold:.4f}\")\n",
        "print(f\"Number of samples in top 10%: {sum(top_10_percent_mask)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold 가장 높은 값 Test [main]"
      ],
      "metadata": {
        "id": "ShZC-wGDF0aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "가장 높은 값의 Threshold 를 가지는 Test 결과를 얻을 수 있지만 조금 더 오래걸림.\n",
        "매 epoch 마다 test 진행"
      ],
      "metadata": {
        "id": "4Fy6V7TOGGMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params 파일로 불러오거나 하이퍼파라미터 최적화 수행 안했을 때만 실행\n",
        "\n",
        "params_path = \"/content/5_layer_MLP_params.json\"\n",
        "best_params = json.load(open(params_path, 'r'))"
      ],
      "metadata": {
        "id": "6PTAlIJJF7nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ii6gjj0K68R",
        "outputId": "a3daffb6-846e-4f1b-c670-d9247eb0f959"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 128,\n",
              " 'hidden_sizes': [64, 128, 256, 128, 64],\n",
              " 'learning_rate': 0.01}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에서 정의했으면 안해도 됨\n",
        "mode = 'MLP5Hidden' # 사용할 모델 class 이름 : [모델 정의 참조]\n",
        "method = \"GS\" # 사용할 모델 하이퍼파라미터 최적화 알고리즘 이름"
      ],
      "metadata": {
        "id": "VcfXOuxcKjVv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dataloader(train_csv_path):\n",
        "    train_data = pd.read_csv(train_csv_path)\n",
        "\n",
        "    X_train = train_data.iloc[:,1:-1].values\n",
        "    y_train = train_data.iloc[:,-1].values\n",
        "\n",
        "    X_train = torch.tensor(X_train, dtype = torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype = torch.float32).view(-1,1)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "\n",
        "    return train_dataset\n",
        "\n",
        "def test_dataloader(test_csv_path):\n",
        "    test_data = pd.read_csv(test_csv_path)\n",
        "\n",
        "    X_test = pd.read_csv(test_csv_path).iloc[:,1:].values\n",
        "    X_test = torch.tensor(X_test, dtype = torch.float32)\n",
        "\n",
        "    test_dataset = TensorDataset(X_test)\n",
        "\n",
        "    return test_dataset\n",
        "\n",
        "train_dataset = train_dataloader(train_csv_path)\n",
        "test_dataset = test_dataloader(test_csv_path)"
      ],
      "metadata": {
        "id": "FCcGtueqGZP4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_test(mode, num_epochs, hidden_sizes, learning_rate, batch_size):\n",
        "\n",
        "    loss_file_path = 'train_loss.txt'\n",
        "    threshold_file_path = 'threshold.txt'\n",
        "    loss_log = open(loss_file_path, 'w')\n",
        "    threshold_log = open(threshold_file_path, 'w')\n",
        "\n",
        "    model = mode_model(mode, hidden_sizes).cuda()\n",
        "    criterion = nn.MSELoss().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    max_threshold = 0.0\n",
        "    best_epoch = 0\n",
        "    best_pred = None\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}')\n",
        "        loss_log.write(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\\n')\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        test_pred = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch_X = batch[0].cuda()\n",
        "                outputs = model(batch_X)\n",
        "                test_pred.extend(outputs.cpu().numpy())\n",
        "\n",
        "        test_pred = np.array(test_pred)\n",
        "        threshold = np.percentile(test_pred, 90)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Top 10% threshold: {threshold:.4f}\")\n",
        "        threshold_log.write(f\"Epoch {epoch+1}/{num_epochs}, Top 10% threshold: {threshold:.4f}\\n\")\n",
        "\n",
        "        if threshold > max_threshold:\n",
        "            max_threshold = threshold\n",
        "            best_epoch = epoch+1\n",
        "            best_pred = test_pred\n",
        "\n",
        "    loss_log.close()\n",
        "    threshold_log.close()\n",
        "    return best_pred, best_epoch, test_pred\n",
        "\n",
        "num_epochs = 300\n",
        "best_pred, best_epoch, last_pred  = get_max_test(mode, num_epochs, best_params['hidden_sizes'], best_params['learning_rate'], best_params['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27_SFOneGbvJ",
        "outputId": "9a594104-5efd-4133-b405-63a084399820"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Train Loss: 1130.1131\n",
            "Epoch 1/300, Top 10% threshold: 89.9993\n",
            "Epoch 2/300, Train Loss: 165.2049\n",
            "Epoch 2/300, Top 10% threshold: 87.0240\n",
            "Epoch 3/300, Train Loss: 150.6404\n",
            "Epoch 3/300, Top 10% threshold: 82.2825\n",
            "Epoch 4/300, Train Loss: 138.1146\n",
            "Epoch 4/300, Top 10% threshold: 92.2973\n",
            "Epoch 5/300, Train Loss: 129.2171\n",
            "Epoch 5/300, Top 10% threshold: 88.2449\n",
            "Epoch 6/300, Train Loss: 125.6304\n",
            "Epoch 6/300, Top 10% threshold: 85.7442\n",
            "Epoch 7/300, Train Loss: 118.9308\n",
            "Epoch 7/300, Top 10% threshold: 85.9580\n",
            "Epoch 8/300, Train Loss: 111.1294\n",
            "Epoch 8/300, Top 10% threshold: 86.9467\n",
            "Epoch 9/300, Train Loss: 107.4864\n",
            "Epoch 9/300, Top 10% threshold: 84.8815\n",
            "Epoch 10/300, Train Loss: 102.6980\n",
            "Epoch 10/300, Top 10% threshold: 84.7950\n",
            "Epoch 11/300, Train Loss: 98.7920\n",
            "Epoch 11/300, Top 10% threshold: 85.8580\n",
            "Epoch 12/300, Train Loss: 96.7163\n",
            "Epoch 12/300, Top 10% threshold: 86.4825\n",
            "Epoch 13/300, Train Loss: 92.7446\n",
            "Epoch 13/300, Top 10% threshold: 87.4138\n",
            "Epoch 14/300, Train Loss: 89.5084\n",
            "Epoch 14/300, Top 10% threshold: 87.7274\n",
            "Epoch 15/300, Train Loss: 85.4874\n",
            "Epoch 15/300, Top 10% threshold: 86.8546\n",
            "Epoch 16/300, Train Loss: 82.5788\n",
            "Epoch 16/300, Top 10% threshold: 86.5718\n",
            "Epoch 17/300, Train Loss: 78.6273\n",
            "Epoch 17/300, Top 10% threshold: 88.4940\n",
            "Epoch 18/300, Train Loss: 75.5159\n",
            "Epoch 18/300, Top 10% threshold: 89.9161\n",
            "Epoch 19/300, Train Loss: 70.5005\n",
            "Epoch 19/300, Top 10% threshold: 89.2700\n",
            "Epoch 20/300, Train Loss: 67.1491\n",
            "Epoch 20/300, Top 10% threshold: 89.4026\n",
            "Epoch 21/300, Train Loss: 63.6556\n",
            "Epoch 21/300, Top 10% threshold: 90.2979\n",
            "Epoch 22/300, Train Loss: 59.9984\n",
            "Epoch 22/300, Top 10% threshold: 87.5800\n",
            "Epoch 23/300, Train Loss: 56.7401\n",
            "Epoch 23/300, Top 10% threshold: 89.0102\n",
            "Epoch 24/300, Train Loss: 52.9437\n",
            "Epoch 24/300, Top 10% threshold: 88.6236\n",
            "Epoch 25/300, Train Loss: 49.4507\n",
            "Epoch 25/300, Top 10% threshold: 88.4257\n",
            "Epoch 26/300, Train Loss: 46.7596\n",
            "Epoch 26/300, Top 10% threshold: 87.4596\n",
            "Epoch 27/300, Train Loss: 42.7727\n",
            "Epoch 27/300, Top 10% threshold: 89.5857\n",
            "Epoch 28/300, Train Loss: 40.2261\n",
            "Epoch 28/300, Top 10% threshold: 91.9674\n",
            "Epoch 29/300, Train Loss: 37.2949\n",
            "Epoch 29/300, Top 10% threshold: 88.2071\n",
            "Epoch 30/300, Train Loss: 34.1871\n",
            "Epoch 30/300, Top 10% threshold: 91.5881\n",
            "Epoch 31/300, Train Loss: 32.1629\n",
            "Epoch 31/300, Top 10% threshold: 88.2045\n",
            "Epoch 32/300, Train Loss: 28.8226\n",
            "Epoch 32/300, Top 10% threshold: 88.3580\n",
            "Epoch 33/300, Train Loss: 26.4925\n",
            "Epoch 33/300, Top 10% threshold: 88.5315\n",
            "Epoch 34/300, Train Loss: 24.4395\n",
            "Epoch 34/300, Top 10% threshold: 89.2128\n",
            "Epoch 35/300, Train Loss: 22.5874\n",
            "Epoch 35/300, Top 10% threshold: 89.9982\n",
            "Epoch 36/300, Train Loss: 20.5382\n",
            "Epoch 36/300, Top 10% threshold: 89.6638\n",
            "Epoch 37/300, Train Loss: 18.5943\n",
            "Epoch 37/300, Top 10% threshold: 89.6758\n",
            "Epoch 38/300, Train Loss: 16.7021\n",
            "Epoch 38/300, Top 10% threshold: 90.5941\n",
            "Epoch 39/300, Train Loss: 15.2009\n",
            "Epoch 39/300, Top 10% threshold: 89.8810\n",
            "Epoch 40/300, Train Loss: 13.9989\n",
            "Epoch 40/300, Top 10% threshold: 89.8362\n",
            "Epoch 41/300, Train Loss: 12.6046\n",
            "Epoch 41/300, Top 10% threshold: 89.9619\n",
            "Epoch 42/300, Train Loss: 11.3307\n",
            "Epoch 42/300, Top 10% threshold: 89.7227\n",
            "Epoch 43/300, Train Loss: 10.2980\n",
            "Epoch 43/300, Top 10% threshold: 90.9132\n",
            "Epoch 44/300, Train Loss: 9.1889\n",
            "Epoch 44/300, Top 10% threshold: 90.2628\n",
            "Epoch 45/300, Train Loss: 8.3024\n",
            "Epoch 45/300, Top 10% threshold: 89.2961\n",
            "Epoch 46/300, Train Loss: 7.6384\n",
            "Epoch 46/300, Top 10% threshold: 90.0154\n",
            "Epoch 47/300, Train Loss: 6.8904\n",
            "Epoch 47/300, Top 10% threshold: 89.7149\n",
            "Epoch 48/300, Train Loss: 6.2243\n",
            "Epoch 48/300, Top 10% threshold: 89.6455\n",
            "Epoch 49/300, Train Loss: 5.6603\n",
            "Epoch 49/300, Top 10% threshold: 90.7876\n",
            "Epoch 50/300, Train Loss: 5.2164\n",
            "Epoch 50/300, Top 10% threshold: 90.9286\n",
            "Epoch 51/300, Train Loss: 4.9298\n",
            "Epoch 51/300, Top 10% threshold: 90.2073\n",
            "Epoch 52/300, Train Loss: 4.5498\n",
            "Epoch 52/300, Top 10% threshold: 90.1341\n",
            "Epoch 53/300, Train Loss: 4.2855\n",
            "Epoch 53/300, Top 10% threshold: 89.0601\n",
            "Epoch 54/300, Train Loss: 4.0449\n",
            "Epoch 54/300, Top 10% threshold: 90.6902\n",
            "Epoch 55/300, Train Loss: 3.9112\n",
            "Epoch 55/300, Top 10% threshold: 90.8006\n",
            "Epoch 56/300, Train Loss: 3.7170\n",
            "Epoch 56/300, Top 10% threshold: 89.3777\n",
            "Epoch 57/300, Train Loss: 3.6334\n",
            "Epoch 57/300, Top 10% threshold: 90.0536\n",
            "Epoch 58/300, Train Loss: 3.5217\n",
            "Epoch 58/300, Top 10% threshold: 90.1620\n",
            "Epoch 59/300, Train Loss: 3.4558\n",
            "Epoch 59/300, Top 10% threshold: 90.9316\n",
            "Epoch 60/300, Train Loss: 3.3583\n",
            "Epoch 60/300, Top 10% threshold: 91.2577\n",
            "Epoch 61/300, Train Loss: 3.3391\n",
            "Epoch 61/300, Top 10% threshold: 89.9102\n",
            "Epoch 62/300, Train Loss: 3.2653\n",
            "Epoch 62/300, Top 10% threshold: 89.4939\n",
            "Epoch 63/300, Train Loss: 3.2236\n",
            "Epoch 63/300, Top 10% threshold: 89.9973\n",
            "Epoch 64/300, Train Loss: 3.2189\n",
            "Epoch 64/300, Top 10% threshold: 90.0399\n",
            "Epoch 65/300, Train Loss: 3.2289\n",
            "Epoch 65/300, Top 10% threshold: 89.7808\n",
            "Epoch 66/300, Train Loss: 3.1984\n",
            "Epoch 66/300, Top 10% threshold: 90.7453\n",
            "Epoch 67/300, Train Loss: 3.1826\n",
            "Epoch 67/300, Top 10% threshold: 90.0584\n",
            "Epoch 68/300, Train Loss: 3.1956\n",
            "Epoch 68/300, Top 10% threshold: 90.9147\n",
            "Epoch 69/300, Train Loss: 3.2183\n",
            "Epoch 69/300, Top 10% threshold: 89.9145\n",
            "Epoch 70/300, Train Loss: 3.1921\n",
            "Epoch 70/300, Top 10% threshold: 89.7991\n",
            "Epoch 71/300, Train Loss: 3.1657\n",
            "Epoch 71/300, Top 10% threshold: 90.4626\n",
            "Epoch 72/300, Train Loss: 3.1582\n",
            "Epoch 72/300, Top 10% threshold: 90.2651\n",
            "Epoch 73/300, Train Loss: 3.1772\n",
            "Epoch 73/300, Top 10% threshold: 90.3182\n",
            "Epoch 74/300, Train Loss: 3.1795\n",
            "Epoch 74/300, Top 10% threshold: 90.7078\n",
            "Epoch 75/300, Train Loss: 3.1511\n",
            "Epoch 75/300, Top 10% threshold: 90.4162\n",
            "Epoch 76/300, Train Loss: 3.1814\n",
            "Epoch 76/300, Top 10% threshold: 90.0249\n",
            "Epoch 77/300, Train Loss: 3.1498\n",
            "Epoch 77/300, Top 10% threshold: 90.4358\n",
            "Epoch 78/300, Train Loss: 3.1368\n",
            "Epoch 78/300, Top 10% threshold: 90.3024\n",
            "Epoch 79/300, Train Loss: 3.1541\n",
            "Epoch 79/300, Top 10% threshold: 88.9699\n",
            "Epoch 80/300, Train Loss: 3.1621\n",
            "Epoch 80/300, Top 10% threshold: 91.0641\n",
            "Epoch 81/300, Train Loss: 3.1591\n",
            "Epoch 81/300, Top 10% threshold: 89.9942\n",
            "Epoch 82/300, Train Loss: 3.1476\n",
            "Epoch 82/300, Top 10% threshold: 90.5458\n",
            "Epoch 83/300, Train Loss: 3.1211\n",
            "Epoch 83/300, Top 10% threshold: 90.6538\n",
            "Epoch 84/300, Train Loss: 3.1441\n",
            "Epoch 84/300, Top 10% threshold: 90.4726\n",
            "Epoch 85/300, Train Loss: 3.1107\n",
            "Epoch 85/300, Top 10% threshold: 90.1495\n",
            "Epoch 86/300, Train Loss: 3.1598\n",
            "Epoch 86/300, Top 10% threshold: 89.6474\n",
            "Epoch 87/300, Train Loss: 3.1468\n",
            "Epoch 87/300, Top 10% threshold: 89.2661\n",
            "Epoch 88/300, Train Loss: 3.1705\n",
            "Epoch 88/300, Top 10% threshold: 90.1631\n",
            "Epoch 89/300, Train Loss: 3.1422\n",
            "Epoch 89/300, Top 10% threshold: 89.9186\n",
            "Epoch 90/300, Train Loss: 3.1264\n",
            "Epoch 90/300, Top 10% threshold: 91.0177\n",
            "Epoch 91/300, Train Loss: 3.1289\n",
            "Epoch 91/300, Top 10% threshold: 91.3287\n",
            "Epoch 92/300, Train Loss: 3.1495\n",
            "Epoch 92/300, Top 10% threshold: 90.8357\n",
            "Epoch 93/300, Train Loss: 3.1173\n",
            "Epoch 93/300, Top 10% threshold: 90.5218\n",
            "Epoch 94/300, Train Loss: 3.1123\n",
            "Epoch 94/300, Top 10% threshold: 90.4708\n",
            "Epoch 95/300, Train Loss: 3.1208\n",
            "Epoch 95/300, Top 10% threshold: 90.4154\n",
            "Epoch 96/300, Train Loss: 3.1430\n",
            "Epoch 96/300, Top 10% threshold: 90.7167\n",
            "Epoch 97/300, Train Loss: 3.1469\n",
            "Epoch 97/300, Top 10% threshold: 90.4115\n",
            "Epoch 98/300, Train Loss: 3.1207\n",
            "Epoch 98/300, Top 10% threshold: 89.8844\n",
            "Epoch 99/300, Train Loss: 3.1433\n",
            "Epoch 99/300, Top 10% threshold: 90.8747\n",
            "Epoch 100/300, Train Loss: 3.1293\n",
            "Epoch 100/300, Top 10% threshold: 90.9170\n",
            "Epoch 101/300, Train Loss: 3.1072\n",
            "Epoch 101/300, Top 10% threshold: 89.1645\n",
            "Epoch 102/300, Train Loss: 3.1210\n",
            "Epoch 102/300, Top 10% threshold: 90.0842\n",
            "Epoch 103/300, Train Loss: 3.1234\n",
            "Epoch 103/300, Top 10% threshold: 90.5802\n",
            "Epoch 104/300, Train Loss: 3.1393\n",
            "Epoch 104/300, Top 10% threshold: 90.3678\n",
            "Epoch 105/300, Train Loss: 3.1257\n",
            "Epoch 105/300, Top 10% threshold: 90.0680\n",
            "Epoch 106/300, Train Loss: 3.1456\n",
            "Epoch 106/300, Top 10% threshold: 89.9154\n",
            "Epoch 107/300, Train Loss: 3.1231\n",
            "Epoch 107/300, Top 10% threshold: 90.5911\n",
            "Epoch 108/300, Train Loss: 3.0844\n",
            "Epoch 108/300, Top 10% threshold: 90.0313\n",
            "Epoch 109/300, Train Loss: 3.1163\n",
            "Epoch 109/300, Top 10% threshold: 90.5424\n",
            "Epoch 110/300, Train Loss: 3.1202\n",
            "Epoch 110/300, Top 10% threshold: 89.9908\n",
            "Epoch 111/300, Train Loss: 3.1104\n",
            "Epoch 111/300, Top 10% threshold: 89.5633\n",
            "Epoch 112/300, Train Loss: 3.1282\n",
            "Epoch 112/300, Top 10% threshold: 89.9717\n",
            "Epoch 113/300, Train Loss: 3.1136\n",
            "Epoch 113/300, Top 10% threshold: 90.3319\n",
            "Epoch 114/300, Train Loss: 3.0991\n",
            "Epoch 114/300, Top 10% threshold: 90.1426\n",
            "Epoch 115/300, Train Loss: 3.1065\n",
            "Epoch 115/300, Top 10% threshold: 89.9271\n",
            "Epoch 116/300, Train Loss: 3.1396\n",
            "Epoch 116/300, Top 10% threshold: 90.6931\n",
            "Epoch 117/300, Train Loss: 3.1256\n",
            "Epoch 117/300, Top 10% threshold: 89.7062\n",
            "Epoch 118/300, Train Loss: 3.1141\n",
            "Epoch 118/300, Top 10% threshold: 90.2358\n",
            "Epoch 119/300, Train Loss: 3.1221\n",
            "Epoch 119/300, Top 10% threshold: 89.1881\n",
            "Epoch 120/300, Train Loss: 3.1183\n",
            "Epoch 120/300, Top 10% threshold: 90.0671\n",
            "Epoch 121/300, Train Loss: 3.1085\n",
            "Epoch 121/300, Top 10% threshold: 90.4147\n",
            "Epoch 122/300, Train Loss: 3.0947\n",
            "Epoch 122/300, Top 10% threshold: 89.9091\n",
            "Epoch 123/300, Train Loss: 3.1051\n",
            "Epoch 123/300, Top 10% threshold: 90.7232\n",
            "Epoch 124/300, Train Loss: 3.1157\n",
            "Epoch 124/300, Top 10% threshold: 89.6051\n",
            "Epoch 125/300, Train Loss: 3.1345\n",
            "Epoch 125/300, Top 10% threshold: 89.7523\n",
            "Epoch 126/300, Train Loss: 3.1078\n",
            "Epoch 126/300, Top 10% threshold: 89.6784\n",
            "Epoch 127/300, Train Loss: 3.1318\n",
            "Epoch 127/300, Top 10% threshold: 89.8331\n",
            "Epoch 128/300, Train Loss: 3.0947\n",
            "Epoch 128/300, Top 10% threshold: 90.6586\n",
            "Epoch 129/300, Train Loss: 3.0828\n",
            "Epoch 129/300, Top 10% threshold: 89.7262\n",
            "Epoch 130/300, Train Loss: 3.0983\n",
            "Epoch 130/300, Top 10% threshold: 89.7120\n",
            "Epoch 131/300, Train Loss: 3.1139\n",
            "Epoch 131/300, Top 10% threshold: 89.7909\n",
            "Epoch 132/300, Train Loss: 3.1156\n",
            "Epoch 132/300, Top 10% threshold: 90.6779\n",
            "Epoch 133/300, Train Loss: 3.1022\n",
            "Epoch 133/300, Top 10% threshold: 88.8877\n",
            "Epoch 134/300, Train Loss: 3.1117\n",
            "Epoch 134/300, Top 10% threshold: 90.5751\n",
            "Epoch 135/300, Train Loss: 3.1236\n",
            "Epoch 135/300, Top 10% threshold: 89.9498\n",
            "Epoch 136/300, Train Loss: 3.1207\n",
            "Epoch 136/300, Top 10% threshold: 89.8532\n",
            "Epoch 137/300, Train Loss: 3.1055\n",
            "Epoch 137/300, Top 10% threshold: 89.8564\n",
            "Epoch 138/300, Train Loss: 3.1104\n",
            "Epoch 138/300, Top 10% threshold: 89.4727\n",
            "Epoch 139/300, Train Loss: 3.0988\n",
            "Epoch 139/300, Top 10% threshold: 89.1619\n",
            "Epoch 140/300, Train Loss: 3.1256\n",
            "Epoch 140/300, Top 10% threshold: 90.0208\n",
            "Epoch 141/300, Train Loss: 3.1203\n",
            "Epoch 141/300, Top 10% threshold: 91.1250\n",
            "Epoch 142/300, Train Loss: 3.0782\n",
            "Epoch 142/300, Top 10% threshold: 89.5256\n",
            "Epoch 143/300, Train Loss: 3.0997\n",
            "Epoch 143/300, Top 10% threshold: 90.1542\n",
            "Epoch 144/300, Train Loss: 3.1149\n",
            "Epoch 144/300, Top 10% threshold: 89.7892\n",
            "Epoch 145/300, Train Loss: 3.1035\n",
            "Epoch 145/300, Top 10% threshold: 90.0080\n",
            "Epoch 146/300, Train Loss: 3.1251\n",
            "Epoch 146/300, Top 10% threshold: 90.1149\n",
            "Epoch 147/300, Train Loss: 3.1056\n",
            "Epoch 147/300, Top 10% threshold: 89.4706\n",
            "Epoch 148/300, Train Loss: 3.0821\n",
            "Epoch 148/300, Top 10% threshold: 90.9446\n",
            "Epoch 149/300, Train Loss: 3.0942\n",
            "Epoch 149/300, Top 10% threshold: 90.1731\n",
            "Epoch 150/300, Train Loss: 3.1011\n",
            "Epoch 150/300, Top 10% threshold: 90.2767\n",
            "Epoch 151/300, Train Loss: 3.0797\n",
            "Epoch 151/300, Top 10% threshold: 90.0368\n",
            "Epoch 152/300, Train Loss: 3.1096\n",
            "Epoch 152/300, Top 10% threshold: 90.1246\n",
            "Epoch 153/300, Train Loss: 3.0837\n",
            "Epoch 153/300, Top 10% threshold: 90.8053\n",
            "Epoch 154/300, Train Loss: 3.1018\n",
            "Epoch 154/300, Top 10% threshold: 90.3143\n",
            "Epoch 155/300, Train Loss: 3.1025\n",
            "Epoch 155/300, Top 10% threshold: 90.7457\n",
            "Epoch 156/300, Train Loss: 3.0916\n",
            "Epoch 156/300, Top 10% threshold: 90.3943\n",
            "Epoch 157/300, Train Loss: 3.1000\n",
            "Epoch 157/300, Top 10% threshold: 90.0257\n",
            "Epoch 158/300, Train Loss: 3.0966\n",
            "Epoch 158/300, Top 10% threshold: 90.4151\n",
            "Epoch 159/300, Train Loss: 3.0773\n",
            "Epoch 159/300, Top 10% threshold: 89.5917\n",
            "Epoch 160/300, Train Loss: 3.1214\n",
            "Epoch 160/300, Top 10% threshold: 90.3124\n",
            "Epoch 161/300, Train Loss: 3.0808\n",
            "Epoch 161/300, Top 10% threshold: 90.6755\n",
            "Epoch 162/300, Train Loss: 3.0930\n",
            "Epoch 162/300, Top 10% threshold: 90.5221\n",
            "Epoch 163/300, Train Loss: 3.0988\n",
            "Epoch 163/300, Top 10% threshold: 90.5158\n",
            "Epoch 164/300, Train Loss: 3.0907\n",
            "Epoch 164/300, Top 10% threshold: 92.2105\n",
            "Epoch 165/300, Train Loss: 3.1107\n",
            "Epoch 165/300, Top 10% threshold: 89.7578\n",
            "Epoch 166/300, Train Loss: 3.0877\n",
            "Epoch 166/300, Top 10% threshold: 90.1518\n",
            "Epoch 167/300, Train Loss: 3.1090\n",
            "Epoch 167/300, Top 10% threshold: 90.0570\n",
            "Epoch 168/300, Train Loss: 3.0813\n",
            "Epoch 168/300, Top 10% threshold: 89.5325\n",
            "Epoch 169/300, Train Loss: 3.0946\n",
            "Epoch 169/300, Top 10% threshold: 90.5146\n",
            "Epoch 170/300, Train Loss: 3.1024\n",
            "Epoch 170/300, Top 10% threshold: 90.3549\n",
            "Epoch 171/300, Train Loss: 3.0797\n",
            "Epoch 171/300, Top 10% threshold: 89.5103\n",
            "Epoch 172/300, Train Loss: 3.0938\n",
            "Epoch 172/300, Top 10% threshold: 90.7479\n",
            "Epoch 173/300, Train Loss: 3.0777\n",
            "Epoch 173/300, Top 10% threshold: 89.7886\n",
            "Epoch 174/300, Train Loss: 3.0796\n",
            "Epoch 174/300, Top 10% threshold: 90.3556\n",
            "Epoch 175/300, Train Loss: 3.1197\n",
            "Epoch 175/300, Top 10% threshold: 90.6448\n",
            "Epoch 176/300, Train Loss: 3.1069\n",
            "Epoch 176/300, Top 10% threshold: 90.0616\n",
            "Epoch 177/300, Train Loss: 3.0702\n",
            "Epoch 177/300, Top 10% threshold: 90.7614\n",
            "Epoch 178/300, Train Loss: 3.0875\n",
            "Epoch 178/300, Top 10% threshold: 89.6168\n",
            "Epoch 179/300, Train Loss: 3.0704\n",
            "Epoch 179/300, Top 10% threshold: 90.0860\n",
            "Epoch 180/300, Train Loss: 3.0714\n",
            "Epoch 180/300, Top 10% threshold: 90.7164\n",
            "Epoch 181/300, Train Loss: 3.1042\n",
            "Epoch 181/300, Top 10% threshold: 89.6684\n",
            "Epoch 182/300, Train Loss: 3.0929\n",
            "Epoch 182/300, Top 10% threshold: 89.5357\n",
            "Epoch 183/300, Train Loss: 3.1003\n",
            "Epoch 183/300, Top 10% threshold: 90.1006\n",
            "Epoch 184/300, Train Loss: 3.1041\n",
            "Epoch 184/300, Top 10% threshold: 90.6533\n",
            "Epoch 185/300, Train Loss: 3.0828\n",
            "Epoch 185/300, Top 10% threshold: 89.3727\n",
            "Epoch 186/300, Train Loss: 3.1026\n",
            "Epoch 186/300, Top 10% threshold: 90.3916\n",
            "Epoch 187/300, Train Loss: 3.0785\n",
            "Epoch 187/300, Top 10% threshold: 90.1229\n",
            "Epoch 188/300, Train Loss: 3.0961\n",
            "Epoch 188/300, Top 10% threshold: 90.6249\n",
            "Epoch 189/300, Train Loss: 3.0801\n",
            "Epoch 189/300, Top 10% threshold: 90.5064\n",
            "Epoch 190/300, Train Loss: 3.1064\n",
            "Epoch 190/300, Top 10% threshold: 90.0248\n",
            "Epoch 191/300, Train Loss: 3.0842\n",
            "Epoch 191/300, Top 10% threshold: 90.3822\n",
            "Epoch 192/300, Train Loss: 3.0864\n",
            "Epoch 192/300, Top 10% threshold: 90.1936\n",
            "Epoch 193/300, Train Loss: 3.0897\n",
            "Epoch 193/300, Top 10% threshold: 90.2775\n",
            "Epoch 194/300, Train Loss: 3.0931\n",
            "Epoch 194/300, Top 10% threshold: 90.3822\n",
            "Epoch 195/300, Train Loss: 3.1135\n",
            "Epoch 195/300, Top 10% threshold: 89.3567\n",
            "Epoch 196/300, Train Loss: 3.0866\n",
            "Epoch 196/300, Top 10% threshold: 89.6122\n",
            "Epoch 197/300, Train Loss: 3.0927\n",
            "Epoch 197/300, Top 10% threshold: 90.0487\n",
            "Epoch 198/300, Train Loss: 3.0989\n",
            "Epoch 198/300, Top 10% threshold: 89.6336\n",
            "Epoch 199/300, Train Loss: 3.0797\n",
            "Epoch 199/300, Top 10% threshold: 89.8088\n",
            "Epoch 200/300, Train Loss: 3.0902\n",
            "Epoch 200/300, Top 10% threshold: 90.5969\n",
            "Epoch 201/300, Train Loss: 3.0947\n",
            "Epoch 201/300, Top 10% threshold: 89.7872\n",
            "Epoch 202/300, Train Loss: 3.0898\n",
            "Epoch 202/300, Top 10% threshold: 89.8314\n",
            "Epoch 203/300, Train Loss: 3.1054\n",
            "Epoch 203/300, Top 10% threshold: 90.2736\n",
            "Epoch 204/300, Train Loss: 3.0922\n",
            "Epoch 204/300, Top 10% threshold: 90.3213\n",
            "Epoch 205/300, Train Loss: 3.0916\n",
            "Epoch 205/300, Top 10% threshold: 90.3067\n",
            "Epoch 206/300, Train Loss: 3.0907\n",
            "Epoch 206/300, Top 10% threshold: 89.8948\n",
            "Epoch 207/300, Train Loss: 3.0700\n",
            "Epoch 207/300, Top 10% threshold: 89.9963\n",
            "Epoch 208/300, Train Loss: 3.0873\n",
            "Epoch 208/300, Top 10% threshold: 90.2097\n",
            "Epoch 209/300, Train Loss: 3.0831\n",
            "Epoch 209/300, Top 10% threshold: 89.8956\n",
            "Epoch 210/300, Train Loss: 3.1004\n",
            "Epoch 210/300, Top 10% threshold: 90.8670\n",
            "Epoch 211/300, Train Loss: 3.0989\n",
            "Epoch 211/300, Top 10% threshold: 90.1726\n",
            "Epoch 212/300, Train Loss: 3.0945\n",
            "Epoch 212/300, Top 10% threshold: 89.8802\n",
            "Epoch 213/300, Train Loss: 3.0831\n",
            "Epoch 213/300, Top 10% threshold: 90.4946\n",
            "Epoch 214/300, Train Loss: 3.0915\n",
            "Epoch 214/300, Top 10% threshold: 90.8682\n",
            "Epoch 215/300, Train Loss: 3.0881\n",
            "Epoch 215/300, Top 10% threshold: 90.3404\n",
            "Epoch 216/300, Train Loss: 3.0774\n",
            "Epoch 216/300, Top 10% threshold: 90.6806\n",
            "Epoch 217/300, Train Loss: 3.0607\n",
            "Epoch 217/300, Top 10% threshold: 91.0892\n",
            "Epoch 218/300, Train Loss: 3.0926\n",
            "Epoch 218/300, Top 10% threshold: 90.1899\n",
            "Epoch 219/300, Train Loss: 3.0586\n",
            "Epoch 219/300, Top 10% threshold: 89.9253\n",
            "Epoch 220/300, Train Loss: 3.0993\n",
            "Epoch 220/300, Top 10% threshold: 90.6286\n",
            "Epoch 221/300, Train Loss: 3.0871\n",
            "Epoch 221/300, Top 10% threshold: 90.3579\n",
            "Epoch 222/300, Train Loss: 3.1045\n",
            "Epoch 222/300, Top 10% threshold: 89.7788\n",
            "Epoch 223/300, Train Loss: 3.0885\n",
            "Epoch 223/300, Top 10% threshold: 90.1038\n",
            "Epoch 224/300, Train Loss: 3.1035\n",
            "Epoch 224/300, Top 10% threshold: 89.7818\n",
            "Epoch 225/300, Train Loss: 3.0834\n",
            "Epoch 225/300, Top 10% threshold: 90.3610\n",
            "Epoch 226/300, Train Loss: 3.0817\n",
            "Epoch 226/300, Top 10% threshold: 89.8871\n",
            "Epoch 227/300, Train Loss: 3.0749\n",
            "Epoch 227/300, Top 10% threshold: 90.6866\n",
            "Epoch 228/300, Train Loss: 3.0809\n",
            "Epoch 228/300, Top 10% threshold: 90.0230\n",
            "Epoch 229/300, Train Loss: 3.0795\n",
            "Epoch 229/300, Top 10% threshold: 89.8675\n",
            "Epoch 230/300, Train Loss: 3.0772\n",
            "Epoch 230/300, Top 10% threshold: 90.2672\n",
            "Epoch 231/300, Train Loss: 3.0912\n",
            "Epoch 231/300, Top 10% threshold: 90.3374\n",
            "Epoch 232/300, Train Loss: 3.1024\n",
            "Epoch 232/300, Top 10% threshold: 90.5064\n",
            "Epoch 233/300, Train Loss: 3.1007\n",
            "Epoch 233/300, Top 10% threshold: 90.6982\n",
            "Epoch 234/300, Train Loss: 3.1055\n",
            "Epoch 234/300, Top 10% threshold: 89.9349\n",
            "Epoch 235/300, Train Loss: 3.0861\n",
            "Epoch 235/300, Top 10% threshold: 89.7976\n",
            "Epoch 236/300, Train Loss: 3.0819\n",
            "Epoch 236/300, Top 10% threshold: 89.9602\n",
            "Epoch 237/300, Train Loss: 3.0908\n",
            "Epoch 237/300, Top 10% threshold: 89.5951\n",
            "Epoch 238/300, Train Loss: 3.0710\n",
            "Epoch 238/300, Top 10% threshold: 91.2053\n",
            "Epoch 239/300, Train Loss: 3.0820\n",
            "Epoch 239/300, Top 10% threshold: 90.4918\n",
            "Epoch 240/300, Train Loss: 3.0882\n",
            "Epoch 240/300, Top 10% threshold: 89.1142\n",
            "Epoch 241/300, Train Loss: 3.0771\n",
            "Epoch 241/300, Top 10% threshold: 90.1075\n",
            "Epoch 242/300, Train Loss: 3.0811\n",
            "Epoch 242/300, Top 10% threshold: 90.7171\n",
            "Epoch 243/300, Train Loss: 3.0774\n",
            "Epoch 243/300, Top 10% threshold: 89.7986\n",
            "Epoch 244/300, Train Loss: 3.0760\n",
            "Epoch 244/300, Top 10% threshold: 89.6897\n",
            "Epoch 245/300, Train Loss: 3.0846\n",
            "Epoch 245/300, Top 10% threshold: 90.1776\n",
            "Epoch 246/300, Train Loss: 3.0822\n",
            "Epoch 246/300, Top 10% threshold: 90.8087\n",
            "Epoch 247/300, Train Loss: 3.0820\n",
            "Epoch 247/300, Top 10% threshold: 89.5748\n",
            "Epoch 248/300, Train Loss: 3.0796\n",
            "Epoch 248/300, Top 10% threshold: 90.1079\n",
            "Epoch 249/300, Train Loss: 3.0805\n",
            "Epoch 249/300, Top 10% threshold: 90.1510\n",
            "Epoch 250/300, Train Loss: 3.1016\n",
            "Epoch 250/300, Top 10% threshold: 91.3391\n",
            "Epoch 251/300, Train Loss: 3.0893\n",
            "Epoch 251/300, Top 10% threshold: 90.2571\n",
            "Epoch 252/300, Train Loss: 3.0974\n",
            "Epoch 252/300, Top 10% threshold: 90.0413\n",
            "Epoch 253/300, Train Loss: 3.0974\n",
            "Epoch 253/300, Top 10% threshold: 90.2595\n",
            "Epoch 254/300, Train Loss: 3.0884\n",
            "Epoch 254/300, Top 10% threshold: 90.8963\n",
            "Epoch 255/300, Train Loss: 3.0912\n",
            "Epoch 255/300, Top 10% threshold: 90.0380\n",
            "Epoch 256/300, Train Loss: 3.0753\n",
            "Epoch 256/300, Top 10% threshold: 89.6793\n",
            "Epoch 257/300, Train Loss: 3.1010\n",
            "Epoch 257/300, Top 10% threshold: 90.1242\n",
            "Epoch 258/300, Train Loss: 3.0978\n",
            "Epoch 258/300, Top 10% threshold: 90.1974\n",
            "Epoch 259/300, Train Loss: 3.0732\n",
            "Epoch 259/300, Top 10% threshold: 89.9589\n",
            "Epoch 260/300, Train Loss: 3.0865\n",
            "Epoch 260/300, Top 10% threshold: 90.5836\n",
            "Epoch 261/300, Train Loss: 3.0703\n",
            "Epoch 261/300, Top 10% threshold: 90.7262\n",
            "Epoch 262/300, Train Loss: 3.0953\n",
            "Epoch 262/300, Top 10% threshold: 90.2817\n",
            "Epoch 263/300, Train Loss: 3.0819\n",
            "Epoch 263/300, Top 10% threshold: 90.2298\n",
            "Epoch 264/300, Train Loss: 3.0788\n",
            "Epoch 264/300, Top 10% threshold: 89.9072\n",
            "Epoch 265/300, Train Loss: 3.1019\n",
            "Epoch 265/300, Top 10% threshold: 90.0184\n",
            "Epoch 266/300, Train Loss: 3.0972\n",
            "Epoch 266/300, Top 10% threshold: 91.0899\n",
            "Epoch 267/300, Train Loss: 3.0785\n",
            "Epoch 267/300, Top 10% threshold: 90.3714\n",
            "Epoch 268/300, Train Loss: 3.0739\n",
            "Epoch 268/300, Top 10% threshold: 89.7043\n",
            "Epoch 269/300, Train Loss: 3.0920\n",
            "Epoch 269/300, Top 10% threshold: 89.4129\n",
            "Epoch 270/300, Train Loss: 3.0552\n",
            "Epoch 270/300, Top 10% threshold: 90.1072\n",
            "Epoch 271/300, Train Loss: 3.0758\n",
            "Epoch 271/300, Top 10% threshold: 90.0142\n",
            "Epoch 272/300, Train Loss: 3.0890\n",
            "Epoch 272/300, Top 10% threshold: 90.6663\n",
            "Epoch 273/300, Train Loss: 3.0608\n",
            "Epoch 273/300, Top 10% threshold: 90.1669\n",
            "Epoch 274/300, Train Loss: 3.0797\n",
            "Epoch 274/300, Top 10% threshold: 89.6809\n",
            "Epoch 275/300, Train Loss: 3.0628\n",
            "Epoch 275/300, Top 10% threshold: 90.3900\n",
            "Epoch 276/300, Train Loss: 3.0798\n",
            "Epoch 276/300, Top 10% threshold: 89.5772\n",
            "Epoch 277/300, Train Loss: 3.0763\n",
            "Epoch 277/300, Top 10% threshold: 90.4451\n",
            "Epoch 278/300, Train Loss: 3.0681\n",
            "Epoch 278/300, Top 10% threshold: 89.4952\n",
            "Epoch 279/300, Train Loss: 3.0883\n",
            "Epoch 279/300, Top 10% threshold: 90.1429\n",
            "Epoch 280/300, Train Loss: 3.0614\n",
            "Epoch 280/300, Top 10% threshold: 90.6104\n",
            "Epoch 281/300, Train Loss: 3.0841\n",
            "Epoch 281/300, Top 10% threshold: 90.2265\n",
            "Epoch 282/300, Train Loss: 3.0806\n",
            "Epoch 282/300, Top 10% threshold: 89.8358\n",
            "Epoch 283/300, Train Loss: 3.0934\n",
            "Epoch 283/300, Top 10% threshold: 90.1749\n",
            "Epoch 284/300, Train Loss: 3.0790\n",
            "Epoch 284/300, Top 10% threshold: 90.9722\n",
            "Epoch 285/300, Train Loss: 3.1006\n",
            "Epoch 285/300, Top 10% threshold: 90.0644\n",
            "Epoch 286/300, Train Loss: 3.0619\n",
            "Epoch 286/300, Top 10% threshold: 90.3455\n",
            "Epoch 287/300, Train Loss: 3.0382\n",
            "Epoch 287/300, Top 10% threshold: 90.3446\n",
            "Epoch 288/300, Train Loss: 3.0745\n",
            "Epoch 288/300, Top 10% threshold: 90.4227\n",
            "Epoch 289/300, Train Loss: 3.0514\n",
            "Epoch 289/300, Top 10% threshold: 90.9379\n",
            "Epoch 290/300, Train Loss: 3.0937\n",
            "Epoch 290/300, Top 10% threshold: 90.7929\n",
            "Epoch 291/300, Train Loss: 3.0981\n",
            "Epoch 291/300, Top 10% threshold: 90.4732\n",
            "Epoch 292/300, Train Loss: 3.0537\n",
            "Epoch 292/300, Top 10% threshold: 90.0365\n",
            "Epoch 293/300, Train Loss: 3.0734\n",
            "Epoch 293/300, Top 10% threshold: 88.9078\n",
            "Epoch 294/300, Train Loss: 3.0775\n",
            "Epoch 294/300, Top 10% threshold: 90.4788\n",
            "Epoch 295/300, Train Loss: 3.0764\n",
            "Epoch 295/300, Top 10% threshold: 89.5692\n",
            "Epoch 296/300, Train Loss: 3.0914\n",
            "Epoch 296/300, Top 10% threshold: 90.2616\n",
            "Epoch 297/300, Train Loss: 3.0657\n",
            "Epoch 297/300, Top 10% threshold: 90.4263\n",
            "Epoch 298/300, Train Loss: 3.0876\n",
            "Epoch 298/300, Top 10% threshold: 90.2754\n",
            "Epoch 299/300, Train Loss: 3.0896\n",
            "Epoch 299/300, Top 10% threshold: 90.0264\n",
            "Epoch 300/300, Train Loss: 3.0657\n",
            "Epoch 300/300, Top 10% threshold: 90.6175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 상위 10% 임계값 계산\n",
        "threshold = np.percentile(best_pred, 90)\n",
        "top_10_percent_mask = best_pred >= threshold\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission_df = pd.read_csv(submission_csv_path)\n",
        "submission_df['y'] = best_pred\n",
        "submission_df.to_csv(f'{mode}_{method}_bestepoch{best_epoch}.csv', index=False)\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"Top 10% threshold: {threshold:.4f}\")\n",
        "print(f\"Number of samples in top 10%: {sum(top_10_percent_mask)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHAQkQHlKBdK",
        "outputId": "2647824c-be0b-4d04-fdf0-e35ed3c65fa5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10% threshold: 92.2973\n",
            "Number of samples in top 10%: [499]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 상위 10% 임계값 계산\n",
        "threshold = np.percentile(last_pred, 90)\n",
        "top_10_percent_mask = last_pred >= threshold\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission_df = pd.read_csv(submission_csv_path)\n",
        "submission_df['y'] = last_pred\n",
        "submission_df.to_csv(f'{mode}_{method}_lastepoch{num_epochs}.csv', index=False)\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"Top 10% threshold: {threshold:.4f}\")\n",
        "print(f\"Number of samples in top 10%: {sum(top_10_percent_mask)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1oYZ-QY0IJ8",
        "outputId": "6e5788b4-c36b-4f6a-9504-364bbbbb190a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10% threshold: 90.6175\n",
            "Number of samples in top 10%: [499]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}